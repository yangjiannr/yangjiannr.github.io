<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Qt中事件处理]]></title>
    <url>%2F2018%2F11%2F20%2FQt%E4%B8%AD%E4%BA%8B%E4%BB%B6%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[事件处理当您执行QApplication的exec()方法之后，应用程序会进入事件循环来倾听应用程序的事件，事件来源通常是窗口系统，例如使用者的鼠标事件或键盘事件，事件来源可以是Qt应用程序事件本身，例如QTimerEvent，事件来源也可以是使用者自定义的事件，透过QApplicaiton的sendEvent()或postEvent()来发送。 事件的种类事件处理分为两种机制： 高级的：信号和槽机制 低级的：事件处理程序（Event Handle） 如果窗口部件不能处理这个事件，事件就会传递到该窗口部件的父窗口部件中。这样就可以把未处理事件从自窗口部件一直传递到父窗口部件直至顶层窗口部件，而如果还是没能处理掉这个时间，那么就会简单地予以扔掉。 PyQt为拦截和处理事件提供了5种不同方式： 重新实现特殊的事件处理器 通过重新实现一些事件处理程序是可以控制窗口部件的行为的——例如，可以重新实现KeyPressEvent()、mousePressEvent()和mouseReleaseEvent()。通过重新实现resizeEvent()和paintEvent()，也可以控制一个窗口部件的外观。 重新实现QObject::event() 在任何特定事件处理程序调用之前，都会调用event()事件处理程序。重新实现这个方法可以允许我们处理那些不能在某一特定事件处理程序中处理的事件，或者实现那些勇于不存在明确事件处理程序的事件，不如QEvent.ToolBarChange。当需要重新事件这些处理程序时，可以对任何不是自己亲自处理的时间调用它们的基类实现。 在QObject中安装事件过滤器 可以对任何QOject调用installEventFilter()。这就意味着，对于QObject的全部事件来说，它们都会先传递到我们自己的事件过滤eventFilter()函数：在它们到达目标对象之前，可以抛弃或者修改这些事件中的任何一个。如果在同一个对象上安装了多个事件处理器，那么就会按照安装顺序逆序，从最近安装的到最先安装的，依次激活这些事件处理器。 在QApplication对象中安装事件过滤器 一旦在qApp（唯一的QApplication对象）中注册了事件过滤器，那么应用程序中每个对象的每个事件都会在发送到其他事件过滤器之前，先发送给这个eventFilter()函数。这种处理方式对于调试非常有用。它也可以用来处理哪些发送给失效窗口部件的鼠标事件，因为QApplication通常都会忽略这些事件。 子类化QApplication并且重新实现notify() Qt调用QApplication::notify()来发送一个事件。重新实现这个函数是在事件过滤器得到所有事件之前获得它们的唯一方式。事件过滤器通常可以同时有多个，而notify()函数却只有一个。这个方法会在任何事件过滤程序或者事件处理程序之前得到调用，因此它提供了终极控制能力。实践中，仅会在调试时才会用到这一方法。 QWidget部分事件 语法 说明 w.closeEvent(e) 重新实现本函数可以给用户一个保存那些未保存修改和用户设置的机会；w是QWidget的一个子类，e是QEvent的一个相关子类 w.contextMenuEvent(e) 重新实现本函数可以提供自定义的上下文菜单。一种更为简洁的替代方法是调用setContextMenuPolicy(Qt.ActionsContextMenu)并借助QWidget.addAction()想窗口部件添加一些动作 w.dragEnterEvent(e) 重新实现本函数可以用来说明窗口部件是否接受或者拒绝在QDragEnterEvent e中进行拖放操作 w.dragMoveEvent(e) 是对可接受的拖放动作的重新实现——例如，对QDragMoveEvent e不接受，或者接受一个或者多个移动、复制以及连接等操作事件 w.dropEvent(e) 是对在QDropEvent e中进行拖放操作的重新实现 w.event(e) 是对没有特定时间处理程序的那些事件处理方法的重新实现——例如，对于Tab键的处理。这个事件是从QObject继承来的 w.keyPressEvent(e) 是对按键事件的重新实现 w.mouseDoubleClickEvent(e) 是对QMouseEvent e中给定的鼠标双击事件响应的重新实现 w.mouseMoveEvent(e) 重新实现来响应在QMouseEvent e中给定的鼠标移动事件。这个事件处理程序会受QWidget.setMouseTracking()的影响 w.mousePressEvent(e) 重新实现对鼠标按下事件的影响 w.mouseReleaseEvent(e) 重新实现对鼠标松开事件的影响 w.paintEvent(e) 窗口部件绘制事件的重新实现 w.resizeEvent(e) 窗口部件尺寸大小改变事件的重新实现 如果用户打开了鼠标状态跟踪（通过单机鼠标），将会产生一些鼠标移动事件，并且会在每个鼠标移动事件中都调用这个方法。 事件与信号-槽机制的不同Qt的事件跟Signal、Slot机制是不同的。Signal与Slot的机制是同步的（Synchronous），Signal是由对象发出的，使 用QObject的connect()连接对象上定义的Slot来立即处理。Qt的事件可以是异步的（Asynchronous）的，Q使用一个事件队列来维护，新的事件产生时基本上会被排到队列的尾端，前一个事 件处理完成，再从队列的前端取出下一个队列来处理，必要的时候，Qt的事件也可以是同步的，而事件还可以使用事件过滤器进行过滤处理。 Example123456789101112131415161718192021222324252627282930313233343536373839from PySide2.QtWidgets import *import sysclass EventLabel(QLabel): def __init__(self, parent=None): super(EventLabel, self).__init__(parent) def mouseMoveEvent(self, event): # 坐标系：物理坐标和逻辑坐标 msg = "&lt;center&gt;&lt;h1&gt;Move: \ (&#123;&#125;, &#123;&#125;)&lt;/h1&gt;&lt;/center&gt;".format(event.globalX(), event.globalY()) self.setText(msg) def mousePressEvent(self, event): msg = "&lt;center&gt;&lt;h1&gt;Press: \ (&#123;&#125;, &#123;&#125;)&lt;/h1&gt;&lt;/center&gt;".format(event.x(), event.y()) self.setText(msg) def mouseReleaseEvent(self, event): msg = "&lt;center&gt;&lt;h1&gt;Release: \ (&#123;&#125;, &#123;&#125;)&lt;/h1&gt;&lt;/center&gt;".format(event.x(), event.y()) self.setText(msg) def mouseDoubleClickEvent(self, event): msg = "&lt;center&gt;&lt;h1&gt;Double Click: \ (&#123;&#125;, &#123;&#125;)&lt;/h1&gt;&lt;/center&gt;".format(event.x(), event.y()) self.setText(msg)app = QApplication(sys.argv) window = EventLabel()window.setGeometry(200, 200, 500, 400)# 默认MouseTracking是False# 如果mouseTracking是False, 光标在窗口中至少点击一次并且光标仍在窗口中移动时，触发鼠标移动事件# 如果mouseTracking是True, 光标只要在窗口中移动就触发鼠标移动事件，不论是否点击窗口.window.setMouseTracking(True)window.show()sys.exit(app.exec_())]]></content>
      <categories>
        <category>Qt</category>
      </categories>
      <tags>
        <tag>Qt</tag>
        <tag>Event</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python三种抓取方法性能对比（re、BeautifulSoup、lxml）]]></title>
    <url>%2F2018%2F10%2F07%2FPython%E4%B8%89%E7%A7%8D%E6%8A%93%E5%8F%96%E6%96%B9%E6%B3%95%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546import lxml.htmlimport requestsimport refrom bs4 import BeautifulSoupimport time&lt;!-- more --&gt;FIELDS = ('area', 'population', 'iso', 'country', 'capital', 'continent', 'tld', 'currency_code', 'currency_name', 'phone', 'postal_code_format', 'postal_code_regex', 'languages', 'neighbours')def re_scraper(html): results = &#123;&#125; for field in FIELDS: results[field] = re.search('&lt;tr id="places_%s__row"&gt;.*?&lt;td class="w2p_fw"&gt;(.*?)&lt;/td&gt;' % field, html).groups()[0] return resultsdef bs_scraper(html): soup = BeautifulSoup(html, 'html.parser') results = &#123;&#125; for field in FIELDS: results[field] = soup.find('table').find('tr', id='places_%s__row' % field).find( 'td', class_='w2p_fw').text return resultsdef lxml_scraper(html): tree = lxml.html.fromstring(html) results = &#123;&#125; for field in FIELDS: results[field] = tree.cssselect('#places_%s__row &gt; td.w2p_fw' % field)[0].text_content() return resultsif __name__ == '__main__': response = requests.get('http://example.webscraping.com/places/default/view/Aland-Islands-2') html = response.text NUM_ITERATIONS = 1000 for name, scraper in [('Regular expressions', re_scraper), ('BeautifulSoup', bs_scraper), ('Lxml', lxml_scraper)]: start = time.time() for i in range(NUM_ITERATIONS): if scraper == re_scraper: re.purge() result = scraper(html) assert(result['area'] == '1,580 square kilometres') end = time.time() print('%s: %.2f seconds' % (name, end - start)) 结果： Regular expressions: 2.66 secondsBeautifulSoup: 22.00 secondsLxml: 5.35 seconds[Finished in 31.0s] 这段代码中，每个爬虫都会执行1000次，然后打印总用时。从结果可以看出，Beautiful Soup比其他两种方法慢了很多。因为lxml和正则表达式模块都是C语言编写的，而Beautiful Soup则是纯Python编写的。由于lxml在搜索元素之前，必须将输入解析为内部结构，因此会产生额外的开销。而当抓取同一网页的多个特征时，这种初始化解析产生的开销就会降低，lxml也就更具有竞争力。 如果你的爬虫瓶颈是下载网页，而不是抽取数据的话，那么使用较慢的方法（如BeautifulSoup）也不成问题。如果只需要少量数据，并且想要避免额外依赖的话，那么正则表达式可能更加适合。不过，通常情况下，lxml是抓取数据的最好选择，这是因为该方法既快速又健壮，而正则表达式和BeautifulSoup只在某些特定场景下有用。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Regular</tag>
        <tag>BeautifulSoup</tag>
        <tag>Lxml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（二）ZeroMQ编程初探]]></title>
    <url>%2F2018%2F09%2F18%2F%EF%BC%88%E4%BA%8C%EF%BC%89ZeroMQ%E7%BC%96%E7%A8%8B%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[编程环境搭建MacOS通过brew安装：brew install zmq 如果使用c++语言编程还需要获取到zeroMQ的c++版头文件git clone https://github.com/zeromq/cppzmq.git将zmq.hpp拷贝到/usr/local/include下 Linux12345wget https://github.com/zeromq/libzmq/releases/download/v4.2.3/zeromq-4.2.3.tar.gztar -zxvf zeromq-4.2.3.tar.gzcd zeromq-4.2.3.tar.gz./configuremake &amp;&amp; make install 如果使用c++语言编程还需要获取到zeroMQ的c++版头文件git clone https://github.com/zeromq/cppzmq.git将zmq.hpp拷贝到/usr/local/include下 三种模式实例请求回应简单模式（Request-Reply）： 123456789101112131415161718192021222324252627282930313233343536373839// helloWorldServer.cpp // Hello World server in C++// Binds REP socket to tcp://*:5555// Expects "Hello" from client, replies with "World"#include &lt;zmq.hpp&gt;#include &lt;string&gt;#include &lt;iostream&gt;#ifndef _WIN32#include &lt;unistd.h&gt;#else#include &lt;windows.h&gt;#define sleep(n) Sleep(n)#endifint main () &#123; // Prepare our context and socket zmq::context_t context (1); zmq::socket_t socket (context, ZMQ_REP); socket.bind ("tcp://*:5555"); while (true) &#123; zmq::message_t request; // Wait for next request from client socket.recv (&amp;request); std::cout &lt;&lt; "Received Hello" &lt;&lt; std::endl; // Do some 'work' sleep(1); // Send reply back to client zmq::message_t reply (5); memcpy (reply.data (), "World", 5); socket.send (reply); &#125; return 0;&#125; 1234567891011121314151617181920212223242526272829303132// helloWorldClient.cpp// Hello World client in C++// Connects REQ socket to tcp://localhost:5555// Sends "Hello" to server, expects "World" back#include &lt;zmq.hpp&gt;#include &lt;string&gt;#include &lt;iostream&gt;int main ()&#123; // Prepare our context and socket zmq::context_t context (1); zmq::socket_t socket (context, ZMQ_REQ); std::cout &lt;&lt; "Connecting to hello world server…" &lt;&lt; std::endl; socket.connect ("tcp://localhost:5555"); // Do 10 requests, waiting each time for a response for (int request_nbr = 0; request_nbr != 10; request_nbr++) &#123; zmq::message_t request (5); memcpy (request.data (), "Hello", 5); std::cout &lt;&lt; "Sending Hello " &lt;&lt; request_nbr &lt;&lt; "…" &lt;&lt; std::endl; socket.send (request); // Get the reply. zmq::message_t reply; socket.recv (&amp;reply); std::cout &lt;&lt; "Received World " &lt;&lt; request_nbr &lt;&lt; std::endl; &#125; return 0;&#125; 编译运行：(1)helloWorldServer: 12g++ -o helloWorldServer helloWorldServer.cpp./helloWorldServer helloWorldClient: 12g++ -o helloWorldClient helloWorldClient.cpp./helloWorldClient (2)Or Makefile: 12345678910111213.PHONY : clean all CC = g++ BIN = helloWorldClient helloWorldServerall: $(BIN)client: $(CC) $@.cpp -o $@ -lzmq -I./include -L./libserver: $(CC) $@.cpp -o $@ -lzmq -I./include -L./libclean: rm $(BIN) helloWorldServer: 1./helloWorldServer helloWorldClient: 1./helloWorldClient 结果： 发布订阅模式（Publish-Subscribe）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// weatherUpdateServer.cpp// Weather update server in C++// Binds PUB socket to tcp://*:5556// Publishes random weather updates//// Olivier Chamoux &lt;olivier.chamoux@fr.thalesgroup.com&gt;//#include &lt;zmq.hpp&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;#if (defined (WIN32))#include &lt;zhelpers.hpp&gt;#endif#define within(num) (int) ((float) num * random () / (RAND_MAX + 1.0))int main () &#123; // Prepare our context and publisher zmq::context_t context (1); zmq::socket_t publisher (context, ZMQ_PUB); publisher.bind("tcp://*:5556"); publisher.bind("ipc://weather.ipc"); // Not usable on Windows. // Initialize random number generator srandom ((unsigned) time (NULL)); while (1) &#123; int zipcode, temperature, relhumidity; // Get values that will fool the boss zipcode = within (100000); temperature = within (215) - 80; relhumidity = within (50) + 10; // Send message to all subscribers zmq::message_t message(20); snprintf ((char *) message.data(), 20 , "%05d %d %d", zipcode, temperature, relhumidity); publisher.send(message); &#125; return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344// weatherUpdateClient.cpp// Weather update client in C++// Connects SUB socket to tcp://localhost:5556// Collects weather updates and finds avg temp in zipcode//// Olivier Chamoux &lt;olivier.chamoux@fr.thalesgroup.com&gt;//#include &lt;zmq.hpp&gt;#include &lt;iostream&gt;#include &lt;sstream&gt;int main (int argc, char *argv[])&#123; zmq::context_t context (1); // Socket to talk to server std::cout &lt;&lt; "Collecting updates from weather server…\n" &lt;&lt; std::endl; zmq::socket_t subscriber (context, ZMQ_SUB); subscriber.connect("tcp://localhost:5556"); // Subscribe to zipcode, default is NYC, 10001 const char *filter = (argc &gt; 1)? argv [1]: "10001 "; subscriber.setsockopt(ZMQ_SUBSCRIBE, filter, strlen (filter)); // Process 100 updates int update_nbr; long total_temp = 0; for (update_nbr = 0; update_nbr &lt; 100; update_nbr++) &#123; zmq::message_t update; int zipcode, temperature, relhumidity; subscriber.recv(&amp;update); std::istringstream iss(static_cast&lt;char*&gt;(update.data())); iss &gt;&gt; zipcode &gt;&gt; temperature &gt;&gt; relhumidity ; total_temp += temperature; &#125; std::cout &lt;&lt; "Average temperature for zipcode '"&lt;&lt; filter &lt;&lt;"' was "&lt;&lt;(int) (total_temp / update_nbr) &lt;&lt;"F" &lt;&lt; std::endl; return 0;&#125; 编译运行：(1)weatherUpdateServer: 12g++ -o weatherUpdateServer weatherUpdateServer.cpp./weatherUpdateServer weatherUpdateClient: 12g++ -o weatherUpdateClient weatherUpdateClient.cpp./weatherUpdateClient (2)Or Makefile: 12345678910111213.PHONY : clean all CC = g++ BIN = weatherUpdateServer weatherUpdateClientall: $(BIN)client: $(CC) $@.cpp -o $@ -lzmq -I./include -L./libserver: $(CC) $@.cpp -o $@ -lzmq -I./include -L./libclean: rm $(BIN) weatherUpdateServer: 1./weatherUpdateServer weatherUpdateClient: 1./weatherUpdateClient 运行结果： 管道模式（Parallel Pipeline）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// taskSink.cpp// Task sink in C++// Binds PULL socket to tcp://localhost:5558// Collects results from workers via that socket//// Olivier Chamoux &lt;olivier.chamoux@fr.thalesgroup.com&gt;//#include &lt;zmq.hpp&gt;#include &lt;time.h&gt;#include &lt;sys/time.h&gt;#include &lt;iostream&gt;int main (int argc, char *argv[])&#123; // Prepare our context and socket zmq::context_t context(1); zmq::socket_t receiver(context,ZMQ_PULL); receiver.bind("tcp://*:5558"); // Wait for start of batch zmq::message_t message; receiver.recv(&amp;message); // Start our clock now struct timeval tstart; gettimeofday (&amp;tstart, NULL); // Process 100 confirmations int task_nbr; int total_msec = 0; // Total calculated cost in msecs for (task_nbr = 0; task_nbr &lt; 100; task_nbr++) &#123; receiver.recv(&amp;message); if ((task_nbr / 10) * 10 == task_nbr) std::cout &lt;&lt; ":" &lt;&lt; std::flush; else std::cout &lt;&lt; "." &lt;&lt; std::flush; &#125; // Calculate and report duration of batch struct timeval tend, tdiff; gettimeofday (&amp;tend, NULL); if (tend.tv_usec &lt; tstart.tv_usec) &#123; tdiff.tv_sec = tend.tv_sec - tstart.tv_sec - 1; tdiff.tv_usec = 1000000 + tend.tv_usec - tstart.tv_usec; &#125; else &#123; tdiff.tv_sec = tend.tv_sec - tstart.tv_sec; tdiff.tv_usec = tend.tv_usec - tstart.tv_usec; &#125; total_msec = tdiff.tv_sec * 1000 + tdiff.tv_usec / 1000; std::cout &lt;&lt; "\nTotal elapsed time: " &lt;&lt; total_msec &lt;&lt; " msec\n" &lt;&lt; std::endl; return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// taskWorker.cpp// Task worker in C++// Connects PULL socket to tcp://localhost:5557// Collects workloads from ventilator via that socket// Connects PUSH socket to tcp://localhost:5558// Sends results to sink via that socket//// Olivier Chamoux &lt;olivier.chamoux@fr.thalesgroup.com&gt;//#include "zhelpers.hpp"#include &lt;string&gt;int main (int argc, char *argv[])&#123; zmq::context_t context(1); // Socket to receive messages on zmq::socket_t receiver(context, ZMQ_PULL); receiver.connect("tcp://localhost:5557"); // Socket to send messages to zmq::socket_t sender(context, ZMQ_PUSH); sender.connect("tcp://localhost:5558"); // Process tasks forever while (1) &#123; zmq::message_t message; int workload; // Workload in msecs receiver.recv(&amp;message); std::string smessage(static_cast&lt;char*&gt;(message.data()), message.size()); std::istringstream iss(smessage); iss &gt;&gt; workload; // Do the work s_sleep(workload); // Send results to sink message.rebuild(); sender.send(message); // Simple progress indicator for the viewer std::cout &lt;&lt; "." &lt;&lt; std::flush; &#125; return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// taskVentilator.cpp// Task ventilator in C++// Binds PUSH socket to tcp://localhost:5557// Sends batch of tasks to workers via that socket//// Olivier Chamoux &lt;olivier.chamoux@fr.thalesgroup.com&gt;//#include &lt;zmq.hpp&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;iostream&gt;#define within(num) (int) ((float) num * random () / (RAND_MAX + 1.0))int main (int argc, char *argv[])&#123; zmq::context_t context (1); // Socket to send messages on zmq::socket_t sender(context, ZMQ_PUSH); sender.bind("tcp://*:5557"); std::cout &lt;&lt; "Press Enter when the workers are ready: " &lt;&lt; std::endl; getchar (); std::cout &lt;&lt; "Sending tasks to workers…\n" &lt;&lt; std::endl; // The first message is "0" and signals start of batch zmq::socket_t sink(context, ZMQ_PUSH); sink.connect("tcp://localhost:5558"); zmq::message_t message(2); memcpy(message.data(), "0", 1); sink.send(message); // Initialize random number generator srandom ((unsigned) time (NULL)); // Send 100 tasks int task_nbr; int total_msec = 0; // Total expected cost in msecs for (task_nbr = 0; task_nbr &lt; 100; task_nbr++) &#123; int workload; // Random workload from 1 to 100msecs workload = within (100) + 1; total_msec += workload; message.rebuild(10); memset(message.data(), '\0', 10); sprintf ((char *) message.data(), "%d", workload); sender.send(message); &#125; std::cout &lt;&lt; "Total expected cost: " &lt;&lt; total_msec &lt;&lt; " msec" &lt;&lt; std::endl; sleep (1); // Give 0MQ time to deliver return 0;&#125; 编译运行：(1)taskSink: 12g++ -o taskSink taskSink.cpp./taskSink taskWorker: 12g++ -o taskWorker taskWorker.cpp./taskWorker taskVentilator: 12g++ -o taskVentilator taskVentilator.cpp./taskVentilator (2)Or Makefile: 123456789101112131415.PHONY : clean all CC = g++ BIN = taskSink taskWorker taskVentilatorall: $(BIN)taskSink: $(CC) $@.cpp -o $@ -lzmq -I./include -L./libtaskWorker: $(CC) $@.cpp -o $@ -lzmq -I./include -L./libtaskVentilator: $(CC) $@.cpp -o $@ -lzmq -I./include -L./libclean: rm $(BIN) taskSink: 1./taskSink taskWorker: 1./taskWorker taskVentilator: 1./taskVentilator 运行结果：]]></content>
      <categories>
        <category>Distributed</category>
      </categories>
      <tags>
        <tag>Distributed</tag>
        <tag>ZeroMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（一）ZeroMQ概述初探]]></title>
    <url>%2F2018%2F09%2F18%2F%EF%BC%88%E4%B8%80%EF%BC%89ZeroMQ%E6%A6%82%E8%BF%B0%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[ZeroMQ概述ZeroMQ是一种基于消息队列的多线程网络库，其对套接字类型、连接处理、帧、甚至路由的底层细节进行抽象，提供跨越多种传输协议的套接字。ZeroMQ是网络通信中新的一层，介于应用层和传输层之间（按照TCP/IP划分），其是一个可伸缩层，可并行运行，分散在分布式系统间。它有比 tcp协议更高一级的协议。 ZeroMQ不是单独的服务或者程序，仅仅是一套组件，其封装了网络通信、消息队列、线程调度等功能，向上层提供简洁的API，应用程序通过加载库文件，调用API函数来实现高性能网络通信。 特点 支持多种通信协议，可以灵活地适应多种通信环境，包括进程内、进程间、机器间、广播。 支持多种消息模型，消息模型之间可以相互组合，形成特定的解决方案。 仅仅提供24个API接口，风格类似于BSD Socket。 处理了网络异常，包括连接异常中断、重连等。 改变TCP基于字节流收发数据的方式，处理了粘包、半包等问题，以msg为单位收发数据，结合Protocol Buffers，可以对应用层彻底屏蔽网络通信层。 对大数据通过SENDMORE/RECVMORE提供分包收发机制。 通过线程间数据流动来保证同一时刻任何数据都只会被一个线程持有，以此实现多线程的“去锁化”。 通过高水位HWM来控制流量，用交换SWAP来转储内存数据，弥补HWM丢失数据的缺陷。 服务器端和客户端的启动没有先后顺序。 支持Linux、Windows、OS X等。 可以绑定C、C++、Java、.NET、Python等30多种开发语言。 系统架构总体架构ZeroMQ几乎所有的I/O操作都是异步的，主线程不会被阻塞。ZeroMQ会根据用户调用zmq_init函数时传入的接口参数，创建对应数量的I/O Thread。每个I/O Thread都有与之绑定的Poller，Poller采用经典的Reactor模式实现，Poller根据不同操作系统平台使用不同的网络I/O模型（select、poll、epoll、devpoll、kequeue等）。主线程与I/O线程通过Mail Box传递消息来进行通信。Server开始监听或者Client发起连接时，在主线程中创建zmq_connecter或zmq_listener，通过Mail Box发消息的形式将其绑定到I/O线程，I/O线程会把zmq_connecter或zmq_listener添加到Poller中用以侦听读/写事件。Server与Client在第一次通信时，会创建zmq_init来发送identity，用以进行认证。认证结束后，双方会为此次连接创建Session，以后双方就通过Session进行通信。每个Session都会关联到相应的读/写管道， 主线程收发消息只是分别从管道中读/写数据。Session并不实际跟kernel交换I/O数据，而是通过plugin到Session中的Engine来与kernel交换I/O数据。 内部架构zmq的内部架构涉及到zmq全局变量的管理，并发模型，线程模型（主线程，i/o线程，回收线程，zmq_socket线程，存活于线程中的对象），对象回收模型（对象树），消息的组织等。 zmq基本流程 zmq消息模型ZeroMQ将消息通信分成4种模型，分别是 一对一结对模型（Exclusive-Pair） 请求回应模型（Request-Reply） 发布订阅模型（Publish-Subscribe） 推拉模型（Push-Pull） 这4种模型总结出了通用的网络通信模型，在实际中可以根据应用需要，组合其中的2种或多种模型来形成自己的解决方案。 （1）一对一结对模型最简单的1:1消息通信模型，可以认为是一个TCP Connection，但是TCP Server只能接受一个连接。数据可以双向流动，这点不同于后面的请求回应模型。 （2）请求回应模型（Request-Reply） 由请求端发起请求，然后等待回应端应答。一个请求必须对应一个回应，从请求端的角度来看是发-收配对，从回应端的角度是收-发对。跟一对一结对模型的区别在于请求端可以是1~N个。该模型主要用于远程调用及任务分配等。Echo服务就是这种经典模型的应用。 （3）发布订阅模型（ Publish-Subscribe） 发布端单向分发数据，且不关心是否把全部信息发送给订阅端。如果发布端开始发布信息时，订阅端尚未连接上来，则这些信息会被直接丢弃。订阅端未连接导致信息丢失的问题，可以通过与请求回应模型组合来解决。订阅端只负责接收，而不能反馈，且在订阅端消费速度慢于发布端的情况下，会在订阅端堆积数据。该模型主要用于数据分发。天气预报、微博明星粉丝可以应用这种经典模型。 （4）平行管道模式（Parallel Pipeline）(推拉模型) Server端作为Push端，而Client端作为Pull端，如果有多个Client端同时连接到Server端，则Server端会在内部做一个负载均衡，采用平均分配的算法，将所有消息均衡发布到Client端上。与发布订阅模型相比，推拉模型在没有消费者的情况下，发布的消息不会被消耗掉；在消费者能力不够的情况下，能够提供多消费者并行消费解决方案。该模型主要用于多任务并行。 zmq对象交互zmq_socket以ZMQ_REQ模式实例化，用以进行tcp通信： 12void *requester = zmq_socket (context, ZMQ_REQ);zmq_connect (requester, "tcp://localhost:5555"); zmq类层次 ① object_t，主要用于发送命令和处理命令，所有继承object_t的子类都具备该类的功能 ② io_thread_t，内含一个poller，可监听句柄的读、写、异常状态，继承自object_t，具有接收命令、处理命令、发送命令的功能 ③ io_object_t，可以获取一个io_thread_t的poller，从而具备poller功能，所有继承自该类的子类都具有pollere功能，可监听句柄的读、写、异常状态 ④ reaper_t，zmq的回收线程 ⑤ own_t，zmq的对象树结点，或者说多叉树的结点，其主要用于对象的销毁，可以想到，对象的销毁就是这棵树的销毁过程，必须要使用深度优先的算法来销毁。关于zmq对象树在Internal Architecture of libzmq有详细讲解 ⑥ tcp_connector_t，zmq_socket的连接器，使用她来建立tcp连接 ⑦ tcp_listener_t，zmq_socket的监听器 ⑧ stream_engine，负责处理io事件中的一种—-网络事件，把网络字节流转换成zeromq的msg_t消息传递给session_base_t。另外一些和版本兼容相关的杂务也stream_engine处理的。stream_engine_t处理完杂务，到session_base_t就只看见msg_t了。 ⑨ session_base_t，管理zmq_socket的连接和通信，主要与engine进行交换 ⑩ socket_base_t，zeromq的socket，在zmq中，被当成一种特殊的”线程“，具有收发命令的功能 应用场景应用ZeroMQ的Push-Pull模型实现联众游戏服务器的“热插拔”、负载均衡和消息派发。按照如图8部署服务器，Push端充当Gateway，作为一组游戏服务器集群最上层的一个Proxy，起负载均衡的作用，所有Gameserver作为Pull端。当一个请求到达Push端（Gateway）时，Push端根据一定的分配策略将任务派发到Pull端（Gameserver）。以联众某款游戏A为例，游戏A刚上线时，预计最大同时在线人数是10W，单台Gameserver并发处理能力为1W，需要10台Gameserver，由于游戏A可玩性非常好，半个月后最大同时在线人数暴增到50W，那么不需要在某天的凌晨将Gateway和Gameserver停机，只需要随时在机房新添加40台Gameserver，启动并连接到Gateway即可。 ZeroMQ中对Client和Server的启动顺序没有要求，Gameserver之间如果需要通信的话，Gameserver的应用层不需要管理这些细节，ZeroMQ已经做了重连处理。 对比 libevent封装了对网络I/O、信号、定时器等的处理，可以基于它之上做网络层的开发。 ACE封装了不同平台下的系统调用，也提供好几种网络编程的模型。 然而，zeromq不是libevent，也不是ACE,因为它的主要特性是：面向消息进行通信。所以，它提供的是比libevent，ACE处在网络通信中更高一层的组件。]]></content>
      <categories>
        <category>Distributed</category>
      </categories>
      <tags>
        <tag>Distributed</tag>
        <tag>ZeroMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群搭建配置实例]]></title>
    <url>%2F2018%2F09%2F16%2FHadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E9%85%8D%E7%BD%AE%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[重要的放在前面：遇到问题，看Logs很重要 环境 三台CenOS6.10 X86_64 uname -acat /etc/redhat-release jdk1.8.0_181 hadoop-2.8.4 分配IP地址及相应的角色： 192.168.31.100 - master namenode jobtracker-master 192.168.31.101 - slave1 datanode tasktracker-slave1 192.168.31.102 - slave2 datanode tasktracker-slave2 配置1. 网卡配置 vim /etc/sysconfig/network-scripts/ifcfg-enp0s3 (ifcfg-exxxxxx) 需要修改的是： 12345678NM_CONTROLLED=noONBOOT=yesBOOTPROTO=noneIPADDR=192.168.31.100NETMASK=255.255.255.0GATEWAY=192.168.31.1DNS1=8.8.8.8DNS2=114.114.114.114 chkconfig network on # 网络服务进程network配置为开机即启动service network restart # 重启网络服务进程 关闭防火墙Centos6 123sudo service iptables stop --临时关闭防火墙sudo chkconfig iptables off --永久关闭防火墙sudo service iptables status --查看防火墙状态 iptables: Firewall is not running. Centos7 123sudo systemctl stop firewalld.service #停止firewallsudo systemctl disable firewalld.service #禁止firewall开机启动sudo firewall-cmd --state #查看默认防火墙状态（关闭后显示notrunning，开启后显示running） ping网络其他主机测试网络是否设置好。 2. 三台主机均安装JDK，并设置环境变量https://yangjiannr.github.io/2017/12/25/Hadoop%E7%8E%AF%E5%A2%83%E6%90%AD%E8%BD%BD/ tar -zxvf jdk-8u181-linux-x64.tar.gz -C ~/app/lib/tar -zxvf hadoop-2.8.4.tar.gz -C ../app/lib/ sudo vim ~/.bash_profile 或者 vim /etc/profile尾部添加 123export JAVA_HOME=/home/hadoop/app/lib/jdk1.8.0_181export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=&quot;.:$JAVA_HOME/lib:$CLASSPATH&quot; 使配置文件生效 source /etc/profile 或者 . /etc/profile source .~/.bash_profile 或者 . .~/.bash_profile 验证： java -version 3. 三台主机分别设置/etc/hosts及/etc/hostname vim /etc/hosts hosts中添加 123192.168.31.100 master192.168.31.101 slave1192.168.31.102 slave2 修改/etc/hostname文件 vim /etc/hostname在master中，输入：master 在slave1中，输入：slave1在slave2中，输入：slave2 重启三台机器，输入命令：sudo reboot 4. 安装OpenSSH，并配置SSH可以免密登录三台主机分别执行： ssh-keygen -t rsacp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys 在master主机的终端中输入命令： cd ~/.sshscp authorized_keys slave1:~/.ssh/scp authorized_keys slave2:~/.ssh/ 查看是否可以从Master主机免密码登录Slave，输入命令： ssh slave1ssh slave2 5. 安装Hadoop，并设置环境变量https://yangjiannr.github.io/2017/12/25/Hadoop%E7%8E%AF%E5%A2%83%E6%90%AD%E8%BD%BD/ tar -zxvf hadoop-2.8.4.tar.gz -C ../app/lib/ 如果是linux系统下则可以 rm -rf .cmd，删除cmd文件`find . -name ‘.cmd’ -type f -print -exec rm -rf {} \;` 6. 配置三台主机的Hadoop文件 编辑hadoop-env.sh文件 vim hadoop-2.8.4/etc/hadoop/hadoop-env.sh修改hadoop中Java路径export JAVA_HOME=/home/hadoop/app/lib/jdk1.8.0_181/ 编辑core-site.xml vim hadoop-2.8.4/etc/hadoop/core-site.xml 三台主机上的core-site.xml文件： 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 编辑hdfs-site.xml 三台主机上的hdfs-site.xml文件： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 编辑yarn-site.xml 三台主机上的yarn-site.xml文件： 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;description&gt;The hostname of the RM.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 编辑mapred-site.xml 三台主机上的mapred-site.xml文件： 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;master:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 编辑masters vim masters 输入内容：master 编辑slaves vim slaves 输入内容：slave1 `slave2` 7. 启动Hadoop 关闭三台主机的防火墙，在三台主机中输入命令：service iptables stop 格式化NameNode，在master中/home/hadoop/app/lib/hadoop-2.8.4/bin目录下输入命令：./hadoop namenode -format 启动Hadoop，在master中/home/hadoop/app/lib/hadoop-2.8.4/sbin目录下输入命令：./start-all.sh 8. 使用jps检验各后台进程是否成功启动 在主节点master上查看namenode、SecondaryNameNode、ResourceManager进程是否启动，如图： 在slave1和slave2节点查看DataNode、NodeManager进程是否启动，如图： 9. 通过网站（Web UI）查看集群情况 在浏览器中输入:http://192.168.31.100:50070 ，网址为master节点所对应的IP，如图： 在浏览器中输入:http://192.168.31.100:8088 ，网址为master节点所对应的IP，如图： 运行实例（1）下载英文小说txt: （2）上传到集群 hadoop fs -mkdir /testhadoop fs -put data.txt /test/ （3）运行Hadoop的woedcount样例程序（向YARN提交作业） 参数中指定jar执行文件、输入数据目录（需要先创建好，并将待处理文本上传至其中）、输出目录（无需创建，由样例程序自己生成）。 hadoop jar hadoop-2.8.4/share/hadoop/mapreduce/hadoop-mapreduce-examples.jar wordcount /test/data.txt /result (4) 查看运行结果 hadoop fs -ls /result hadoop fs -cat /result/part-r-00000]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
        <tag>Hadoop</tag>
        <tag>集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux设置静态IP(固定IP)]]></title>
    <url>%2F2018%2F09%2F15%2FLinux%E8%AE%BE%E7%BD%AE%E9%9D%99%E6%80%81IP(%E5%9B%BA%E5%AE%9AIP)%2F</url>
    <content type="text"><![CDATA[切换成root用户： # su# 输入密码 修改网络配置文件： # cd /etc/sysconfig/network-scripts/# vim ifcfg-enp0s3 修改配置如下： 12345678910111213141516171819TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noNETWORKING_IPV6=noIPV6INIT=noIPV6_AUTOCONF=noIPV6_DEFROUTE=noIPV6_FAILURE_FATAL=noNAME=enp0s3UUID=75026fc8-e226-4754-9ae6-998a294c89feDEVICE=enp0s3ONBOOT=yesIPADDR=192.168.31.100NETMASK=255.255.255.0GATEWAY=192.168.31.1 设置DNS vim /etc/resolv.conf 重启网络 重新启动网络配置# service network restart或# /etc/init.d/network restart 查看修改后网络 ifconfig ping www.baidu.com 注： 如果是用ssh远程修改的，需要用新的IP重新进行ssh远程连接]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop参数配置 详解]]></title>
    <url>%2F2018%2F08%2F31%2FHadoop%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%20%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[hadoop doc： http://hadoop.apache.org/docs/current/ 配置文件介绍Hadoop的配置文件分为两类： 只读类型的默认文件： src/core/core-default.xml src/hdfs/hdfs-default.xml src/mapred/mapred-default.xml conf/mapred-queues.xml 定位(site-specific)设置： conf/core-site.xml conf/hdfs-site.xml conf/mapred-site.xml conf/mapred-queues.xml 除此之外，也可以通过设置conf/Hadoop-env.sh来为Hadoop的守护进程设置环境变量(在bin/文件夹内）。 hadoop-env.sh中比较重要的环境变量： 参数名 默认值 说明 JAVA_HOME 无 用于确定Hadoop使用的JDK，需要正确设置，否则无法启动Hadoop。 HADOOP_LOG_DIR \${HADOOP_HOME}/logs 守护进程的日志目录，默认值是\${HADOOP_HOME}/logs HADOOP_NAMENODE_OPTS 无 HDFS的NameNode的Java虚拟机选项，默认为空。对于规模较大的集群（二十个节点以上），可以通过这个选项增大NameNode进程的堆内存（例如-Xmx2048M）。 hadoop是通过org.apache.hadoop.conf.configuration来读取配置文件的。在Hadoop的设置中，Hadoop的配置是通过资源（resource）定位的，每个资源由一系列name/value对以XML文件的形式构成，它以一个字符串命名或以Hadoop定义的Path类命名（这个类是用于定义文件系统内的文件或文件夹的）。如果是以字符串命名的，Hadoop会通过classpath调用此文件。如果以Path类命名，那么Hadoop会直接在本地文件系统中搜索文件。 配置文件名 配置对象 主要内容 core-site.xml 集群全局参数 用于定义系统级别的参数，如HDFS URL、Hadoop的临时目录等 hdfs-site.xml HDFS参数 如名称节点和数据节点的存放位置、文件副本的个数、文件读取权限等 mapred-site.xml Mapreduce参数 包括JobHistory Server和应用程序参数两部分，如reduce任务的默认个数、任务所能够使用内存的默认上下限等 yarn-site.xml 集群资源管理系统参数 配置 ResourceManager，NodeManager 的通信端口，web监控端口等 core-site.xml 参数名 默认值 参数解释 fs.defaultFS file:/// 文件系统主机和端口 io.file.buffer.size 4096 流文件的缓冲区大小 hadoop.tmp.dir /tmp/hadoop-${user.name} 临时文件夹 hdfs-site.xmlHDFS参数配置 序号 参数名 默认值 参数解释 1 dfs.namenode.secondary.http-address 0.0.0.0:50090 定义HDFS对应的HTTP服务器地址和端口 2 dfs.namenode.name.dir file://${hadoop.tmp.dir}/dfs/name 定义DFS的名称节点在本地文件系统的位置 3 dfs.datanode.data.dir file://${hadoop.tmp.dir}/dfs/data 定义DFS数据节点存储数据块时存储在本地文件系统的位置 4 dfs.replication 3 缺省的块复制数量 5 dfs.webhdfs.enabled true 是否通过http协议读取hdfs文件，如果选是，则集群安全性较差 6 dfs.blocksize 64M 大型的文件系统HDFS块大小为64M 7 dfs.namenode.handler.count 100 设置更多的namenode线程，处理从 datanode发出的大量RPC请求 HDFS常用端口 组件 节点 默认端口 配置 用途说明 HDFS DataNode 50010 dfs.datanote.address datanote服务端口，用于数据传输 HDFS DataNode 50075 dfs.datanote.http.address http服务的端口 HDFS DataNode 50475 dfs.datanote.https.address https服务的端口 HDFS DataNode 50020 dfs.datanote.ipc.address IPC服务的端口 HDFS NameNode 50070 dfs.namenode.http-address http服务的端口 HDFS NameNode 50470 dfs.namenode.https-address https服务的端口 HDFS NameNode 8020 fs.defaultFs 接收Client连接的RPC端口，用于获取文件系统metadata信息 HDFS JournalNode 8485 dfs.journalnode.rpc-address RPC服务 HDFS JournalNode 8480 dfs.journalnode.http-address HTTP服务 HDFS ZKFC 8019 dfs.ha.zkfc.port ZooKeeper Failover Controller，用于NN HA yarn-site.xmlYARN常用端口 组件 节点 默认端口 配置 用途说明 YARN ResourceManager 8032 yarn.resourcemanager.address RM的applications manager（ASM）端口，ResourceManager对客户端暴露的地址。客户端通过该地址向RM提交应用程序，杀死应用程序等 YARN ResourceManager 8030 yarn.resourcemanager.scheduler.address scheduler组件的IPC端口，ResourceManager对ApplicationMaser暴露的访问地址。ApplicationMaster通过该地址向RM申请资源、释放资源等。 YARN ResourceManager 8031 yarn.resourcemanager.resource-tracker.address IPCResourceManager对NodeManager暴露的地址。NodeManager通过该地址向RM汇报心跳，领取任务等。 YARN ResourceManager 8033 yarn.resourcemanager.admin.address IPCResourceManager 对管理员暴露的访问地址。管理员通过该地址向RM发送管理命令等。 YARN ResourceManager 8088 yarn.resourcemanager.webapp.address http服务端口ResourceManager对外web ui地址。用户可以通过该地址在浏览器中查看集群各类信息 YARN NodeManager 8040 yarn.nodemanager.localizer.address localizer IPC YARN NodeManager 8042 yarn.nodemanager.webapp.address http服务端口 YARN NodeManager 8041 yarn.nodemanager.address NodeManager中container manager的端口 YARN JobHistory Server 10020 mapreduce.jobhistory.address IPC YARN JobHistory Server 19888 mapreduce.jobhistory.webapp.address http服务端口 YARN参数配置 组件 节点 缺省值 配置 用途说明 YARN NodeManager 8192 yarn.nodemanager.resource.memory-mb NodeManager总的可用物理内存。注意，该参数是不可修改的，一旦设置，整个运行过程中不 可动态修改。另外，该参数的默认值是8192MB，即使你的机器内存不够8192MB，YARN也会按照这些内存来使用，因此，这个值通过一 定要配置。不过，Apache已经正在尝试将该参数做成可动态修改的。 YARN NodeManager 2.1 yarn.nodemanager.vmem-pmem-ratio 每使用1MB物理内存，最多可用的虚拟内存数。 YARN NodeManager 8 yarn.nodemanager.resource.cpu-vcores NodeManager总的可用虚拟CPU个数。 YARN NodeManager ${hadoop.tmp.dir}/nm-local-dir yarn.nodemanager.local-dirs 中间结果存放位置，类似于1.0中的mapred.local.dir。注意，这个参数通常会配置多个目录，已分摊磁盘IO负载。 YARN NodeManager 10800（3小时） yarn.nodemanager.log.retain-seconds NodeManager上日志最多存放时间（不启用日志聚集功能时有效）。 YARN NodeManager yarn.nodemanager.aux-services NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序 YARN NodeManager 1024/8192 yarn.scheduler.minimum-allocation-mb/ yarn.scheduler.maximum-allocation-mb 单个可申请的最小/最大内存资源量。比如设置为1024和3072，则运行MapRedce作业时，每个Task最少可申请1024MB内存，最多可申请3072MB内存。 YARN NodeManager yarn.scheduler.minimum-allocation-vcores / yarn.scheduler.maximum-allocation-vcores 单个可申请的最小/最大虚拟CPU个数。比如设置为1和4，则运行MapRedce作业时，每个Task最少可申请1个虚拟CPU，最多可申请4个虚拟CPU。什么是虚拟CPU,查看：http://dongxicheng.org/mapreduce-nextgen/yarnmrv2-resource-manager-resource-manager/ mapred-site.xmlMapReduce参数配置 参数名称 缺省值 说明 mapreduce.framework.name local 取值local、classic或yarn其中之一，如果不是yarn，则不会使用YARN集群来实现资源的分配 mapreduce.jobhistory.address 0.0.0.0:10020 定义历史服务器的地址和端口，通过历史服务器查看已经运行完的MapReduce作业记录 mapreduce.jobhistory.webapp.address 0.0.0.0:19888 定义历史服务器web应用访问的地址和端口 mapreduce.job.name 作业名称 mapreduce.job.priority NORMAL 作业优先级 yarn.app.mapreduce.am.resource.mb 1536 MR ApplicationMaster占用的内存量 yarn.app.mapreduce.am.resource.cpu-vcores 1 MR ApplicationMaster占用的虚拟CPU个数 mapreduce.am.max-attempts 2 MR ApplicationMaster最大失败尝试次数 mapreduce.map.memory.mb 1024 每个Map Task需要的内存量 mapreduce.map.cpu.vcores 1 每个Map Task需要的虚拟CPU个数 mapreduce.map.maxattempts 4 Map Task最大失败尝试次数 mapreduce.reduce.memory.mb 1024 每个Reduce Task需要的内存量 mapreduce.reduce.cpu.vcores 1 每个Reduce Task需要的虚拟CPU个数 mapreduce.reduce.maxattempts 4 Reduce Task最大失败尝试次数 mapreduce.map.speculative false 是否对Map Task启用推测执行机制 mapreduce.reduce.speculative false 是否对Reduce Task启用推测执行机制 mapreduce.job.queuename default 作业提交到的队列 mapreduce.task.io.sort.mb 100 任务内部排序缓冲区大小 mapreduce.map.sort.spill.percent 0.8 Map阶段溢写文件的阈值（排序缓冲区大小的百分比） mapreduce.reduce.shuffle.parallelcopies 5 Reduce Task启动的并发拷贝数据的线程数目 mapred-queues.xml 标签或属性 （Tag/Attribute） 值（value） 是否可刷新 Queues 配置文件的根元素 无意义 AclsEnabled 布尔类型标签的属性，表示存取控制列表是否支持控制Job的提交及所有queue的管理 是 queue 的子元素，定义系统中的queue 无意义 Name 的子元素，代表名字 否 State 的子元素，代表queue的状态 是 Acl-submit-job 的子元素，定义一个能提交Job到该queue的用户或组的名单列表 是 Acl-administrator-job 的子元素，定义一个能更改Job的优先级或能杀死已提交到该queue的Job用户或组的名单列表 是 properties 的子元素，定义优先调度规则 无意义 Property 的子元素 无意义 Key 的子元素 调度程序指定 Value 的属性 调度程序指定 其他常用端口HBase常用端口 组件 节点 默认端口 配置 用途说明 HBase Master 60000 hbase.master.port IPC HBase Master 60010 hbase.master.info.port http服务端口 HBase RegionServer 60020 hbase.regionserver.port IPC HBase RegionServer 60030 hbase.regionserver.info.port http服务端口 HBase HQuorumPeer 2181 hbase.zookeeper.property.clientPort HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。 HBase HQuorumPeer 2888 hbase.zookeeper.peerport HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。 HBase HQuorumPeer 3888 hbase.zookeeper.leaderport HBase-managed ZK mode，使用独立的ZooKeeper集群则不会启用该端口。 Hive常用端口 组件 节点 默认端口 配置 用途说明 Hive Metastore 9083 /etc/default/hive-metastore中export PORT=来更新默认端口 Hive HiveServer 10000 /etc/hive/conf/hive-env.sh中export HIVE_SERVER2_THRIFT_PORT=来更新默认端口 ZooKeep常用端口 组件 节点 默认端口 配置 用途说明 ZooKeeper Server 2181 /etc/zookeeper/conf/zoo.cfg中clientPort= 对客户端提供服务的端口 ZooKeeper Server 2888 /etc/zookeeper/conf/zoo.cfg中server.x=[hostname]:nnnnn[:nnnnn]，标蓝部分 follower用来连接到leader，只在leader上监听该端口。 ZooKeeper Server 3888 /etc/zookeeper/conf/zoo.cfg中server.x=[hostname]:nnnnn[:nnnnn]，标蓝部分 用于leader选举的。只在electionAlg是1,2或3(默认)时需要。]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>HDFS</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[eclipse代码补全按键修改成Tab]]></title>
    <url>%2F2018%2F01%2F07%2Feclipse%E4%BB%A3%E7%A0%81%E8%A1%A5%E5%85%A8%E6%8C%89%E9%94%AE%E4%BF%AE%E6%94%B9%E6%88%90Tab%2F</url>
    <content type="text"><![CDATA[https://www.eclipse.org/downloads/compare.php?release=oxygen 下载eclipse带有源文件的版本 打开Eclipse，点击 window -&gt; show view -&gt; other，搜索并选择 Plug-ins； 在其中找到org.eclipse.jface.text，右键单击导入源文件项目，import as －&gt; Source Project， 然后你就会在workspace中看到此项目 找到verifykey函数 将其中的代码： 1234567891011121314151617181920212223242526272829// key != 0 switch (key) &#123; case 0x1B: // Esc e.doit= false; hide(); break; case &apos;\n&apos;: // Ctrl-Enter on w2k case &apos;\r&apos;: // Enter e.doit= false; insertSelectedProposalWithMask(e.stateMask); break; case &apos;\t&apos;: e.doit= false; fProposalShell.setFocus(); return false; default: ICompletionProposal p= getSelectedProposal(); if (p instanceof ICompletionProposalExtension) &#123; ICompletionProposalExtension t= (ICompletionProposalExtension) p; char[] triggers= t.getTriggerCharacters(); if (contains(triggers, key)) &#123; e.doit= false; hide(); insertProposal(p, key, e.stateMask, fContentAssistSubjectControlAdapter.getSelectedRange().x); &#125; &#125; &#125; return true;&#125; 修改为如下代码： 1234567891011121314151617181920212223242526272829// key != 0 switch (key) &#123; case 0x1B: // Esc e.doit= false; case &apos; &apos;: // 空格、分号、等于号直接上屏不弹出代码提示 case &apos;;&apos;: case &apos;=&apos;: hide(); break; case &apos;\n&apos;: // Ctrl-Enter on w2k case &apos;\r&apos;: // Enter case &apos;\t&apos;: // TAB键上屏 e.doit= false; insertSelectedProposalWithMask(e.stateMask); break; default: ICompletionProposal p= getSelectedProposal(); if (p instanceof ICompletionProposalExtension) &#123; ICompletionProposalExtension t= (ICompletionProposalExtension) p; char[] triggers= t.getTriggerCharacters(); if (contains(triggers, key)) &#123; e.doit= false; hide(); insertProposal(p, key, e.stateMask, fContentAssistSubjectControlAdapter.getSelectedRange().x); &#125; &#125; &#125; return true;&#125; 导出修改后的插件，右键点击org.eclipse.jface.text项目，选择 Export－&gt; Deployable plugins and fragments，点击 Next，在 Destination 中的 Directory，选择一个要保存插件的目录，然后 Finish。就会在你所选的目录下产生一个新的 plugins 目录，里面有一个jar文件，用它替换掉你使用的 Eclipse 目录下的 plugins 里面对应的org.eclipse.jface.text_*.jar，各个版本的版本号不太一样，替换之后重启 Eclipse，最好 “-clean” 一下。 代码提示设置：增加Eclipse的提示功能 在Eclipse中，从Window -&gt; preferences -&gt; Java -&gt; Editor -&gt; Content assist -&gt; Auto-Activation下，我们可以在”.”号后面加入我们需要自动提示的首字幕，比如”abc”，可以把26个字母的大小写全部加上(.abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_(@）]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>eclipse</tag>
        <tag>Java</tag>
        <tag>IDE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop环境搭载]]></title>
    <url>%2F2017%2F12%2F25%2FHadoop%E7%8E%AF%E5%A2%83%E6%90%AD%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[官网安装教程：http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.13.0/hadoop-project-dist/hadoop-common/SingleCluster.html 环境本教程使用 CentOS 7 64位 作为系统环境，请自行安装系统。装好了 CentOS 系统之后，在安装 Hadoop 前还需要做一些必备工作。 创建hadoop用户如果你安装 CentOS 的时候不是用的 “hadoop” 用户，那么需要增加一个名为 hadoop 的用户。 123456su # 上述提到的以 root 用户登录useradd -m hadoop -s /bin/bash # 创建新用户hadooppasswd hadoop # 修改密码，按提示输入两次密码 visudo 或 vim /etc/sudoers# 找到 root ALL=(ALL) ALL 这行# 然后在这行下面增加一行内容：hadoop ALL=(ALL) ALL （当中的间隔为tab） 免密登录设置sudo yum install sshsudo yum install openssh-clientssudo yum install openssh-server vi /etc/ssh/sshd_config 找到以下内容，并去掉注释符“#”RSAAuthentication yesPubbkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keys 重启sshd服务service sshd restart 或 /etc/init.d/sshd restart //免密登录ssh-keygen -t rsa //回车回车ll -a //生成了 .ssh文件夹，文件夹内有id_rsa 和 id_rsa.pub 两个文件 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 600 ~/.ssh/authorized_keys //测试是否成功ssh localhost // 无需密码则表示设置成功 关闭防火墙Centos6 123sudo service iptables stop --临时关闭防火墙sudo chkconfig iptables off --永久关闭防火墙sudo service iptables status --查看防火墙状态 iptables: Firewall is not running. Centos7 123sudo systemctl stop firewalld.service #停止firewallsudo systemctl disable firewalld.service #禁止firewall开机启动sudo firewall-cmd --state #查看默认防火墙状态（关闭后显示notrunning，开启后显示running） 从本机拷贝文件至远程主机jdk-9.0.1_linux-x64_bin.tar.gz hadoop-2.6.0-cdh5.7.0.tar.gz 远程主机（用户名hadoop）： [hadoop@localhost ~]$ mkdir -p app/lib [hadoop@localhost ~]$ mkdir package 本机，将需要的文件远程拷贝到远程主机中 scp jdk-9.0.1_linux-x64_bin.tar.gz hadoop@hadoop:/home/hadoop/package scp hadoop-2.6.0-cdh5.7.0.tar.gz hadoop@hadoop:/home/hadoop/package 安装Java环境远程主机 /home/hadoop/package : tar -zxvf jdk-9.0.1_linux-x64_bin.tar.gz -C ~/app/lib tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C ../app/lib/ sudo vim /etc/profile 或者 vim ~/.bash_profile尾部添加 123export JAVA_HOME=/home/hadoop/app/lib/jdk-9.0.1export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=&quot;.:$JAVA_HOME/lib:$CLASSPATH&quot; 使配置文件生效 . /etc/profile 或者 source /etc/profile . .~/.bash_profile 或者 source .~/.bash_profile 验证： echo $JAVA_HOME /home/hadoop/app/lib/jdk-9.0.1 java -version 123java version &quot;9.0.1&quot;Java(TM) SE Runtime Environment (build 9.0.1+11)Java HotSpot(TM) 64-Bit Server VM (build 9.0.1+11, mixed mode) Hadoop配置文件的修改 如果是linux系统下则可以 rm -rf .cmd，删除cmd文件[hadoop@localhost hadoop-2.6.0-cdh5.7.0]$ `find . -name ‘.cmd’ -type f -print -exec rm -rf {} \;` 在hadoop_home/etc/hadoop路径下 $ vim hadoop-env.sh找到export JAVA_HOME=${JAVA_HOME}，并且注释掉这一行，添加下面的一行（jdk的根目录） export JAVA_HOME=/home/hadoop/app/lib/jdk-9.0.1 etc/hadoop/core-site.xml: 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/hdfs-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; sudo vim /etc/profile 或者 vim ~/.bash_profile尾部添加 123456789export HADOOP_HOME=/home/hadoop/app/lib/hadoop-2.6.0-cdh5.7.0export PATH=$HADOOP/bin:$PATHexport HADOOP_INSTALL=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport YARN_HOME=$HADOOP_HOMEexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 使配置文件生效 . /etc/profile 或者 source /etc/profile . .~/.bash_profile 或者 source .~/.bash_profile 启动hdfs 格式化文件系统（仅第一次执行即可，不要重复执行）：./bin/hdfs namenode -format ./sbin/start-dfs.sh 可能会出现需要yes, 输入密码 验证是否成功 启动完成后，可以通过命令 jps 来判断是否成功启动，若成功启动则会列出如下进程: “NameNode”、”DataNode”和SecondaryNameNode（如果 SecondaryNameNode 没有启动，请运行 sbin/stop-dfs.sh 关闭进程，然后再次尝试启动尝试）。如果没有 NameNode 或 DataNode ，那就是配置不成功，请仔细检查之前步骤，或通过查看启动日志排查原因。$ jps 1425 NameNode 1702 SecondaryNameNode 1546 DataNode 1855 Jps 浏览器方式验证 http://localhost:50070/ （如果是远程主机则将localhost换成远程主机IP） 停止$ sbin/stop-dfs.sh]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[共识机制]]></title>
    <url>%2F2017%2F11%2F01%2F%E5%85%B1%E8%AF%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[区块链要成为一个难以攻破的、公开的、不可篡改数据记录的去中心化诚实可信系统，需要在尽可能短的时间内做到分布式数据记录的安全、明确及不可逆，提供一个最坚实且去中心化的系统。在实践中，该流程分为两个方面： 一是选择一个独特的节点来产生一个区块二是使分布式数据记录不可逆 实现上述流程的技术核心就是：共识机制。共识机制是区块链节点就区块信息达成全网一致共识的机制，可以保证最新区块被准确添加至区块链、节点存储的区块链信息一致不分叉甚至可以抵御恶意攻击。 当前主流的共识机制包括：工作量证明、权益证明、工作量证明与权益证明混合、股份授权证明、瑞波共识协议等。 （一）工作量证明 工作量证明（Proof of Work，PoW），顾名思义，即指工作量的证明。PoW机制的基本步骤如下： 节点监听全网数据记录，通过基本合法性验证的数据记录将进行暂存； 节点消耗自身算力尝试不同的随机数，进行指定哈希计算，并不断重复该过程直至找到合理的随机数； 找到合理的随机数后，生成区块信息，首先输入区块头信息，然后是区块记录信息； 接着对外广播出新产生的区块，其他节点验证通过后，连接至区块链中，主链高度加一，然后所有节点切换至新区块后面继续进行工作量证明的区块产生。 PoW叫工作量证明体现在步骤2中，节点需要不断消耗算力工作，进行哈希计算，以找到期望的随机数。如果两个节点在同一时间找到区块，那么网络将根据后续节点和区块生成情况来确定哪个区块构建最终区块链。 其工作量主要体现在：一个符合要求的区块随机数由N个前导零构成，零的个数取决于网络的难度值。要得到合理的随机数需要经过大量尝试计算，计算时间取决于机器的哈希运算速度。当某个节点提供出一个合理的随机数值，说明该节点确实经过了大量的尝试计算。当然，这并不能得出计算次数的绝对值，因为寻找合理随机数值是一个概率事件。 挖矿有三个重要功能： 发行新的货币 维系系统的支付功能 通过算力保障系统安全 PoW机制存在两个方面明显缺陷： 算力的消耗与浪费。 算力集中化凸显。 （二）权益证明 PoS是一个根据持有货币的量和时间，进行利息分发和区块产生的机制。 币天：每个币每天产生1币天。 如果发现了一个新PoS区块，币天就会被清空为0。每被清空365币天，将会从区块中获得0.05个币的利息（可理解为年利率5%）。 未来币 每个区块链的备份都存放在未来币网络的每个节点里，而且在每个节点上没有加密的每个账户都能够生成区块，只要至少一个新入账户的交易已经确认了1440次。任何账户只要达到了这个标准就会被视为“激活账户”。在未来币里，每个区块都包含着255个交易，每个交易都是由包含识别参数的192字节的数据头开始的。一个区块里的每个交易都是由128个字节所代表着。总共加在一起就意味着最大的区块大小有32K字节。 每个区块都有一个“生成签名”的参数。激活账户用自己的私钥在原先的区块上签署“生成签名”。这就产生了一个64字节的签名，之后通过SHA256散列该签名。哈希产生的前八个字节给出了一个数字，作为一个“hit”。“hit”与目前的目标值相比较，如果计算出的“hit”值要比“目标值”低，那么就可以生成下一个区块了。 对于每个活动账户来讲，“目标值”都是与它自身所确认的余额成比例的。一个持有1000个币的账户得到的目标值是持有20个币账户所得到目标值的50倍。因此，拥有1000个币的持有者产生的区块数是持有20个币的人产生的50倍。同事，“目标值”并不是固定的，随着先前区块的时间戳的流逝时刻都在增长。如果在最初的一秒钟内没有哪个账户的“hit”值是低于“目标值”的，则下一秒钟“目标值”就会翻倍。“目标值”会连续的翻倍，直到一个活动账户的“hit”值有一个较低的数值。还有一个“基本目标”值，它以60秒的间隔设定为目标值。正是这个原因，一个区块平均产生的时间会在60秒。即使在网络上只有很少的激活账户，它们其中的一个最终会产生一个区块，因为“目标”值会变得相当大。通过将你账户的“hit”值与目前的“目标”值相比，你就可以估算出你的“hit”值还有多久能成功。 当一个激活账户赢得产生区块的权利时，就能将任何可获得的且未确认的交易放入区块中，并用所有需要的参数来填充该区块。然后，这个区块就会被传播到网络中作为一个区块链的备选。每一个区块中的负载值、“hit”、产生的账户以及签名都能被网络上接收到它的节点所确认。每个区块参考之前的区块，区块形成的区块链可以用来追溯和查询网络中所有的交易历史，所有这些都会追溯到创世区块。 （三）权益证明+工作量证明 用工作量证明机制PoW发行新币，用权益证明机制PoS维护网络安全，即PoW+PoS机制。该机制中，区块被分成两种形式——PoW区块及PoS区块。在这种新型区块链体系里，区块持有人可以消耗他的币天获得利息，同时获得为网络产生一个区块和用PoS造币的优先权。 在PoW+PoS机制下，只要持有币的人，不论持有的数量多少，都可以挖到数据块，而不用采用任何的矿池导致算力集中。同时，由于多采用币天生成区块，而不是算力，降低了资源消耗，解决了单纯PoW机制在维护网络安全方面先天不足的问题。 （四）股份授权证明 PoS机制使用一个确定性算法以随机选择一个股东来产生下一个区块，该算法中，账户余额决定了节点被选中的可能性。然而，该系统并未使区块链变得越来越安全而不可逆。同时PoS面临的挑战是如何通过及时而高效的方法达成共识。 为达到这个目标，每个持币节点可以将其投票权授予一名代表。获票数最多的前100位代表按既定时间表轮流产生区块。每名代表被分配到一个时间段生产区块。所有代表将收到等同于一个平均水平的区块所含交易费的1%作为报酬。如果一个平均水平的区块含有100股作为交易费，一名代表将获得1股作为报酬，即可大大提高共识效率。这是DPoS的核心思想。 在DPoS中， 第一步是称为一名代表，必须在网络上注册公钥，然后分配一个32位的特有标识符。然后该标识符会被每笔交易数据的“头部”引用。 第二步是授权选票。每个钱包有一个参数设置窗口，在该窗口里用户可以选择一个或更多的代表，并将其分级。一经设定，用户所做的每笔交易将把选票从“输入代表”转移至“输出代表”。 一般情况下，用户不会创建特别以投票为目的的交易，因为那将耗费他们一笔交易费。每个钱包将显示一个状态指示器，让用户知道代表的表现如何。如果某代表错过了太多的区块，那么系统将会推荐用户去换一个新的代表。如果任何代表被发现签发了一个无效的区块，那么所有标准钱包将在每个钱包进行更多交易前要求选出一个新代表。 （五）瑞波共识协议 瑞波共识协议（Ripple Consensus Protocol，RCP），使一组节点能够基于特殊节点列表达成共识。初始特殊节点列表就像一个俱乐部，要接纳一个新成员，必须由一定比例的该俱乐部会员投票通过。 RCP机制的工作原理如下： 验证节点接收存储待验证交易。首先验证节点接收待验证交易，将其存储在本地；其次本轮共识过程中新到的交易需要等待，在下次共识时再确认。 活跃信任节点发送提议：首先，信任节点列表是验证池的一个子集，其信任节点来源于验证池；其次，参与共识过程的信任节点须出于活跃状态，验证节点与信任节点间存在保活机制，长期不活跃节点将被从信任节点列表删除；最后，信任节点根据自身掌握的交易双方额度、交易历史等信息对交易做出判断，并加入到提议中进行发送。 本验证节点检查收到的提议是否来自信任节点列表中的合法信任节点，如果是，则存储；如果不是，则丢弃。 验证节点根据提议确定认可交易列表的步骤如下：首先，令信任节点列表中活跃的信任节点个数为M（比如5个），本轮中交易认可阔值为N（百分比，比如50%），则每一个超过M*N个信任节点认可的交易将被本验证节点认可；其次，本验证节点生成认可交易列表。系统为验证节点设置一个计时器，如果计时器时间已到，本信任节点需要发送自己的认可交易列表。 账本共识达成的步骤如下：首先，本验证节点仍然在接收来自信任节点列表中信任节点的提议，并持续更新认可交易列表；其次，验证节点认可列表的生成并不代表最终账本的形成以及共识的达成，账本共识只有在每笔交易都获得至少超过一定阔值（比如80%）的信任节点列表认可才能达成。如果账本中每笔交易都获得至少超过一定阔值的信任节点列表认可，则共识达成，交易验证结束，否则继续上述过程。 共识过程结束后，已经形成最新的账本，现将上轮剩余的待确认交易以及新交易纳入待确认交易列表，开始新一轮共识过程。 除上述机制外，还有恒星共识协议（Stellar Consensus Protocol，SCP）、改进型使用拜占庭容错机制（Practical Byzantine Fault Tolerance，PBFT）和Pool验证池机制等共识机制被提出，甚至已经应用在区块链系统中，不同共识机制各有其应用场景和优势。 ——《区块链-从数字货币到信用社会》 http://www.frankyang.cn/2017/11/01/consensus/]]></content>
      <categories>
        <category>BlockChain</category>
      </categories>
      <tags>
        <tag>BlockChain</tag>
        <tag>Distributed</tag>
        <tag>Consensus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比特币交易本质--UTXO(Unspent Transaction Output)]]></title>
    <url>%2F2017%2F10%2F02%2F%E6%AF%94%E7%89%B9%E5%B8%81%E4%BA%A4%E6%98%93%E6%9C%AC%E8%B4%A8--UTXO(Unspent%20Transaction%20Output)%2F</url>
    <content type="text"><![CDATA[UTXO 代表 Unspent Transaction Output。 Transaction 被简称为 TX，所以上面这个短语缩写为 UTXO。 现在的银行也好、信用卡也好、证券交易系统也好，互联网第三方支付系统也好，其核心都是基于账户（account based）的设计，由关系数据库支撑。 数据库要确保两点，第一是你要确保业务规则得到遵守，张三的余额充足。第二是确保事务性，也就是原子性、一致性、隔离性、持久性（ACID）。这种基于账户的设计，简单直观，而且在 IT 系统设计里用了几十年，应该说没有什么问题。 我们假设一个这样的场景：张三挖到12.5 枚比特币。过了几天，他把其中 2.5 枚支付给李四。又过了几天，他和李四各出资 2.5 比特币凑成 5 比特币付给王五。 如果是基于账户的设计，张、李、王三人在数据库中各有一个账户，则他们三人的账户变化如下图所示： 但在比特币中，这个过程是通过 UTXO 实现的，图示如下： 比特币的区块链账本里记录的是一笔又一笔的交易。 每笔交易都有若干交易输入，也就是资金来源，也都有若干笔交易输出，也就是资金去向。一般来说，每一笔交易都要花费（spend）一笔输入，产生一笔输出，而其所产生的输出，就是“未花费过的交易输出”，也就是 UTXO。 比特币交易遵守几个规则： 除了 coinbase 交易之外，所有的资金来源都必须来自前面某一个或者几个交易的 UTXO，就像接水管一样，一个接一个，此出彼入，此入彼出，生生不息，钱就在交易之间流动起来了。 任何一笔交易的交易输入总量必须等于交易输出总量，等式两边必须配平。 上图第一个交易#1001号交易是coinbase交易。比特币是矿工挖出来的。当一个矿机费尽九牛二虎之力找到一个合格的区块之后，它就获得一个特权，能够创造一个coinbase交易，在其中放入一笔新钱，并且在交易输出的收款人地址一栏，堂堂正正的写上自己的地址。这个coinbase交易随着张三挖出来的区块被各个节点接受，经过六个确认以后永远的烙印在历史中。 过了几天，张三打算付2.5个比特币给李四，张三就发起一#2001号交易，这个交易的资金来源项写着“#1001(1)”，也就是#1001号交易——张三挖出矿的那个coinbase交易——的第一项UTXO。然后在本交易的交易输出UTXO项中，把2.5个比特币的收款人地址设为李四的地址。 请注意，这一笔交易必须将前面产生那一项12.5个比特币的输出项全部消耗，而由于张三只打算付给李四2.5个比特币，为了要消耗剩下的10比特币，他只好把剩余的那10个比特币支付给自己，这样才能符合输入与输出配平的规则。 再过几天，张三和李四打算AA制合起来给王五付5枚比特币。那么张三或李四发起#3001号交易，在交易输入部分，有两个资金来源，分别是#2001(1)和#2001(2)，代表第#2001号交易的第(1)和第(2)项 UTXO。然后在这个交易的输出部分里如法炮制，给王五5比特币，把张三剩下的7.5比特币发还给自己。以后王五若要再花他这5比特币，就必须在他的交易里注明资金的来源是#3001(1)。 所以，其实并没有什么比特币，只有UTXO。当我们说张三拥有10枚比特币的时候，我实际上是说，当前区块链账本中，有若干笔交易的UTXO项收款人写的是张三的地址，而这些UTX 项的数额总和是10。因为在比特币系统里，一个人可以拥有的地址资源，可谓取之不尽用之不竭。要知道自己的一大堆地址里一共收了多少UTXO，人是算不过来的，需要由比特币钱包代为跟踪计算。 如果采用基于账户的方案，需要一个数据库。这个数据库能够让你很方便的查到张三、李四各自的账户余额。 而 UTXO 方案当然也需要一个数据库，这个数据库记录着当前系统里每一笔“没有花出去的交易输出”，也是就比特币。当节点接收到一笔交易的时候，它需要去 UTXO 数据库里查，看看这笔交易所引用的 UTXO 是否存在，它的收款人（拥有者）是不是当前新交易的付款者。而交易结束之后，数据库要做相应的更新。 首先要明确，无论是账户数据库还是 UTXO 数据库，必须是分散的，每结点一个克隆，一定不能是中心化的。如果比特币系统有一个中心数据库，不管你有多少节点，每一笔交易都要跑去中心数据库验证一下、然后再执行“转账”的事务操作，那就完全谈不上“去中心化”，比特币就毫无价值了，不如老老实实用支付宝。 长期来看，账户数据库会无限膨胀，而UTXO 数据库体积会小很多。 比特币是个匿名体系，它的账户就是“地址”。每一个比特币用户可以拥有几乎无限多的地址，在比特币系统来看，它完全不知道两个地址背后对应的是不是同一个人。 在版本控制方面的考虑，svn 是中心化的数据库保持一份账本，这和区块链的设计自然是相违背的，git 是去中心化的数据库，但会保存太多冗余数据，对于分布式性能肯定是要大打折扣。UTXO数据库是抛弃了历史包袱的git， 只存储了最后一个版本。简易实用。 UTXO具有天然的匿名效果，一个账户所对应的未花费交易是难以发现的。 在性能方面，由于UTXO是独立的数据记录，那么就存在极大的并行性可以提升区块链交易验证速度。 http://www.frankyang.cn/2017/09/30/utxo/]]></content>
      <categories>
        <category>BlockChain</category>
      </categories>
      <tags>
        <tag>BlockChain</tag>
        <tag>BitCoin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多重签名]]></title>
    <url>%2F2017%2F10%2F01%2F%E5%A4%9A%E9%87%8D%E7%AD%BE%E5%90%8D%2F</url>
    <content type="text"><![CDATA[在数字签名应用中，有时需要多个用户对同一个文件进行签名和认证。比如，一个公司发布的声明中涉及财务部、开发部、销售部、售后服务部等部门，需要的到这些部门签名认可，那么，就需要这些部门对这个声明文件进行签名。能够实现多个用户对同一文件进行签名的数字签名方案称作多重数字签名方案。 多重签名就是多个用户对同一个消息进行数字签名。 多重签名，可以简单的理解为一个数字资产的多个签名。签名标定的是数字资产所属和权限，多重签名预示着数字资产可由多人支配和管理。在加密货币领域，如果要动用一个加密货币地址的资金，通常需要该地址的所有人使用他的私钥（由用户专属保护）进行签名。那么，多重签名，就是动用这笔资金需要多个私钥签名，通常这笔资金或数字资产会保存在一个多重签名的地址或帐号里（就比特币而言，多重签名地址通常以3开头）。 Alice和Bob怎么对同一个数字文件签名呢？不用单向散列函数，有两种选择： 第一种选择是Alice和Bob分别对文件的副本签名，结果签名的消息是原文的两倍； 第二种就是Alice首先签名，然后Bob对Alice的签名再进行签名。这是可行的，但是在不验证Bob签名的情况下就验证Alice的签名是不可能的。 采用单向散列函数，很容易实现多重签名： (1)Alice对文件的散列签名。 (2)Bob对文件的散列签名。 (3)Bob将他的签名交给Alice。 (4)Alice把文件、她的签名和Bob的签名发给Carol。 (5)Carol验证Alice和Bob的签名。 Alice和Bob能同时或顺序地完成第(1)步和第(2)步；在第(5)步中Carol可以只验证其中一人的签名而不用验证另一人的签名。 关于 MultiSig（多重签名），从原理角度上讲，多重签名本身并不复杂，简单来说，一句话就够了：“用 m 把钥匙生成一个多重签名的地址，需要其中的 n 把钥匙才能花费这个地址上的比特币，m &gt;= n，这就是 n/m 的多重签名” 在实际的操作过程中，一个多重签名地址可以关联n个私钥，在需要转账等操作时，只要其中的m个私钥签名就可以把资金转移了，其中m要小于等于n，也就是说m/n小于1，可以是2/3, 3/5等等，是要在建立这个多重签名地址的时候确定好的。 多重签名给了加密货币腾飞的翅膀，让它单一单项支付的能力更具吸引力，让加密货币技术应用到各行各业成为可能。这里简单的罗列几个应用场景，供探索和思考： 电子商务。比较常见的是2/3的模式。上面电子商务网站的例子，就是最典型的场景之一，目前已经有成功的案例了。延伸一下，这类应用本质就是中介，所以还可用在各类中介机构性质的服务上。 财产分割。比如夫妻双方共有财产，可以使用1/2的模式，一个账户谁都可以使用，跟各自拥有帐号一样，好处是系统忠实记录了每个人的花销，闹掰的时候很容易清算。扩展到公司合伙经营，可以使用1/n模式，n个人合伙人，都可以直接支配共有资金，具体清算时，一目了然。 资金监管。其实，这是多重签名的最直接作用，一笔钱需要多个人签名才能使用，任何一个人都无法直接动用资金，这在生活中太常见了，只要灵活设置多重签名的比重模式，就能解决生活中很多问题。比如，接着上面夫妻的例子，夫妻要储备一笔资金，供孩子上大学使用，在这之前谁都不能动，那么把模式改为2/2，不仅限制了夫妻双方，也给黑客攻击增加了难度。 多重签名的设计，让各种业务去中心化充满无限可能。 http://www.frankyang.cn/2017/09/30/multisignature/]]></content>
      <categories>
        <category>Cryptography</category>
      </categories>
      <tags>
        <tag>BlockChain</tag>
        <tag>Cryptography</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据处理——Bloom Filter]]></title>
    <url>%2F2017%2F10%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-Bloom%20Filter%2F</url>
    <content type="text"><![CDATA[大数据处理——Bloom Filter 布隆过滤器（Bloom Filter）是由巴顿.布隆于一九七零年提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。 如果想判断一个元素是不是在一个集合里，一般想到的是将集合中所有元素保存起来，然后通过比较确定。链表、树、散列表（又叫哈希表，Hash table）等等数据结构都是这种思路。但是随着集合中元素的增加，我们需要的存储空间越来越大。同时检索速度也越来越慢。 Bloom Filter 是一种空间效率很高的随机数据结构，Bloom filter 可以看做是对 bit-map 的扩展, 它的原理是： 当一个元素被加入集合时，通过 K 个 Hash 函数将这个元素映射成一个位阵列（Bit array）中的 K 个点，把它们置为 1。检索时，我们只要看看这些点是不是都是 1 就（大约）知道集合中有没有它了： 如果这些点有任何一个 0，则被检索元素一定不在；如果都是 1，则被检索元素很可能在。 当一个元素被加入集合中时,通过k各散列函数将这个元素映射成一个位数组中的k个点,并将这k个点全部置为1. Bloom Filter使用k个相互独立的哈希函数（Hash Function），它们分别将集合中的每个元素映射到{1,…,m}的范围中。对任意一个元素x，第i个哈希函数映射的位置hi(x)就会被置为1（1≤i≤k）。注：如果一个位置多次被置为1，那么只有第一次会起作用，后面几次将没有任何效果。在判断y是否属于这个集合时，对y应用k次哈希函数，若所有hi(y)的位置都是1（1≤i≤k），就认为y是集合中的元素，否则就认为y不是集合中的元素。 优点 它的优点是空间效率和查询时间都远远超过一般的算法，布隆过滤器存储空间和插入 / 查询时间都是常数O(k)。另外, 散列函数相互之间没有关系，方便由硬件并行实现。布隆过滤器不需要存储元素本身，在某些对保密要求非常严格的场合有优势。 缺点 有一定的误判率–在判断一个元素是否属于某个集合时,有可能会把不属于这个集合的元素误判为属于这个集合.因此,它不适合那些”零误判”的应用场合.在能容忍低误判的应用场景下,布隆过滤器通过极少的误判换区了存储空间的极大节省. 但是布隆过滤器的缺点和优点一样明显。误算率是其中之一。随着存入的元素数量增加，误算率随之增加。但是如果元素数量太少，则使用散列表足矣。 另外，一般情况下不能从布隆过滤器中删除元素。我们很容易想到把位数组变成整数数组，每插入一个元素相应的计数器加1, 这样删除元素时将计数器减掉就可以了。然而要保证安全地删除元素并非如此简单。首先我们必须保证删除的元素的确在布隆过滤器里面。这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。 一个Bloom Filter有以下参数： 表示 意义 m bit数组的宽度（bit数） n 加入其中的key的数量 k 使用的hash函数的个数 f False Positive的比率，错误率（判错率） Bloom Filter的f满足下列公式：$$\left ( 1-\left ( 1-\frac{1}{m} \right )^{kn} \right )^{k} \approx \left ( 1-e^{\frac{-kn}{m}}\right )^{k} $$ 在给定m和n时，能够使f最小化的k值为：$\frac{m}{n}\ln 2\approx \frac{9m}{13n}\approx 0.7\frac{m}{n}$此时给出的f为：$\frac{1}{2}^{k}\approx 0.6185^{\frac{m}{n}}$对于任意给定的f，我们有：$n= \frac{m\ln 0.6185}{\ln f}$同时，我们需要k个hash来达成这个目标：$k = -\frac{\ln f}{\ln 2}$由于k必须取整数，我们在Bloom Filter的程序实现中，还应该使用上面的公式来求得实际的f：$f= \left ( 1- e^{-\frac{kn}{m}} \right )^{k}$ Example1、可以快速且空间效率高的判断一个元素是否属于一个集合；用来实现数据字典，或者集合求交集。2、使用bloom filter识别恶意链接3、检测垃圾邮件4、网页URL的去重: A,B两个文件，各存放50亿条URL，每条URL占用64字节，内存限制4G，让你找出 A,B 文件共同的URL。如果是三个乃至n个文件呢？ 分析 ：如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿 bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的 url（注意会有一定的错误率）。” http://www.frankyang.cn/2017/10/01/bloom-filter/]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Bloom</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链的跨链技术介绍]]></title>
    <url>%2F2017%2F10%2F01%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E7%9A%84%E8%B7%A8%E9%93%BE%E6%8A%80%E6%9C%AF%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[区块链技术是一种分布式账本技术，可以被应用在金融、健康医疗、供应链、资产管理等诸多领域，但是受吞吐量、网络孤立性、监管、伸缩性等因素的制约，目前的区块链项目并不能很好的服务于商业应用。在区块链所面临的诸多问题中，网络孤立性阻碍了不同区块链之间的协同操作，极大程度的限制了区块链的发挥空间。 如果说共识机制是区块链的灵魂核心，那么对于区块链特别是联盟链及私链来看，跨链技术就是实现价值网络的关键，它是把联盟链从分散单独的孤岛中拯救出来的良药，是区块链向外拓展和连接的桥梁。 侧链是以锚定比特币为基础的新型区块链，它以融合的方式实现新的金融生态目标和进一步扩展了比特币网络的应用场景。 联盟链和私链的方式从一定程度违背了区块链的去中心价值和信任体系，也让区块链里面的数字资产不能在不同的区块链间直接转移，主动或被动地导致了价值的孤岛，由此各种连接不同区块链的跨链技术也被人们开始关注和探索。 以太坊创始人Vitalik为银行联盟链R3写了一份关于跨链互操作的报告，文中提到三种跨链方式： 公证人机制（Notary schemes） 侧链/中继（Sidechains/relays） 哈希锁定（Hash-locking）。 早期跨链技术包括以Blockstream为代表的侧链技术(Sidechain)和以BTC-Relay为代表的中继技术(Relay)。早期的跨链技术更多关注的是资产转移，现有项目更多关注的是链状态的转移。 一、侧链 侧链是以锚定比特币为基础的新型区块链，就像美金锚定到金条一样。侧链是以融合的方式实现加密货币金融生态的目标，而不是像其它加密货币一样排斥现有的系统。利用侧链，我们可以轻松的建立各种智能化的金融合约，股票、期货、衍生品等等。 侧链技术进一步扩展了区块链技术的应用范围和创新空间，使传统区块链可以支持多种资产类型，以及小微支付、智能合约、安全处理机制、真实世界财产注册等，并可以增强区块链的隐私保护。 BTC Relay BTC Relay是一种基于以太坊区块链的智能合约，将把以太坊网络与比特币网络以一种安全去中心化的方式连接起来。BTC Relay通过使用以太坊的智能合约功能可以允许用户在以太坊区块链上验证比特币交易。BTC Relay使用区块头创建一种小型版本的比特币区块链，以太坊DApp开发者可以从智能合约向BTC Relay进行API调用来验证比特币网络活动。BTC Relay进行了跨区块链通信的有意义的尝试，打开了不同区块链交流的通道。 RootStock RootStock 是一个建立在比特币区块链上的智能合约分布式平台。它的目标是，将复杂的智能合约实施为一个侧链，为核心比特币网络增加价值和功能。RootStock实现了以太坊虚拟机的一个改进版本，它将作为比特币的一个侧链，使用了一种可转换为比特币的代币作为智能合约的“燃料”。 元素链 元素链是Blockstream的开源侧链项目，使用了比特币双向挂钩技术，侧链协议的目的是实现双向锚定（Two-way Peg），使得比特币可以在主链和侧链中互转。元素链给比特币快速带来许多创新技术，除了智能合约外，他还给比特币快速带来许多创新技术，包括私密交易、证据分离、相对锁定时间、新操作码、签名覆盖金额等等特性。这些技术可以被任意组合应用到任意侧链中。 LISK LISK是新一代的区块链平台，它把每个应用加到LISK的单独侧链上。用过比特币和以太坊的朋友都知道，由于比特币和以太坊只有一条主链，所有功能和数据都加入这条主链导致区块快速膨胀，超大的区块体积，超长的同步时间，这个一个很痛苦的经历。Lisk的侧链模式给在处理高交易量下如何解决网络拥堵的问题提供了一种方法，用户只有用到相关的应用时才需要下载对应的侧链，大大减小了无效的同步数据，保持了整个Lisk网络的高效运行，而且，Lisk网络的速度随着时间的推移会继续加快，越显示他的特别优势。 二、M2 公证通（Factom）利用区块链技术来革新商业社会和政府部门的数据管理和数据记录方式，核心是在区块链上建立不可更改的审计公证业务流程。Factom原来是通过比特币网络进行数据存证，后来通过M2功能，同时整合比特币和以太坊区块链，以使得确保数据时时刻刻都是安全和可信的。 M2功能大概情况是，Factom将网络中所有数据整合成条目或者链。用户数据存储在条目中，而链与条目之间形成互动协作。每个链拥有条目区块，这些区块又以每十分钟的速度生成新的。十分钟的结尾所有链的全部新的条目区块会整合到一个目录区块中，然后嵌入比特币和以太坊区块链。如果10分钟之内某个链没有新的条目，该链就不会增加新的条目区块。 Factom这样做的意义，可以使他们的数据存储不会仅仅依赖于比特币账本一个单一的区块链，是作为链接多个公链的有意义的探索。 三、Polkadot Polkadot技术是由以太坊核心开发Ethcore（Parity科技）推出的第三代公开无需授权的区块链科技，它的设计核心理念为即时拓展性和延伸性，解决了当今两大阻止区块链技术传播和接受的难题：即时拓展性和延伸性。 Polkadot计划将私有链/联盟链融入到公有链的共识网络中去，同时又能保有私有链/联盟链的隐私和许可的防护措施。它给予了我们一个全新的交易层，并有机会将数百个区块链互相连接。 Polkadot的核心思想是区分交易方发起和执行交易的方式以及交易方统一记录的方式。Polkadot提供基础的中继链（relay-chain），很多可验证的、全球动态同步的数据架构都建立在这个基础上，这些数据架构为平行链或者侧链。区块链应用可以将以太坊分叉，按照各自需求调整，通过Polkadot与以太坊公有链连接，或者给不同的链设置不同的功能，实现更好的扩展性和效率。 Polkadot目前还是以以太坊为主，实现其与私链的互连，并以其他公有链网络为升级目标，最终让以太坊直接与任何链进行通讯。 Melonport 开发的软件Melon将是运行于多链网络（multi-chain network）Polkadot之上的第一款Dapp。 角色分工 Polkadot的设计中通过建立明确的角色，如下图所示，完成不同的职能分工，再结合一套巧妙的奖惩机制，协同完成跨链通信。这个设想参考了现实社会的社会分工和权力制约体系，实现了其在保证信任、安全的前提下做到系统的水平扩展。 在此分工中还有很多开发性的话题留给大家去思考，如当惩罚的力度小于奖励的力度时，赏金猎人和验证者就有可能联合串通做违规行为。这种情况下，可以采用另外一种经济网络，赏金猎人以工作量证明的共识方式组成赏金网络，激励赏金猎人做正确的行为，维护Polkadot整套体系的健全性。 四、Interledger 早在2012年，Ripple Labs就提出了一种偏离比特币共识的替代性账本，并创造了他们自己的代币瑞波币(XRP)。Ripple的初衷是建立一套适用于所有记账系统，能够包容所有记账系统的差异性协议，从而建立一个全球统一的支付标准。 在不同账本之间进行价值转移和交换，总会碰到各种问题。比如Elwin希望通过比特币作为媒介向海外同事Jam进行汇款，Elwin目前只有人民币，Jam只接受美金。这笔交易是首先Elwin把人民币换成比特币再把比特币换成美金給到Jam，但这里有个问题就是币价会不稳定，导致价值损耗。而Ripple、Stellar、Circle等正是解决这些难题的利器，这几个的核心思想方向基本一致：账本提供的第三方，就会向发送者保证，他们的资金，只有当账本收到证明，且收件人已经收到支付时，才会将资金转移给连接者。第三方也会保证连接者，一旦他们完成了协议的最后部分，他们就会收到发件人的资金。 Interledger Protocol，简称ILP，是由Ripple公司主导发起了互联账目协议，它将实现不同账本之间的连接从而创造账本之间的协作。Interledger协议适用于所有记账系统、能够包容所有记账系统的差异性，ILP推出的目标就是打造全球统一支付标准，创建统一的网络金融传输的协议。 金融机构基本上都是在自己的网络之中运行着各自的记账系统，即使运用了区块链技术后，也是在运行自己的私链或内部圈子的联盟链，这个除了是应对监管合规性的原因外，更重要是保护他们的内部数据避免泄密。ILP的由来是由于Ripple原来推广业务的困难导致的，银行宁愿用Ripple的源代码来搭建他们自己的私链，也不愿意连接到Ripple上。既然建立一个每个人都支持的全球金融传输协议很困难，Ripple就开发一个协议，能将所有我们目前正在使用记账系统连接在一起。 Interledger协议创建了一个这样的系统，在这个系统中，两个不同的记账系统可以通过第三方“连接器”或“验证器”机器来互相自由地传输货币。记账系统无需去信任“连接器”，因为该协议采用密码算法为这两个记账系统和连接器创建资金托管，当所有参与方对资金量达成共识时，便可相互交易。ILP移除了交易参与者所需的信任，连接器不会丢失或窃取资金，这意味着，这种交易无需得到法律合同的保护和过多的审核，大大降低了门槛。同时，只有参与其中的记账系统才可以跟踪交易，交易的详情可隐藏起来，“验证器”是通过加密算法来运行，因此不会直接看到交易的详情。 理论上，interledger可以兼容任何在线记账系统，而银行现有的记账系统只需小小的改变就能使用该协议。 Ripple让世界各地的银行可以无需中央对手方或代理银行就可直接交易，从而使得让世界上的不同货币（包括法定货币和虚拟货币）自由、近乎免费、零延时地进行汇兑；Circle则让用户可以在无需手续费的情况下，以发送消息的形式发起即时的国内或跨境转账、收付款。目前Ripple和Circle正受到资本市场的热捧，Elwin觉得其中的原因，与其说他们的崛起是由于跨境汇兑和P2P支付革新，还不如说他们是对价值交换的革新，它们将各种账本连接起来，实现在互联网上交换资金能像交换信息一样轻松。 无论是原有的Ripple网络还是现在的Interledger，都在强调其是作为一个基于信任转移的价值网络。这与传统区块链系统设计是很不相同的。类似的还有R3的Corda项目，自称为非区块链的区块链项目。正如前文所述，这与这两个项目都专注于银行业务，而银行本身是个封闭的系统的有关。 五、众安科技Anlink（联网+链路由） 众安科技同样看到了单一区块链在许多应用场景中无法解决复杂问题以及在性能上相对于传统中心化系统所呈现的瓶颈，由此提出并发展了称为“安链-链路由”的区块链网络拓扑结构。 安链链路由引入了链网络的概念，如果将链通信比作局域网的话，安链链网络的目标就是打通各个局域网形成一个连接所有区块链/非区块链系统的广域去中心化的信任网络。其母链-子链的拓扑结构以及一系列的通信协议，保证了这个设计目标的实现。这种类互联网的分层协议实现，借鉴了已有的技术实现并且很好的解决了区块链系统的扩展性问题。 在Anlink区块链网络中： “母链”构成了信息主干道，不同的母链之间通过链路由协议交换信息。 同时，一个母链上承载着不同的子链，这些子链可以是以太坊，可以是比特币，事实上也可以是任何一种分布式账本实现。 子链间的通信则由跨链通信协议CBCP负责。 我们通过链路由实现了区块链的分片，提高区块链系统的交易处理能力。 相较于一条单独的区块链系统，链路由系统可以通过连接多条子链的方式在交易处理能力上直线增长。交易的请求通过链路由的分配进入不同子链，可以有效规避针对一条子链的集中请求。 此外，我们可以在链路由上部署同构子链的不同节点数的集群，对于同构链而言，多节点数量的集群会有相对较高的安全性，少节点集群的处理速度则更快。 通过链路由实现区块链的分片，可以帮助链网络根据业务需求灵活部署，为用户提供更高质量的区块链服务。 打通各子链，在链与链间建立信任桥梁。 连接在链路由上的各子链之间通过链路由得以彼此传递消息，协同工作，实现“1+1&gt;2”的效果。 我们也可以部署多个链路由系统，每一个链路由系统中都可以部署包括比特币，以太坊，安链在内的各种子链。因此每一个链路由都可以服务一个更完整的业务生态。 同理，我们可以根据节点数量，地理位置，业务分类等不同需求，部署不同的链路由集群，根据路由规则，对应不同需求将请求分发到合适的集群之中处理。 链路由网络的最终形态，是通过链路由的无限扩展和彼此之间的相互连接，形成连接不同集群的复杂区块链星状网络。创造一个由区块链组成的，互联，互通，互信的网络世界。 对比：]]></content>
      <categories>
        <category>BlockChain</category>
      </categories>
      <tags>
        <tag>BlockChain</tag>
        <tag>跨链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[半小时学会LevelDB原理及应用]]></title>
    <url>%2F2017%2F09%2F22%2F%E5%8D%8A%E5%B0%8F%E6%97%B6%E5%AD%A6%E4%BC%9ALevelDB%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[LevelDBLevelDB之概览 LevelDB是Google传奇工程师Jeff Dean和Sanjay Ghemawat开源的KV存储引擎。 了解原理之前首先要用起来，下面动手实现个例子：安装调试(mac上直接命令行下brew install leveldb即可安装，编译时候记得加上-lleveldb) example: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;assert.h&gt;#include &lt;string.h&gt;#include &lt;leveldb/db.h&gt;#include &lt;iostream&gt;int main(int argc, char** argv)&#123; leveldb::DB* db; leveldb::Options options; // 如果打开已存在数据库的时候，需要抛出错误，将以下代码插在leveldb::DB::Open方法前面 options.create_if_missing = true; // 打开一个数据库实例 leveldb::Status status = leveldb::DB::Open(options, &quot;/tmp/testdb&quot;, &amp;db); assert(status.ok()); // LevelDB提供了Put、Get和Delete三个方法对数据库进行添加、查询和删除 std::string key = &quot;key&quot;; std::string value = &quot;value&quot;; // 添加key=value status = db-&gt;Put(leveldb::WriteOptions(), key, value); assert(status.ok()); // 根据key查询value status = db-&gt;Get(leveldb::ReadOptions(), key, &amp;value); assert(status.ok()); std::cout&lt;&lt;value&lt;&lt;std::endl; // 修改操作（原生没有提供）由添加和删除合起来实现 std::string key2 = &quot;key2&quot;; // 添加key2=value status = db-&gt;Put(leveldb::WriteOptions(),key2,value); assert(status.ok()); // 删除key status = db-&gt;Delete(leveldb::WriteOptions(), key); // 查询key2 assert(status.ok()); status = db-&gt;Get(leveldb::ReadOptions(), key2, &amp;value); assert(status.ok()); std::cout&lt;&lt;key2&lt;&lt;&quot;==&quot;&lt;&lt;value&lt;&lt;std::endl; // 查询key status = db-&gt;Get(leveldb::ReadOptions(), key, &amp;value); if (!status.ok()) &#123; std::cerr &lt;&lt; key &lt;&lt; &quot;: &quot; &lt;&lt; status.ToString() &lt;&lt; std::endl; &#125; else &#123; std::cout &lt;&lt; key &lt;&lt; &quot;==&quot; &lt;&lt; value &lt;&lt; std::endl; &#125; delete db; return 0;&#125; 设计思路 LevelDB的数据是存储在磁盘上的，采用LSM-Tree的结构实现。LSM-Tree将磁盘的随机写转化为顺序写，从而大大提高了写速度。 为了做到这一点LSM-Tree的思路是将索引树结构拆成一大一小两颗树，较小的一个常驻内存，较大的一个持久化到磁盘，他们共同维护一个有序的key空间。 写入操作会首先操作内存中的树，随着内存中树的不断变大，会触发与磁盘中树的归并操作，而归并操作本身仅有顺序写。随着数据的不断写入，磁盘中的树会不断膨胀，为了避免每次参与归并操作的数据量过大，以及优化读操作的考虑，LevelDB将磁盘中的数据又拆分成多层，每一层的数据达到一定容量后会触发向下一层的归并操作，每一层的数据量比其上一层成倍增长。这也就是LevelDB的名称来源。 整体结构内存数据的Memtable，分层数据存储的SST文件，版本控制的Manifest、Current文件，以及写Memtable前的WAL。 WAL: Write-Ahead Logging预写日志系统数据库中一种高效的日志算法，对于非内存数据库而言，磁盘I/O操作是数据库效率的一大瓶颈。在相同的数据量下，采用WAL日志的数据库系统在事务提交时，磁盘写操作只有传统的回滚日志的一半左右，大大提高了数据库磁盘I/O操作的效率，从而提高了数据库的性能。 Memtable： 对应Leveldb中的内存数据，LevelDB的写入操作会直接将数据写入到Memtable后返回。读取操作又会首先尝试从Memtable中进行查询，允许写入和读取。当Memtable写入的数据占用内存到达指定数量，则自动转换为Immutable Memtable，等待Dump到磁盘中，系统会自动生成新的Memtable供写操作写入新数据。 LevelDB采用跳表SkipList实现，在给提供了O(logn)的时间复杂度的同时，又非常的易于实现： 跳表作为一种数据结构通常用于取代平衡树，与红黑树不同的是，skiplist对于树的平衡的实现是基于一种随机化的算法的，也就是说skiplist的插入和删除的工作是比较简单地。 SkipList中单条数据存放一条Key-Value数据，定义为： 12345SkipList Node := InternalKey + ValueStringInternalKey := KeyString + SequenceNum + TypeType := kDelete or kValueValueString := ValueLength + ValueKeyString := UserKeyLength + UserKey Log文件 当应用写入一条Key:Value记录的时候，LevelDb会先往log文件里写入，成功后将记录插进Memtable中，这样基本就算完成了写入操作，Log文件在系统中的作用主要是用于系统崩溃恢复而不丢失数据，假如没有Log文件，因为写入的记录刚开始是保存在内存中的，此时如果系统崩溃，内存中的数据还没有来得及Dump到磁盘，所以会丢失数据（Redis就存在这个问题）。 因为一次写入操作只涉及一次磁盘顺序写和一次内存写入，所以这是为何说LevelDb写入速度极快的主要原因。 LevelDB首先将每条写入数据序列化为一个Record，单个Log文件中包含多个Record。同时，Log文件又划分为固定大小的Block单位，并保证Block的开始位置一定是一个新的Record。这种安排使得发生数据错误时，最多只需丢弃一个Block大小的内容。显而易见地，不同的Record可能共存于一个Block，同时，一个Record也可能横跨几个Block。 Log文件划分为固定长度的Block，由连续的32K为单位的物理Block构成的，每次读取的单位是以一个Block作为基本单位；每个Block中包含多个Record；Record的前56个位为Record头，包括32位checksum用做校验，16位存储Record实际内容数据的长度，8位的Type可以是Full、First、Middle或Last中的一种，表示该Record是否完整的在当前的Block中，如果Type不是Full，则通过Type指明其前后的Block中是否有当前Record的前驱后继。 1234Block := Record * NRecord := Header + ContentHeader := Checksum + Length + TypeType := Full or First or Midder or Last Immutable Memtable 当Memtable插入的数据占用内存到了一个界限后，需要将内存的记录导出到外存文件中，LevleDb会生成新的Log文件和Memtable，Memtable会变为Immutable，为之后向SST文件的归并做准备。顾名思义，Immutable Mumtable不再接受用户写入，只能读不能写入或者删除，同时会有新的Log文件和Memtable生成，LevelDb后台调度会将Immutable Memtable的数据导出到磁盘，形成一个新的SSTable文件。 SST文件 SSTable就是由内存中的数据不断导出并进行Compaction操作(压缩操作，下文会讲到)后形成的，而且SSTable的所有文件是一种层级结构，第一层为Level 0，第二层为Level 1，依次类推，层级逐渐增高，这也是为何称之为LevelDb的原因。 磁盘数据存储文件。分为Level 0到Level N多层，每一层包含多个SST文件；单个SST文件容量随层次增加成倍增长；文件内数据有序；其中Level 0的SST文件由Immutable直接Dump产生，其他Level的SST文件由其上一层的文件和本层文件归并产生；SST文件在归并过程中顺序写生成，生成后仅可能在之后的归并中被删除，而不会有任何的修改操作。 SSTable中的文件是Key有序的，就是说在文件中小key记录排在大Key记录之前，各个Level的SSTable都是如此，但是这里需要注意的一点是：Level 0的SSTable文件（后缀为.sst）和其它Level的文件相比有特殊性：这个层级内的.sst文件，两个文件可能存在key重叠，比如有两个level 0的sst文件，文件A和文件B，文件A的key范围是：{bar, car}，文件B的Key范围是{blue,samecity}: level N .sst max min Level 0 A.sst “bar” “car” Level 0 B.sst “blue” “samecity” 那么很可能两个文件都存在key=”blood”的记录。对于其它Level的SSTable文件来说，则不会出现同一层级内。 SST文件的物理格式 LevelDb不同层级有很多SSTable文件（以后缀.sst为特征），所有.sst文件内部布局都是一样的。上节介绍Log文件是物理分块的，SSTable也一样会将文件划分为固定大小的物理存储块，但是两者逻辑布局大不相同，根本原因是：Log文件中的记录是Key无序的，即先后记录的key大小没有明确大小关系，而.sst文件内部则是根据记录的Key由小到大排列的。 LevelDB将SST文件定义为Table，每个Table又划分为多个连续的Block，每个Block中又存储多条数据Entry： 可以看出，单个Block作为一个独立的写入和解析单位，会在其末尾存储一个字节的Type和4个字节的Crc，其中Type记录的是当前Block的数据压缩策略（Snappy压缩或者无压缩两种），而Crc则存储Block中数据的校验信息。 Block中每条数据Entry是以Key-Value方式存储的，并且是按Key有序存储，Leveldb很巧妙了利用了有序数组相邻Key可能有相同的Prefix的特点来减少存储数据量。如上图所示，每个Entry只记录自己的Key与前一个Entry Key的不同部分， 在Entry开头记录三个长度值，分别是当前Entry和其之前Entry的公共Key Prefix长度、当前Entry Key自有Key部分的长度和Value的长度。通过这些长度信息和其后相邻的特有Key及Value内容，结合前一条Entry的Key内容，我们可以方便的获得当前Entry的完整Key和Value信息。 例如要顺序存储Key值“apple” = value1和“applepen” = value2的两条数据，这里第二个Entry中，key共享长度为5，key非共享长度为3，value长度为6，key非共享内容为“pen”，value内容为“value2”. 这种方式非常好的减少了数据存储，但同时也引入一个风险，如果最开头的Entry数据损坏，其后的所有Entry都将无法恢复。为了降低这个风险，leveldb引入了重启点，每隔固定条数Entry会强制加入一个重启点，这个位置的Entry会完整的记录自己的Key，并将其shared值设置为0。同时，Block会将这些重启点的偏移量及个数记录在所有Entry后边的Tailer中。 SST文件的逻辑格式 Table中不同的Block物理上的存储方式一致，如上文所示，但在逻辑上可能存储不同的内容，包括存储数据的Block，存储索引信息的Block，存储Filter的Block： Data Block:从图中可以看出，其内部也分为两个部分，前面是一个个KV记录，其顺序是根据Key值由小到大排列的，在Block尾部则是一些“重启点”（Restart Point）,其实是一些指针，指出Block内容中的一些记录位置。 Footer：为于Table尾部，记录指向Metaindex Block的Handle和指向Index Block的Handle。需要说明的是Table中所有的Handle是通过偏移量Offset以及Size一同来表示的，用来指明所指向的Block位置。Footer是SST文件解析开始的地方，通过Footer中记录的这两个关键元信息Block的位置，可以方便的开启之后的解析工作。另外Footer中还记录了用于验证文件是否为合法SST文件的常数值Magic num。 Index Block：记录Data Block位置信息的Block，其中的每一条Entry指向一个Data Block，其Key值为所指向的Data Block最后一条数据的Key，Value为指向该Data Block位置的Handle。 Metaindex Block：与Index Block类似，由一组Handle组成，不同的是这里的Handle指向的Meta Block。 12345Data Block：以Key-Value的方式存储实际数据，其中Key定义为：DataBlock Key := UserKey + SequenceNum + Type //对比Memtable中的Key，可以发现Data Block中的Key并没有拼接UserKey的长度在UserKey前，//这是由于上面讲到的物理结构中已经有了Key的长度信息。Type := kDelete or kValue Meta Block：比较特殊的Block，用来存储元信息，目前LevelDB使用的仅有对布隆过滤器的存储。写入Data Block的数据会同时更新对应Meta Block中的过滤器。读取数据时也会首先经过布隆过滤器过滤。Meta Block的物理结构也与其他Block有所不同： 123456789101112[filter 0][filter 1][filter 2]... [filter N-1][offset of filter 0] : 4 bytes[offset of filter 1] : 4 bytes[offset of filter 2] : 4 bytes... [offset of filter N-1] : 4 bytes[offset of beginning of offset array] : 4 byteslg(base) : 1 byte 其中每个filter节对应一段Key Range，落在某个Key Range的Key需要到对应的filter节中查找自己的过滤信息，base指定这个Range的大小。 Manifest文件 Manifest文件中记录SST文件在不同Level的分布，单个SST文件的最大最小key，以及其他一些LevelDB需要的元信息。 SSTable中的某个文件属于特定层级，而且其存储的记录是key有序的，那么必然有文件中的最小key和最大key，这是非常重要的信息，LevelDb应该记下这些信息。Manifest就是干这个的 Current文件 从上面的介绍可以看出，LevelDB启动时的首要任务就是找到当前的Manifest，而Manifest可能有多个。Current文件简单的记录了当前Manifest的文件名，从而让这个过程变得非常简单。 Current文件的内容只有一个信息，就是记载当前的manifest文件名。因为在LevleDb的运行过程中，随着Compaction的进行，SSTable文件会发生变化，会有新的文件产生，老的文件被废弃，Manifest也会跟着反映这种变化，此时往往会新生成Manifest文件来记载这种变化，而Current则用来指出哪个Manifest文件才是我们关心的那个Manifest文件。 主要操作读写操作写流程 LevelDB的写操作包括设置key-value和删除key两种。需要指出的是这两种情况在LevelDB的处理上是一致的，删除操作其实是向LevelDB插入一条标识为删除的数据。 Memtable并不存在真正的删除操作，删除某个Key的Value在Memtable内是作为插入一条记录实施的，但是会打上一个Key的删除标记，真正的删除操作是Lazy的，会在以后的Compaction过程中去掉这个KV。 从图中可以看出，对于一个插入操作Put(Key,Value)来说，完成插入操作包含两个具体步骤： 首先是将这条KV记录以顺序写的方式追加到之前介绍过的log文件末尾，因为尽管这是一个磁盘读写操作，但是文件的顺序追加写入效率是很高的，所以并不会导致写入速度的降低； 第二个步骤是:如果写入log文件成功，那么将这条KV记录插入内存中的Memtable中，前面介绍过，Memtable只是一层封装，其内部其实是一个Key有序的SkipList列表，插入一条新记录的过程也很简单，即先查找合适的插入位置，然后修改相应的链接指针将新记录插入即可。完成这一步，写入记录就算完成了。 所以一个插入记录操作涉及一次磁盘文件追加写和内存SkipList插入操作，这是为何levelDb写入速度如此高效的根本原因。 LevelDb的接口没有直接支持更新操作的接口，如果需要更新某个Key的Value,你可以选择直接生猛地插入新的KV，保持Key相同，这样系统内的key对应的value就会被更新；或者你可以先删除旧的KV， 之后再插入新的KV，这样比较委婉地完成KV的更新操作。 读流程 首先，生成内部查询所用的Key，用生成的Key，依次尝试从 Memtable，Immtable以及SST文件中读取，直到找到（或者查到最高level，查找失败，说明整个系统中不存在这个Key)。 从信息的更新时间来说，很明显Memtable存储的是最新鲜的KV对；Immutable Memtable中存储的KV数据对的新鲜程度次之；而所有SSTable文件中的KV数据新鲜程度一定不如内存中的Memtable和Immutable Memtable的。对于SSTable文件来说，如果同时在level L和Level L+1找到同一个key，level L的信息一定比level L+1的要新。也就是说，上面列出的查找路径就是按照数据新鲜程度排列出来的，越新鲜的越先查找。 举个例子。比如我们先往levelDb里面插入一条数据 {key=&quot;www.samecity.com&quot; value=&quot;我们&quot;},过了几天，samecity网站改名为：69同城，此时我们插入数据{key=&quot;www.samecity.com&quot; value=&quot;69同城&quot;}，同样的key,不同的value；逻辑上理解好像levelDb中只有一个存储记录，即第二个记录，但是在levelDb中很可能存在两条记录，即上面的两个记录都在levelDb中存储了，此时如果用户查询key=&quot;www.samecity.com&quot;,我们当然希望找到最新的更新记录，也就是第二个记录返回，这就是为何要优先查找新鲜数据的原因。 从SST文件中查找需要依次尝试在每一层中读取，得益于Manifest中记录的每个文件的key区间，我们可以很方便的知道某个key是否在文件中。Level 0的文件由于直接由Immutable Dump 产生，不可避免的会相互重叠，所以需要对每个文件依次查找。对于其他层次，由于归并过程保证了其互相不重叠且有序，二分查找的方式提供了更好的查询效率。 可以看出同一个Key出现在上层的操作会屏蔽下层的。也因此删除Key时只需要在Memtable压入一条标记为删除的条目即可。被其屏蔽的所有条目会在之后的归并过程中清除。 相对写操作，读操作处理起来要复杂很多，所以写的速度必然要远远高于读数据的速度，也就是说，LevelDb比较适合写操作多于读操作的应用场合。而如果应用是很多读操作类型的，那么顺序读取效率会比较高，因为这样大部分内容都会在缓存中找到，尽可能避免大量的随机读取操作。 levelDb中的Cache 读取操作如果没有在内存的memtable中找到记录，要多次进行磁盘访问操作。假设最优情况，即第一次就在level 0中最新的文件中找到了这个key，那么也需要读取2次磁盘，一次是将SSTable的文件中的index部分读入内存，这样根据这个index可以确定key是在哪个block中存储；第二次是读入这个block的内容，然后在内存中查找key对应的value。 levelDb中引入了两个不同的Cache： Table Cache 和 Block Cache。其中Block Cache是配置可选的，即在配置文件中指定是否打开这个功能。 在Cache中，key值是SSTable的文件名称，Value部分包含两部分，一个是指向磁盘打开的SSTable文件的文件指针，这是为了方便读取内容；另外一个是指向内存中这个SSTable文件对应的Table结构指针，table结构在内存中，保存了SSTable的index内容以及用来指示block cache用的cache_id ,当然除此外还有其它一些内容。 比如在get(key)读取操作中，如果levelDb确定了key在某个level下某个文件A的key range范围内，那么需要判断是不是文件A真的包含这个KV。此时，levelDb会首先查找Table Cache，看这个文件是否在缓存里，如果找到了，那么根据index部分就可以查找是哪个block包含这个key。如果没有在缓存中找到文件，那么打开SSTable文件，将其index部分读入内存，然后插入Cache里面，去index里面定位哪个block包含这个Key 。如果确定了文件哪个block包含这个key，那么需要读入block内容，这是第二次读取。 File cache_id + block_offset block内容 File cache_id + block_offset block内容 File cache_id + block_offset block内容 File cache_id + block_offset block内容 Block Cache是为了加快这个过程的，如上图。其中的key是文件的cache_id加上这个block在文件中的起始位置block_offset。而value则是这个Block的内容。 如果levelDb发现这个block在block cache中，那么可以避免读取数据，直接在cache里的block内容里面查找key的value就行，如果没找到呢？那么读入block内容并把它插入block cache中。levelDb就是这样通过两个cache来加快读取速度的。 从这里可以看出，如果读取的数据局部性比较好，也就是说要读的数据大部分在cache里面都能读到，那么读取效率应该还是很高的，而如果是对key进行顺序读取效率也应该不错，因为一次读入后可以多次被复用。但是如果是随机读取，您可以推断下其效率如何。 压缩操作 为了加快读取速度，levelDb采取了compaction的方式来对已有的记录进行整理压缩，通过这种方式，来删除掉一些不再有效的KV数据，减小数据规模，减少文件数量等。 数据压缩是LevelDB中重要的部分，即上文提到的归并。冷数据会随着Compaction不断的下移，同时过期的数据也会在合并过程中被删除。 LevelDB的压缩操作由单独的后台线程负责。这里的Compaction包括两个部分，Memtable向Level 0 SST文件的Compaction，以及SST文件向下层的Compaction。 levelDb的compaction机制和过程与Bigtable所讲述的是基本一致的，Bigtable中讲到三种类型的compaction: minor ，major和full。所谓minor Compaction，就是把memtable中的数据导出到SSTable文件中；major compaction就是合并不同层级的SSTable文件，而full compaction就是将所有SSTable进行合并。 LevelDb包含其中两种，minor和major。 minor compaction Minor compaction 的目的是当内存中的memtable大小到了一定值时，将内容保存到磁盘文件中： 当memtable数量到了一定程度会转换为immutable memtable，此时不能往其中写入记录，只能从中读取KV内容。之前介绍过，immutable memtable其实是一个多层级队列SkipList，其中的记录是根据key有序排列的。所以这个minor compaction实现起来也很简单，就是按照immutable memtable中记录由小到大遍历，并依次写入一个level 0的新建SSTable文件中，写完后建立文件的index数据，这样就完成了一次minor compaction。 CompactMemTable函数会将Immutable中的数据整体Dump为Level 0的一个文件，这个过程会在Immutable Memtable存在时被Compaction后台线程调度。 过程比较简单，首先会获得一个Immutable的Iterator用来遍历其中的所有内容，创建一个新的Level 0 SST文件，并将Iterator读出的内容依次顺序写入该文件。之后更新元信息并删除Immutable Memtable。 major compaction 当某个level下的SSTable文件数目超过一定设置值后，levelDb会从这个level的SSTable中选择一个文件（level&gt;0），将其和高一层级的level+1的SSTable文件合并，这就是major compaction。 我们知道在大于0的层级中，每个SSTable文件内的Key都是由小到大有序存储的，而且不同文件之间的key范围（文件内最小key和最大key之间）不会有任何重叠。Level 0的SSTable文件有些特殊，尽管每个文件也是根据Key由小到大排列，但是因为level 0的文件是通过minor compaction直接生成的，所以任意两个level 0下的两个sstable文件可能再key范围上有重叠。所以在做major compaction的时候，对于大于level 0的层级，选择其中一个文件就行，但是对于level 0来说，指定某个文件后，本level中很可能有其他SSTable文件的key范围和这个文件有重叠，这种情况下，要找出所有有重叠的文件和level 1的文件进行合并，即level 0在进行文件选择的时候，可能会有多个文件参与major compaction。 同层的文件轮流来compaction，比如这次是文件A进行compaction，那么下次就是在key range上紧挨着文件A的文件B进行compaction，这样每个文件都会有机会轮流和高层的level 文件进行合并。如果选好了level L的文件A和level L+1层的文件进行合并，那么问题又来了，应该选择level L+1哪些文件进行合并？levelDb选择L+1层中和文件A在key range上有重叠的所有文件来和文件A进行合并。 Major compaction的过程如下：对多个文件采用多路归并排序的方式，依次找出其中最小的Key记录，也就是对多个文件中的所有记录重新进行排序。之后采取一定的标准判断这个Key是否还需要保存，如果判断没有保存价值，那么直接抛掉，如果觉得还需要继续保存，那么就将其写入level L+1层中新生成的一个SSTable文件中。就这样对KV数据一一处理，形成了一系列新的L+1层数据文件，之前的L层文件和L+1层参与compaction 的文件数据此时已经没有意义了，所以全部删除。这样就完成了L层和L+1层文件记录的合并过程。 那么在major compaction过程中，判断一个KV记录是否抛弃的标准是什么呢？其中一个标准是:对于某个key来说，如果在小于L层中存在这个Key，那么这个KV在major compaction过程中可以抛掉。因为我们前面分析过，对于层级低于L的文件中如果存在同一Key的记录，那么说明对于Key来说，有更新鲜的Value存在，那么过去的Value就等于没有意义了，所以可以删除。 BackgroundCompaction函数 SST文件的Compaction可以由用户通过接口手动发起，也可以自动触发。LevelDB中触发SST Compaction的因素包括Level 0 SST的个数，其他Level SST文件的总大小，某个文件被访问的次数。Compaction线程一次Compact的过程如下： 首先根据触发Compaction的原因以及维护的相关信息找到本次要Compact的一个SST文件。对于Level 0的文件比较特殊，由于Level 0的SST文件由Memtable在不同时间Dump而成，所以可能有Key重叠。因此除该文件外还需要获得所有与之重叠的Level 0文件。这时我们得到一个包含一个或多个文件的文件集合，处于同一Level。 SetupOtherInputs： 在Level+1层获取所有与当前的文件集合有Key重合的文件。 DoCompactionWork：对得到的包含相邻两层多个文件的文件集合，进行归并操作并将结果输出到Level + 1层的一个新的SST文件，归并的过程中删除所有过期的数据。删除之前的文件集合里的所有文件。 通过上述过程我们可以看到，这个新生成的文件在其所在Level不会跟任何文件有Key的重叠。 LevelDb 的特点： 首先，LevelDb是一个持久化存储的KV系统，和Redis这种内存型的KV系统不同，LevelDb不会像Redis一样狂吃内存，而是将大部分数据存储到磁盘上。 其次，LevelDb在存储数据时，是根据记录的key值有序存储的，就是说相邻的key值在存储文件中是依次顺序存储的，而应用可以自定义key大小比较函数，LevelDb会按照用户定义的比较函数依序存储这些记录。 再次，像大多数KV系统一样，LevelDb的操作接口很简单，基本操作包括写记录，读记录以及删除记录。也支持针对多条操作的原子批量操作。 另外，LevelDb支持数据快照(snapshot)功能，使得读取操作不受写操作影响，可以在读操作过程中始终看到一致的数据。 除此外，LevelDb还支持数据压缩等操作，这对于减小存储空间以及增快IO效率都有直接的帮助。 LevelDb性能非常突出，官方网站报道其随机写性能达到40万条记录每秒，而随机读性能达到6万条记录每秒。总体来说，LevelDb的写操作要大大快于读操作，而顺序读写操作则大大快于随机读写操作。 http://www.frankyang.cn/2017/09/04/%E5%8D%8A%E5%B0%8F%E6%97%B6%E5%AD%A6%E4%BC%9Aleveldb%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8/]]></content>
      <categories>
        <category>DataBase</category>
      </categories>
      <tags>
        <tag>LevelDB</tag>
        <tag>DataBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2PC&3PC]]></title>
    <url>%2F2017%2F09%2F11%2F2PC%263PC%2F</url>
    <content type="text"><![CDATA[2PC用于保证跨多个节点操作的原子性，也就是说，跨多个节点的操作要么在所有节点上全部执行成功，要么全部失败。Paxos协议用于确保多个节点对某个投票（例如哪个节点为主节点）达成一致。 在分布式系统中，每一个机器节点虽然都能够明确地知道自己在进行实物操作过程中的结果是成功或失败，但却无法直接获取到其他分布式节点的操作结果。为了保持实物处理的ACID特性，就需要引入一个称为“协调者（Coordinator）”的组件来统一调度所有分布式节点的执行逻辑，这些被调度的分布式节点则被称为“参与者（Participant）”。协调者负责调度参与者的行为，并最终决定这些参与者是否要把事务真正进行提交。 2PC（Two-Phase Commit） 二阶段提交，是一种一致性协议，用来保证分布式系统数据的一致性。绝大多数的关系型数据库都是采用二阶段提交协议来完成分布式事务处理的，利用该协议能够非常方便地完成所有分布式事务参与者的协调，统一决定事务的提交或回滚，从而能够有效的保证分布式数据一致性。 阶段一：提交事务请求 事务询问 协调者向所有参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应。 执行事务 各参与者结点执行事务操作，并将Undo和Redo信息记入事务日志中。 各参与者向协调者反馈事务询问的响应 如果参与者成功执行了事务操作，那么就反馈给协调者Yes响应，表示事务可以执行；如果参与者没有成功执行事务，那么就反馈给协调者No响应，表示事务不可以执行。 由于上面讲述的内容在形式上近似是协调者组织各参与者对一次事务曹组的投票表态过程，因此二阶段提交协议的阶段一也被称为“投票阶段”，即各参与者投票表明是否要继续执行接下去的事务提交操作。 阶段二：执行事务提交可能一：执行事务提交 假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务提交。 1. 发送提交请求 协调者向所有参与者节点发出Commit请求 2. 事务提交 参与者接收到Commit请求后，会正式执行事务提交操作，并在完成提交之后释放在整个事务执行期间占用的事务资源。 3. 反馈事务提交结果 参与者在完成事务提交之后，向协调者发送Ack消息 4. 完成事务 协调者接收到所有参与者反馈的Ack消息后，完成事务 可能二：中断事务 假如任何一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，那么就会中断事务。 1. 发送回滚请求 协调者向所有参与者节点发出Rollback请求 2. 事务回滚 参与者接收到Rollback请求后，会利用其再阶段一中记录的Undo信息执行事务回滚操作，并在完成回滚之后释放在整个事务执行期间占用的资源。 3. 反馈事务回滚结果 参与者在完成事务回滚之后，向协调者发送Ack消息 4. 中断事务 协调者接收到所有参与者反馈的Ack消息后，完成事务中断 简单地讲，二阶段提交讲一个事务的处理过程分成了投票和执行两个阶段，其核心是对每个事务都采用先尝试后提交的处理方式。 优缺点二阶段提交协议的优点：原理简单、实现方便二阶段提交协议的缺点：同步阻塞、单点问题、脑裂、太过保守同步阻塞 执行过程中，所有参与该事务操作的逻辑都处于阻塞状态，也就是说，各个参与者在等待其他参与者响应的过程中，将无法进行其他任何操作。 单点问题 协调者的角色在整个二阶段提交协议中起到了非常重要的作用。一旦协调者出现问题，那么整个二阶段提交流程无法运转，更为严重的是，如果协调者是在阶段二中出现问题的话，那么其他参与者将会一直处于锁定事务资源的状态中，而无法继续完成事务操作。 数据不一致 在二阶段提交协议的阶段二，即执行事务提交的时候，当协调者向所有的参与者发送Commit请求之后，发生了局部网络异常或者是协调者在尚未发送完Commit请求之前自身发生了崩溃，导师最终只有部分参与者收到了Commit请求，于是，这部分收到了Commit请求的参与者就会进行事务的提交，而其他没有收到Commit请求的参与者则无法进行事务提交，于是整个分布式系统便出现了数据不一致性现象。 太过保守 二阶段提交协议没有涉及较为完善的容错机制，任意一个节点的失败都会导致整个事务的失败。 二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。 3PC（Three-Phase Commit） 三阶段提交，是2PC的改进版，其将二阶段提交协议的“提交事务请求”过程一分为二，形成了由CanCommit、PreCommit和DoCommit三个阶段组成的视乎处理协议。 三个阶段阶段一：CanCommit 事务询问 协调者向所有的参与者发送一个包含事务内容的CanCommit请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应。 各参与者向协调者反馈事务询问的响应 参与者在接收到来自协调者的CanCommit请求后，正常情况下，如果其自身认为可以顺利执行事务，那么会反馈Yes响应，并进入准备状态，否则反馈No响应。 阶段二：PreCommit在阶段二中，协调者会根据各参与者的反馈情况来决定是否可以进行事务的PreCommit操作，正常情况下，包含两种可能：执行事务预提交 假如协调者从所有参与者获得的反馈都是Yes响应，那么就会执行事务预提交 发送预提交请求 协调者向所有参与者节点发出PreCommit的请求，并进入Prepared阶段。 事务预提交 参与者接收到PreCommit请求后，会执行事务操作，并将Undo和Redo信息记录到事务日志中 各参与者向协调者反馈事务执行的响应 如果参与者成功执行了事务操作，那么就会反馈给协调者Ack响应，同时等待最终的指令：提交（commit）或终止（abort） 中断事务 假如任何一个参与者向协调者反馈了No响应，或者在等待超市之后，协调者尚无法接受所有参与者的反馈响应，那么就会中断事务。 发送中断请求 协调者向所有参与者节点发出abort请求 中断事务 无论是收到来自协调者的abort请求，或者是在等待协调者请求过程中超时，参与者都会中断事务。 阶段三：DoCommit该阶段将进行真正的事务提交，会存在以下两种情况：提交阶段 发送提交请求 事务提交 反馈事务提交结果 完成事务 中断事务进入这一阶段，假设协调者处于正常工作状态，并且有任意一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，那么就会中断事务 发送中断请求 事务回滚 反馈事务回滚结果 中断事务 需要注意的是，一旦进入阶段三，可能会存在以下两种故障： 协调者出现问题 协调者和参与者之间的网络出现故障 无论出现哪种情况，最终都会导致参与者无法及时接收到来自协调者的DoCommit或者abort请求，针对这样的异常情况，参与者都会在等待超时之后，继续进行事务提交。 三阶段提交协议优缺点 优点：相较于二阶段提交协议，三阶段提交协议最大的优点就是降低了参与者的阻塞范围，并且能够在出现单点故障后继续达成一致。 缺点： 在参与者接收到PreCommit消息后，如果网络出现分区，此时协调者所在的节点和参与者无法进行正常的网络通信，在这种情况下，该参与者依然会进行事务的提交，这必然出现数据的不一致性。 在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。所以，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。 相对于2PC，3PC主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。 当协调者和参与者都挂的时候： 第二阶段协调者和参与者挂了，挂了的这个参与者在挂之前已经执行了操作。但是由于他挂了，没有人知道他执行了什么操作。 这种情况下，当新的协调者被选出来之后，他同样是询问所有的参与者的情况来觉得是commit还是roolback。这看上去和二阶段提交一样啊？他是怎么解决一致性问题的呢？ 看上去和二阶段提交的那种数据不一致的情况的现象是一样的，但仔细分析所有参与者的状态的话就会发现其实并不一样。我们假设挂掉的那台参与者执行的操作是commit。那么其他没挂的操作者的状态应该是什么？他们的状态要么是prepare-commit要么是commit。因为3PC的第三阶段一旦有机器执行了commit，那必然第一阶段大家都是同意commit。所以，这时，新选举出来的协调者一旦发现未挂掉的参与者中有人处于commit状态或者是prepare-commit的话，那就执行commit操作。否则就执行rollback操作。这样挂掉的参与者恢复之后就能和其他机器保持数据一致性了。（为了简单的让大家理解，笔者这里简化了新选举出来的协调者执行操作的具体细节，真实情况比我描述的要复杂） 简单概括一下就是，如果挂掉的那台机器已经执行了commit，那么协调者可以从所有未挂掉的参与者的状态中分析出来，并执行commit。如果挂掉的那个参与者执行了rollback，那么协调者和其他的参与者执行的肯定也是rollback操作。 所以，再多引入一个阶段之后，3PC解决了2PC中存在的那种由于协调者和参与者同时挂掉有可能导致的数据一致性问题。]]></content>
      <categories>
        <category>Distributed</category>
      </categories>
      <tags>
        <tag>BlockChain</tag>
        <tag>Distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Paxos发展、算法原理]]></title>
    <url>%2F2017%2F09%2F11%2FPaxos%E5%8F%91%E5%B1%95%E3%80%81%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Paxos发展史 Leslie Lamport所提出的Paxos算法是现代分布式系统中的一项重要的基础性技术，得到广泛的应用。 Paxos的整个发展过程大概可以分为三个阶段： 第一阶段：萌芽期，大致是1988-1996年。Liskov等人在PODC上发表了Viewstamped Replication: A New Primary Copy Method to Support Highly-Available Distributed Systems ，提出了一个在副本出现宕机情况下仍能正常工作的主从备份算法，该算法与Paxos在本质上是一致的(The ABCD’s of Paxos)。 第二阶段：1996-2007年。涌现出一批Paxos的不同版本，这些Paxos的变种从不同侧面完善了基础Paxos算法，提升其性能。Liskov等人在1999年提出了PBFT（实用的拜占庭容错算法），这实际上也是Paxos的一个变种，被Lampson称为Byzantine Paxos，该算法对基础Paxos进行了改进，使其可以处理拜占庭错误。 拜占庭将军问题（Byzantine failures），是由莱斯利·兰伯特提出的点对点通信中的基本问题。含义是在存在消息丢失的不可靠信道上试图通过消息传递的方式达到一致性是不可能的。 拜占庭位于如今的土耳其的伊斯坦布尔，是东罗马帝国的首都。由于当时拜占庭罗马帝国国土辽阔，为了防御目的，因此每个军队都分隔很远，将军与将军之间只能靠信差传消息。 在战争的时候，拜占庭军队内所有将军和副官必需达成一致的共识，决定是否有赢的机会才去攻打敌人的阵营。但是，在军队内有可能存有叛徒和敌军的间谍，左右将军们的决定又扰乱整体军队的秩序。在进行共识时，结果并不代表大多数人的意见。这时候，在已知有成员谋反的情况下，其余忠诚的将军在不受叛徒的影响下如何达成一致的协议，拜占庭问题就此形成。 拜占庭假设是对现实世界的模型化，由于硬件错误、网络拥塞或断开以及遭到恶意攻击，计算机和网络可能出现不可预料的行为。拜占庭容错协议必须处理这些失效，并且这些协议还要满足所要解决的问题要求的规范。 Eli Gafni 和 Lamport 在2000年提出了Disk Paxos，这可以认为是Paxos基于磁盘的版本，以支持持久化。 第三阶段：本阶段。Paxos开始在工业界得到了广泛应用。从2006年开始，谷歌公司有两篇影响深远的论文发表在OSDI上，一篇是“Bigtable:A Distributed Storage System for Structured Data”，另一篇“The Chubby lock service for loosely-coupled distributed systems”。两篇论文可以说是揭开了大数据管理的序幕，而Paxos则在大数据管理的核心技术（容错）中扮演了极为重要的角色。 算法原理 Paxos算法维基百科https://en.wikipedia.org/wiki/Paxos_(computer_science)) Paxos算法是基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一。然而，Paxos算法也因为晦涩难懂而臭名昭著。 Paxos算法的目标就是要保证最终有一个提案会被选定，当提案被选定后，进程最终也能获取到被选定的提案。 问题产生的背景 在常见的分布式系统中，总会发生诸如机器宕机或网络异常（包括消息的延迟、丢失、重复、乱序，还有网络分区）等情况。Paxos算法需要解决的问题就是如何在一个可能发生上述异常的分布式系统中，快速且正确地在集群内部对某个数据的值达成一致，并且保证不论发生以上任何异常，都不会破坏整个系统的一致性。 相关概念在Paxos算法中，有四种角色： Client：产生议题者 Proposer ：提议者 Acceptor(Voters)：决策者（投票者） Learner：最终决策学习者，也就是执行者。 一个进程可能充当不止一种角色。上面4种角色中，提议者和决策者是很重要的，其他的2个角色在整个算法中应该算做打酱油的，Proposer就像Client的使者，由Proposer使者拿着Client的议题去向Acceptor提议，让Acceptor来决策。 Proposer拿着Client的议题去向Acceptor提议，让Acceptor来决策。 Proposer提出议题，Acceptor初步接受或者Acceptor初步不接受。 Acceptor初步接受则Proposer再次向Acceptor确认是否最终接受。 Acceptor最终接受或者Acceptor最终不接受。 Learner最终学习的目标是向所有Acceptor学习，如果有多数派个Acceptor最终接受了某提议，那就得到了最终的结果，算法的目的就达到了。 问题描述 假设有一组可以提出（propose）value（value在提案Proposal里）的进程集合。一个一致性算法需要保证提出的这么多value中： 只有一个value被选定（chosen） 如果没有value被提出，就不应该有value被选定 如果一个value被选定，那么所有进程都应该能学习（learn）到这个被选定的value。 对于一致性算法，安全性（safaty）要求如下： 只有被提出的value才能被选定。 只有一个value被选定，并且 如果某个进程认为某个value被选定了，那么这个value必须是真的被选定的那个。 Paxos的目标：保证最终有一个value会被选定，当value被选定后，进程最终也能获取到被选定的value。 每个参与者以任意的速度执行，可能会因为出错而停止，也可能会重启。同时，即使一个提案被选定后，所有的参与者也都有可能失败或重启，因此除非那些失败或重启的参与者可以记录某些信息，否则将无法确定最终的值。 消息在传输过程中可能会出现不可预知的延迟，也可能会重复或丢失，但是消息不会损坏，即消息内容不会被篡改（拜占庭式的问题）。 算法描述Paxos算法分为两个阶段。具体如下：阶段一：(a) Proposer选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare请求。 (b) 如果一个Acceptor收到一个编号为N的Prepare请求，且N大于该Acceptor已经响应过的所有Prepare请求的编号，那么它就会将它已经接受过的编号最大的提案（如果有的话）作为响应反馈给Proposer，同时该Acceptor承诺不再接受任何编号小于N的提案。 阶段二：(a) 如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，那么它就会发送一个针对[N,V]提案的Accept请求给半数以上的Acceptor。注意：V就是收到的响应中编号最大的提案的value，如果响应中不包含任何提案，那么V就由Proposer自己决定。 (b) 如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor没有对编号大于N的Prepare请求做出过响应，它就接受该提案。 eg. Learner学习被选定的valueLearner学习（获取）被选定的value有如下三种方案：]]></content>
      <categories>
        <category>Distributed</category>
      </categories>
      <tags>
        <tag>BlockChain</tag>
        <tag>Distributed</tag>
        <tag>Paxos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CAP定理(原则)以及BASE理论]]></title>
    <url>%2F2017%2F09%2F05%2FCAP%E5%AE%9A%E7%90%86(%E5%8E%9F%E5%88%99)%E4%BB%A5%E5%8F%8ABASE%E7%90%86%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[#CAP定理(原则)以及BASE理论 CAP定理(原则)概念 CAP原则又称CAP定理，指的是在一个分布式系统中， Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可得兼。 1. 数据一致性(consistency) 一致性（C）：在分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本） 在分布式系统中，如果能够做到针对一个数据项的更新操作执行成功后，所有的用户都可以读到其最新的值，那么这样的系统就被认为具有强一致性（严格的一致性）。 2. 服务可用性(availability) 可用性（A）：系统提供的服务必须一致出于可用的状态，对于用户的每一个请求总是能够有限的时间内返回结果。 在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。（对数据更新具备高可用性） 3. 分区容错性(partition-tolerance) 分区容忍性（P）：在网络分区的情况下，被分隔的节点仍能正常对外服务 由于当前的网络硬件肯定会出现延迟丢包等问题，所以分区容忍性是我们必须需要实现的。 以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。 一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。 当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。 提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里。容忍性就提高了。然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。 总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。 根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项。 理解CAP理论的最简单方式是想象两个节点分处分区两侧。 允许至少一个节点更新状态会导致数据不一致，即丧失了C性质。 如果为了保证数据一致性，将分区一侧的节点设置为不可用，那么又丧失了A性质。 除非两个节点可以互相通信，才能既保证C又保证A，这又会导致丧失P性质。 放弃CAP定理 说明 放弃P 如果希望能够避免系统出现分区容错性问题，一种较为简单地做法是将所有的数据（或者仅仅是那些与事务相关的数据）都放在一个分布式节点上。这样的做法虽然无法100%地保证系统不会出错，但至少不会碰到由于网络分区带来的负面影响。但同时需要注意的是，放弃P的同时也就意味着放弃了系统的可扩展性。 放弃A 相对于放弃“分区容错性”来说，放弃可用性则正好相反，其做法是一旦系统遇到网络分区或其他故障时，那么收到影响的服务需要等待一定的时间，因此在等待期间系统无法对外提供正常的服务，即不可用。 放弃C 这里所说的放弃一致性，并不是完全不需要数据一致性，而是放弃数据的强一致性，而保留数据的最终一致性。这样的系统无法保证数据保持实时的一致性，但是能够承诺的是，数据最终会达到一个一致的状态。这里就引入了一个时间窗口的概念，具体多久能够达到数据一致取决于系统的设计，主要包括数据副本在不同节点之间的复制时间长短。 CA非0/1的选择 P 是必选项，那3选2的选择题不就变成数据一致性(consistency)、服务可用性(availability) 2选1？工程实践中一致性有不同程度，可用性也有不同等级，在保证分区容错性的前提下，放宽约束后可以兼顾一致性和可用性，两者不是非此即彼。 CAP定理证明中的一致性指强一致性，强一致性要求多节点组成的被调要能像单节点一样运作、操作具备原子性，数据在时间、时序上都有要求。如果放宽这些要求，还有其他一致性类型： 序列一致性(sequential consistency)：不要求时序一致，A操作先于B操作，在B操作后如果所有调用端读操作得到A操作的结果，满足序列一致性 最终一致性(eventual consistency)：放宽对时间的要求，在被调完成操作响应后的某个时间点，被调多个节点的数据最终达成一致 可用性在CAP定理里指所有读写操作必须要能终止，实际应用中从主调、被调两个不同的视角，可用性具有不同的含义。当P(网络分区)出现时，主调可以只支持读操作，通过牺牲部分可用性达成数据一致。 工程实践中，较常见的做法是通过异步拷贝副本(asynchronous replication)、quorum/NRW，实现在调用端看来数据强一致、被调端最终一致，在调用端看来服务可用、被调端允许部分节点不可用(或被网络分隔)的效果。 延时(latency)，它是衡量系统可用性、与用户体验直接相关的一项重要指标[16]。CAP理论中的可用性要求操作能终止、不无休止地进行，除此之外，我们还关心到底需要多长时间能结束操作，这就是延时，它值得我们设计、实现分布式系统时单列出来考虑。 延时与数据一致性也是一对“冤家”，如果要达到强一致性、多个副本数据一致，必然增加延时。加上延时的考量，我们得到一个CAP理论的修改版本PACELC：如果出现P(网络分区)，如何在A(服务可用性)、C(数据一致性)之间选择；否则，如何在L(延时)、C(数据一致性)之间选择。 CAP原理实例推导 以数据库为例，讨论一下CAP原理: 单实例 一种最简单的情况：”单数据库实例”。 这也是最常见的小站点、个人博客、小论坛的架构。 可以很容易分析出来，由于单实例，所以不存在“网络分区”、“不一致”， 但单点故障后会导致整个数据库瘫痪，所以可用性不能保证。 这就是CAP定理中的，保证”C”和”P”，舍弃”A”。 所有单机版的系统都属于这个范畴，例如MySQL、memcached、redis。 分片Sharding 为了提升可用性，我们在实际生产环境下经常会在客户端应用一些哈希算法，进行数据分片存放，如下图所示： 由于数据是分片存储在每个数据库中，所以依旧能保证数据一致性。由于数据库之间没有互相通信，并不依赖彼此的存在，所以分区可容忍性依旧没有破坏。那么可用性呢？很多时候会有人直接拍脑袋，这里我们用数学的方式来解答这个问题。 假设，集群有两台服务器，数据分布均匀，我们数据库实例宕机的概率是p。 那么这种利用哈希进行数据分片的集群的可用性为：0.5*p*(1-p) + 0.5*p*(1-p) + 1*p*p = p 即使，数据分布均匀或者集群数量增大，结果也是一样的：“集群可用性依旧为p”。 那我们折腾了半天，CAP和单机竟然是一样的，这种情况下CAP各项指标虽然没有提升，但好处是： 单个服务器宕机只会导致服务降级； 集群有了扩容缩容的可能性，这就叫做scalability。 这种分布式的方式常用于：分布式memcached、redis、传统的数据库Sharding、BigTable (列存储式数据库)、Hypertable (列存储式数据库)、HBase (列存储式数据库)、MongoDB (文档式数据库)、Terrastore (文档式数据库)、Redis (KV数据库)、Scalaris (KV数据库)、MemcacheDB (KV数据库)、Berkeley DB (KV数据库) 多副本写入 Client多副本写入，就是Client在写数据库的时候对多个数据库进行写入，并且在两个都写入成功后才认为成功。 由于数据存在多个副本，这种方式会大大的提高读取的可用性。但由于写入的时候要多写， 副本所在的所有实例都必须可用才能成功。所以写入的可用性反而下降了。 假设单机数据库的故障率为p（p&lt;1.0），那么单机数据库的可用性为1-p。 总结就是： 在写入的场景下，一致性( C )和分区可容忍性( P )没有变化，可用性( A )反而有所下降， 从1-p降低到1-2p-p² 在读取的场景下，一致性( C )和分区可容忍性( P )依旧没有变化，可用性( A )有所上升， 从1-p上升到1-p² “Client多副本写入”这种写入方式非常适合于在”读多写少”的场景下提高可用性。 为了改善写入时糟糕的可用性，这种方式还有一些“变种”，例如： 写成功部分副本就返回成功，剩下的副本写入不保证结果。这样做的结果就是牺牲一定的一致性( C )，换取可用性( A )的提升。 总的来说，这种方式属于广义的”Sharding”的范畴，除去上述的缺点还有一个较大的问题就是： 假设副本数为n，Client写入单实例的耗时为t，多副本写入的耗时就是n*t；当n &gt; 1的时候会成倍的影响Client的写入性能。 集群Clustering 为了解决Client写入慢调用复杂等问题，我们引入了集群方案，也就是Clustering。 Clustering和Sharding对比如下： 多副本模式1： 我们可以看到，由于多个副本写入成功才返回，这种方式一致性( C )依旧是保证的。 但写入可用性( A )和分区可容忍性( P )相对于单机均会下降。换来的是： 较为简单的API，客户端不用关注“多写”问题； 读取操作的高可用(HA)。 由于上述方案是强一致性( C )的，这种应用场景常见于金融系统，这种这方面典型的代表有：ZooKeeper (KV数据库)、Vertica (列存储式数据库)、Aster Data (关系型数据库)、Greenplum (关系型数据库) 多副本模式2： 类似”Sharding”中我们采用的方案，生产环境线上的数据库也往往采用放弃一定的一致性( C ) ，来提高可用性( A )和分区可容忍性( P )。 可以看到，由于大多数互联网公司的需求不是要求强一致性( C )， 所以通过放弃一致性，达到更高的可用性( A )和分区可容忍性 ( P )成了目前市面上大多数NoSQL数据库的核心思想。 在Amazon著名的分布式数据库Dynamo中，就是采用类似的方法：”3副本，写入2个成功后就返回成功，剩下的1个副本后续再进行同步”，我们称这种模式叫”最终一致性”。 这方面典型的代表还有：Dynamo (KV数据库)、Voldemort (KV数据库)、Tokyo Cabinet (KV数据库)、KAI (KV数据库)、Cassandra (列存储式数据库)、CouchDB (文档式数据库)、SimpleDB (文档式数据库)、Riak (文档式数据库)、MooseFS (类GFS分布式文件系统) 我们可以看到，系统设计就像穷人家的被子，盖住头和左脚就露出右脚，盖住头和右脚就露出左脚…… 即使你再有钱也不可能将CAP同时100%满足。 我们可以通过用更高可靠性的服务器、更可靠的网络设备达到CAP同时提升。 BASE理论概念 BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的简写，BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的结论，是基于CAP定理逐步演化而来的，其核心思想是即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。 Basically Availble –基本可用 支持分区失败（Sharding碎片划分数据库），出了问题服务仅降级（部分不可用）。 基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性——但请注意，这绝不等价于系统不可用，以下两个就是“基本可用”的典型例子： 响应时间上的损失：正常情况下，一个在线搜索引擎需要0.5秒内返回给用户相应的查询结果，但由于出现异常（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1~2秒。 功能上的损失：正常情况下，在一个电子商务网站上进行购物，消费者几乎能够顺利地完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。 Soft-state –软状态/柔性 事务”Soft state” 可以理解为”无连接”的, 而 “Hard state” 是”面向连接”的。软状态就是可以有一段时间不同步，异步。 软状态，指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 Eventual Consistency –最终一致性 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 亚马逊首席技术官Werner Vogels在于2008年发表的一篇文章中对最终一致性进行了非常详细的介绍。他认为最终一致性时一种特殊的弱一致性：系统能够保证在没有其他新的更新操作的情况下，数据最终一定能够达到一致的状态，因此所有客户端对系统的数据访问都能够获取到最新的值。同时，在没有发生故障的前提下，数据达到一致状态的时间延迟取决于网络延迟，系统负载和数据复制方案设计等因素。 总的来说，BASE理论面向的是大型高可用可扩展的分布式系统，和传统事务的ACID【原子性（atomicity，或称不可分割性）、一致性（consistency）、隔离性（isolation，又称独立性）、持久性（durability）】特性是相反的，它完全不同于ACID的强一致性模型，而是提出通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性与BASE理论往往又会结合在一起使用。 http://www.frankyang.cn/2017/09/05/cap-base/]]></content>
      <categories>
        <category>Distributed</category>
      </categories>
      <tags>
        <tag>BlockChain</tag>
        <tag>Distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据处理——Trie树]]></title>
    <url>%2F2017%2F09%2F04%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-Trie%E6%A0%91%2F</url>
    <content type="text"><![CDATA[大数据处理——Trie树1.1、什么是Trie树 Trie树，即字典树，又称单词查找树或键树，是一种树形结构，是一种哈希树的变种。典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：最大限度地减少无谓的字符串比较，查询效率比哈希表高。 Trie的核心思想是空间换时间。利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。 它有3个基本性质： 1. 根节点不包含字符，除根节点外每一个节点都只包含一个字符。 2. 从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串。 3. 每个节点的所有子节点包含的字符都不相同。 1.2、树的构建举个在网上流传颇广的例子，如下： 题目：给你100000个长度不超过10的单词。对于每一个单词，我们要判断他出没出现过，如果出现了，求第一次出现在第几个位置。 分析：这题当然可以用hash来解决，但是本文重点介绍的是trie树，因为在某些方面它的用途更大。比如说对于某一个单词，我们要询问它的前缀是否出现过。这样hash就不好搞了，而用trie还是很简单。 现在回到例子中，如果我们用最傻的方法，对于每一个单词，我们都要去查找它前面的单词中是否有它。那么这个算法的复杂度就是O(n^2 )。显然对于100000的范围难以接受。现在我们换个思路想。假设我要查询的单词是abcd，那么在他前面的单词中，以b，c，d，f之类开头的我显然不必考虑。而只要找以a开头的中是否存在abcd就可以了。同样的，在以a开头中的单词中，我们只要考虑以b作为第二个字母的，一次次缩小范围和提高针对性，这样一个树的模型就渐渐清晰了。 好比假设有b，abc，abd，bcd，abcd，efg，hii 这6个单词，我们构建的树就是如下图这样的： 当时第一次看到这幅图的时候，便立马感到此树之不凡构造了。单单从上幅图便可窥知一二，好比大海搜人，立马就能确定东南西北中的到底哪个方位，如此迅速缩小查找的范围和提高查找的针对性，不失为一创举。 ok，如上图所示，对于每一个节点，从根遍历到他的过程就是一个单词，如果这个节点被标记为红色，就表示这个单词存在，否则不存在。 那么，对于一个单词，我只要顺着他从根走到对应的节点，再看这个节点是否被标记为红色就可以知道它是否出现过了。把这个节点标记为红色，就相当于插入了这个单词。 这样一来我们查询和插入可以一起完成，所用时间仅仅为单词长度，在这一个样例，便是10。 我们可以看到，trie树每一层的节点数是26^i 级别的。所以为了节省空间。我们用动态链表，或者用数组来模拟动态。空间的花费，不会超过单词数×单词长度。 1.3、前缀查询 已知n个由小写字母构成的平均长度为10的单词,判断其中是否存在某个串为另一个串的前缀子串。下面对比3种方法： 最容易想到的：即从字符串集中从头往后搜，看每个字符串是否为字符串集中某个字符串的前缀，复杂度为O(n^2)。 使用hash：我们用hash存下所有字符串的所有的前缀子串，建立存有子串hash的复杂度为O(n*len)，而查询的复杂度为O(n)* O(1)= O(n)。 使用trie：因为当查询如字符串abc是否为某个字符串的前缀时，显然以b,c,d….等不是以a开头的字符串就不用查找了。所以建立trie的复杂度为O(n*len)，而建立+查询在trie中是可以同时执行的，建立的过程也就可以成为查询的过程，hash就不能实现这个功能。所以总的复杂度为O(n*len)，实际查询的复杂度也只是O(len)。(说白了，就是Trie树的平均高度h为len，所以Trie树的查询复杂度为O（h）=O（len）。好比一棵二叉平衡树的高度为logN，则其查询，插入的平均时间复杂度亦为O(logN))。 1.4、查询 Trie树是简单但实用的数据结构，通常用于实现字典查询。我们做即时响应用户输入的AJAX搜索框时，就是Trie开始。本质上，Trie是一颗存储多个字符串的树。相邻节点间的边代表一个字符，这样树的每条分支代表一则子串，而树的叶节点则代表完整的字符串。和普通树不同的地方是，相同的字符串前缀共享同一条分支。下面，再举一个例子。给出一组单词，inn, int, at, age, adv, ant, 我们可以得到下面的Trie： 可以看出： 每条边对应一个字母。 每个节点对应一项前缀。叶节点对应最长前缀，即单词本身。 单词inn与单词int有共同的前缀“in”, 因此他们共享左边的一条分支，root-&gt;i-&gt;in。同理，ate, age, adv, 和ant共享前缀”a”，所以他们共享从根节点到节点”a”的边。 查询操纵非常简单。比如要查找int，顺着路径i -&gt; in -&gt; int就找到了。 搭建Trie的基本算法也很简单，无非是逐一把每则单词的每个字母插入Trie。插入前先看前缀是否存在。如果存在，就共享，否则创建对应的节点和边。比如要插入单词add，就有下面几步： 1. 考察前缀”a”，发现边a已经存在。于是顺着边a走到节点a。 2. 考察剩下的字符串”dd”的前缀”d”，发现从节点a出发，已经有边d存在。于是顺着边d走到节点ad 3. 考察最后一个字符”d”，这下从节点ad出发没有边d了，于是创建节点ad的子节点add，并把边ad-&gt;add标记为d。 1.5、Trie树的应用第一：词频统计；第二: 前缀匹配；第三：去重 适用范围：数据量大，重复多，但是数据种类小可以放入内存 基本原理及要点：实现方式，节点孩子的表示方式 扩展：压缩实现。 面试题有： 有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。要你按照query的频度排序。 1000万字符串，其中有些是相同的(重复),需要把重复的全部去掉，保留没有重复的字符串。请问怎么设计和实现？ 寻找热门查询：查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个，每个不超过255字节。 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。 用trie树统计每个词出现的次数，时间复杂度是O(n*le)（le表示单词的平准长度），然后是找出出现最频繁的前10个词，可以用堆来实现。 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。 每个结点增加个count变量，查询、插入时维护count变量进行词频统计，用一个最小堆进行维护最高频的100个词的频率 寻找热门查询：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录，这些查询串的重复读比较高，虽然总数是1千万，但是如果去除重复和，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就越热门。请你统计最热门的10个查询串，要求使用的内存不能超过1G。(1) 请描述你解决这个问题的思路；(2) 请给出主要的处理流程，算法，以及算法的复杂度。 http://www.frankyang.cn/2017/08/21/trie-tree/]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Trie</tag>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux进程状态转换图]]></title>
    <url>%2F2017%2F09%2F04%2FLinux%E8%BF%9B%E7%A8%8B%E7%8A%B6%E6%80%81%2F</url>
    <content type="text"><![CDATA[Linux进程状态Linux内核中的进程状态 ◆运行状态（TASK_RUNNING） 指正在被CPU运行或者就绪的状态。这样的进程被成为runnning进程。运行态的进程可以分为3种情况：内核运行态、用户运行态、就绪态。 ◆可中断睡眠状态（TASK_INTERRUPTIBLE） 处于等待状态中的进程，一旦被该进程等待的资源被释放，那么该进程就会进入运行状态。 ◆不可中断睡眠状态（TASK_UNINTERRUPTIBLE） 该状态的进程只能用wake_up()函数唤醒。 ◆暂停状态（TASK_STOPPED） 当进程收到信号SIGSTOP、SIGTSTP、SIGTTIN或SIGTTOU时就会进入暂停状态。可向其发送SIGCONT信号让进程转换到可运行状态。 ◆僵死状态（TASK_ZOMBIE） 当进程已经终止运行，但是父进程还没有询问其状态的情况。 http://www.frankyang.cn/2017/05/13/linux-jin-cheng-zhuang-tai/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Process</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[密码学hash函数-SHA256-512]]></title>
    <url>%2F2017%2F09%2F04%2F%E5%AF%86%E7%A0%81%E5%AD%A6hash%E5%87%BD%E6%95%B0-SHA256-512%2F</url>
    <content type="text"><![CDATA[Hash函数又称哈希函数、散列函数、杂凑函数。它是一种单向密码体制，即从一个从明文到密文的不可逆映射，只有加密过程，没有解密过程。Hash函数H将可变长度的数据块M作为输入，产生固定长度的Hash值h=H(M)。在安全应用中使用的Hash函数称为密码学Hash函数。（单向性）、（抗碰撞性） 弱抗碰撞性：给定一个消息M，要找到另一个消息M’，使得H(M)=H(M’)很难。强抗碰撞性：要找到两个随机明文M和M’，使得H(M)=H(M’)很难。 Hash函数特点： 易压缩 易计算 单向性 抗碰撞性 高灵敏性 密码学Hash函数的应用范围消息认证消息认证是用来验证消息完整性的一种机制或服务。消息认证确保收到的数据确实和发送时的一样（即没有修改、插入、删除或重放）。当Hash函数用于提供消息认证功能时，Hash函数值通常称为消息摘要。 数字签名在进行数字签名过程中使用用户的私钥加密消息的Hash值，其他任何知道该用户公钥的人都能够通过数字签名来验证消息的完整性。 其他应用单向口令文件。入侵检测。病毒检测。构建随机函数（PRF）或用做伪随机数发生器（PRNG） 安全Hash算法（SHA）安全散列算法SHA（Secure Hash Algorithm）是美国国家安全局 （NSA） 设计，美国国家标准与技术研究院（NIST） 发布的一系列密码散列函数，包括 SHA-1、SHA-224、SHA-256、SHA-384 和 SHA-512 等变体。主要适用于数字签名标准（DigitalSignature Standard DSS）里面定义的数字签名算法（Digital Signature Algorithm DSA）。 SHA-1SHA-1产生160位的Hash值。SHA1始终把消息当成一个位（bit）字符串来处理。 SHA-2Hash值长度依次为256位、384位和512位，分别称为SHA-256、SHA-384和SHA-512，这些算法统称为SHA-2。SHA-2同SHA-1类似，都使用同样的迭代结构和同样的模算术运算与二元逻辑操作。 SHA-1 SHA-224 SHA-256 SHA-384 SHA-512 消息摘要长度 160 224 256 384 512 消息长度 &lt; 2^64 &lt; 2^64 &lt; 2^64 &lt; 2^128 &lt; 2^128 分组长度 512 512 512 1024 1024 字长度 32 32 32 64 64 步骤数 80 64 64 80 80 SHA-256算法SHA-256算法的输入是最大长度小于2^64 位的消息，输出是256位的消息摘要，输入消息以512位的分组为单位进行处理。步骤如下（1）消息填充 添加一个“1”和若干个“0”使其长度模512与448同余（即长度≡448(mod 512)）.在消息后附加64位的长度块，其值为填充前消息的长度。从而产生长度为512整数倍的消息分组，填充后消息的长度最多为2^64位。 （2）初始化链接变量 链接变量的中间结果和最终结果存储于256位的缓冲区中，缓冲区用8个32位的寄存器A、B、C、D、E、F、G和H表示，输出仍放在缓冲区以代替旧的A、B、C、D、E、F、G、H。首先要对链接变量进行初始化，初始链接变量存储于8个寄存器A、B、C、D、E、F、G和H中：A = 0x6a09e667 E = 0x510e527fB = 0xbb67ae85 F = 0x9b05688cC = 0x3c6ef372 G = 0x1f83d9abD = 0xa54ff53a H = 0x5be0cd19初始链接变量是取自前8个素数（2、3、5、7、11、13、17、19）的平方根的小数部分其二级制表示的前32位。 （3）处理主循环模块 消息块是以512位分组为单位进行处理的，要进行64步循环操作（如图）。每一轮的输入均为当前处理的消息分组和得到的上一轮输出的256位缓冲区A、B、C、D、E、F、G、H的值。每一步中均采用了不同的消息字和常数。 （4）得出最终的Hash值 所有512位的消息块分组都处理完以后，最后一个分组处理后得到的结果即为最终输出的256位的消息摘要。 步函数是SHA-256中最为重要的函数，也是SHA-256中最关键的部件。其运算过程如下图：每一步都会生成两个临时变量，即T1、T2：$$T_1=\Sigma_1(E)+Ch(E,F,G)+H+W_t+K_t$$$$T_2=\Sigma_0(A)+Maj(A,B,C) mod 2^{32}$$根据T1、T2的值，对寄存器A、E进行更新。A、B、C、D、E、F、G的输入值则一次赋值给B、C、D、F、G、H。$A=(T_1+T_2) mod 2^{32}$ $E=(D+T_2) mod 2^{32}$其中$Ch(E,F,G) = (E\wedge F)⊕(\overline{E}\wedge G)$$Maj(A,B,C) = (A\wedge B)\bigoplus (A\wedge C)\bigoplus (B\wedge C)$$\Sigma_0(A) = ROTR^{2}(A)\bigoplus ROTR^{13}(A)\bigoplus ROTR^{22}(A)$$\Sigma_1(E) = ROTR^{6}(E)\bigoplus ROTR^{11}(E)\bigoplus ROTR^{25}(E)$且ROTR^n (E)表示对32位的变量x循环右移n位。Kt的获取方法是取前64个素数（2，3，5，7……）立方根的小数部分，将其转换为二进制，然后取这64个数的前64位作为Kt。其作用是提供了64位随机串集合以消除输入数据里的任何规则性。对于每个输入分组导出的消息分组Wt，前16个消息字Wt（0&lt;=t&lt;=15）直接按照消息输入分组对应的16个32位字，其他的则按照如下公式来计算得出：$$W_{t} = W_{t-16} + \sigma 0(W{t-15}) + W_{t-7} + \sigma 1(W{t-2}) , 16\leqslant t\leqslant 63$$其中：$\sigma _0(x) = ROTR^{7}(x) \bigoplus ROTR^{18}(x) \bigoplus SHR^3(x)$$\sigma _1(x) = ROTR^{17}(x) \bigoplus ROTR^{19}(x) \bigoplus SHR^{10}(x)$式中，$SHR^{10}(x)$表示32位的变量x右移n位，其导出方法如图： SHA-512逻辑算法的输入时最大长度小于2^128 位的消息，输出是512位的消息摘要，输入消息1024位的分组为单位进行处理。步骤1：附加填充位 填充消息使其长度模1024与896同余（即长度≡896(mod 1024)），即使消息已经满足上述长度要求，仍然需要进行填充，因此填充位数在1~1024之间，填充由一个1和后续的0组成。 步骤2：附加长度 在消息后附加一个128位的块，将其视为128位的无符号整数（最高有效字节在前），它包含填充前消息的长度。 前两步的结果产生了一个长度为1024整数倍的消息。 步骤3：初始化Hash缓冲区 Hash函数的中间结果和最终结果保存于512位的缓冲区中，缓冲区用8个64位的寄存器（a,b,c,d,e,f,g,h）表示，并将这些寄存器初始化为下列64位的整数（十六进制值）：a = 6A09E667F3BCC908 e = 510E527FADE682D1b = BB67AE8584CAA73B f = 9B05688C2B3E6C1Fc = 3C6EF372FE94F82B g = 1F83D9ABFB41Bd6Bd = A54FF53A5F1D36F1 h = 5BE0CD19137E2179这些值以高位在前格式存储，也就是说，字的最高有效字节存于低地址字节位置（最左面）。这些字的获取方式如下：前8个素数取平方根，取小数部分的前64位。 步骤4：以1024位的分组（128个字节）为单位处理消息 算法的核心是具有80轮运算的模块。每一轮都把512位缓存区的值abcdefgh作为输入，并更新缓冲区的值。第一轮时，缓冲区里的值是中间值Hi-1。每一轮，如第t轮，使用一个64位的值Wt，该值由当前被处理的1024位消息分组Mi导出，导出算法是下面将要讨论的消息扩展算法（如下图）。每一轮还将使用附加的常数Kt，其中0&lt;=t&lt;=79，用来使每轮的运算不同。这些常数提供了64位随机串集合，可以初步消除输入数据里的统计规律。第80轮的输出和第一轮的输入Hi-1相加产生Hi。缓冲区的8个字和Hi-1中对应的字分别进行模2^64的加法运算。 步骤5：输出 所有的N个1024位分组都处理完以后，从第N阶段输出的是512位的消息摘要。 总结SHA-512的运算如下：H0 = IVHi = SUM64(Hi-1, abcdefghi)MD = HN其中，IV为第三步里中定义的abcdefgh缓冲区的初始值；abcdefghi为第i个消息分组处理的最后一轮的输出；N为消息（包括填充和长度域）中的分组数；SUM64为对输入对中的每个字进行独立的模2^64加；MD为最后的消息摘要值。 http://www.frankyang.cn/2017/09/02/mi-ma-xuehash-han-shusha256512/]]></content>
      <categories>
        <category>Cryptography</category>
      </categories>
      <tags>
        <tag>Cryptography</tag>
        <tag>hash</tag>
        <tag>SHA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据处理-Bitmap]]></title>
    <url>%2F2017%2F09%2F04%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-Bitmap%2F</url>
    <content type="text"><![CDATA[MapReduce是一种编程模型，用于大规模数据集（大于1TB）的并行运算。概念”Map（映射）”和”Reduce（归约）” Bit-map空间压缩和快速排序去重1. Bit-map的基本思想 32位机器上，对于一个整型数，比如int a=1 在内存中占32bit位，这是为了方便计算机的运算。但是对于某些应用场景而言，这属于一种巨大的浪费，因为我们可以用对应的32bit位对应存储十进制的0-31个数，而这就是Bit-map的基本思想。Bit-map算法利用这种思想处理大量数据的排序、查询以及去重。 Bitmap在用户群做交集和并集运算的时候也有极大的便利。 2. Bit-map应用之快速排序 假设我们要对0-7内的5个元素(4,7,2,5,3)排序（这里假设这些元素没有重复）,我们就可以采用Bit-map的方法来达到排序的目的。要表示8个数，我们就只需要8个Bit（1Bytes），首先我们开辟1Byte的空间，将这些空间的所有Bit位都置为0， 对应位设置为1: 遍历一遍Bit区域，将该位是一的位的编号输出（2，3，4，5，7），这样就达到了排序的目的，时间复杂度O(n)。 优点： 运算效率高，不需要进行比较和移位； 占用内存少，比如N=10000000；只需占用内存为N/8=1250000Byte=1.25M。 缺点： 所有的数据不能重复。即不可对重复的数据进行排序和查找。 3. Bit-map应用之快速去重 2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。 首先，根据“内存空间不足以容纳这2.5亿个整数”我们可以快速的联想到Bit-map。下边关键的问题就是怎么设计我们的Bit-map来表示这2.5亿个数字的状态了。其实这个问题很简单，一个数字的状态只有三种，分别为不存在，只有一个，有重复。因此，我们只需要2bits就可以对一个数字的状态进行存储了，假设我们设定一个数字不存在为00，存在一次01，存在两次及其以上为11。那我们大概需要存储空间几十兆左右。 接下来的任务就是遍历一次这2.5亿个数字，如果对应的状态位为00，则将其变为01；如果对应的状态位为01，则将其变为11；如果为11，,对应的转态位保持不变。 最后，我们将状态位为01的进行统计，就得到了不重复的数字个数，时间复杂度为O(n)。 4. Bit-map应用之快速查询 同样，我们利用Bit-map也可以进行快速查询，这种情况下对于一个数字只需要一个bit位就可以了，0表示不存在，1表示存在。假设上述的题目改为，如何快速判断一个数字是够存在于上述的2.5亿个数字集合中。 同之前一样，首先我们先对所有的数字进行一次遍历，然后将相应的转态位改为1。遍历完以后就是查询，由于我们的Bit-map采取的是连续存储（整型数组形式，一个数组元素对应32bits），我们实际上是采用了一种分桶的思想。一个数组元素可以存储32个状态位，那将待查询的数字除以32，定位到对应的数组元素（桶），然后再求余（%32），就可以定位到相应的状态位。如果为1，则代表改数字存在；否则，该数字不存在。 5. Bit-map扩展——Bloom Filter(布隆过滤器) 当一个元素被加入集合中时,通过k各散列函数将这个元素映射成一个位数组中的k个点,并将这k个点全部置为1. 有一定的误判率–在判断一个元素是否属于某个集合时,有可能会把不属于这个集合的元素误判为属于这个集合.因此,它不适合那些”零误判”的应用场合.在能容忍低误判的应用场景下,布隆过滤器通过极少的误判换区了存储空间的极大节省. Bloom Filter使用k个相互独立的哈希函数（Hash Function），它们分别将集合中的每个元素映射到{1,…,m}的范围中。对任意一个元素x，第i个哈希函数映射的位置hi(x)就会被置为1（1≤i≤k）。注：如果一个位置多次被置为1，那么只有第一次会起作用，后面几次将没有任何效果。在判断y是否属于这个集合时，对y应用k次哈希函数，若所有hi(y)的位置都是1（1≤i≤k），就认为y是集合中的元素，否则就认为y不是集合中的元素。 6. 总结 使用Bit-map的思想，我们可以将存储空间进行压缩，而且可以对数字进行快速排序、去重和查询的操作。Bloom Fliter是Bit-map思想的一种扩展，它可以在允许低错误率的场景下，大大地进行空间压缩，是一种拿错误率换取空间的数据结构。 7. 应用 适用范围：可进行数据的快速查找，判重，删除，一般来说数据范围是int的10倍以下 基本原理及要点：使用bit数组来表示某些元素是否存在，比如8位电话号码 扩展：bloom filter可以看做是对bit-map的扩展 问题实例：1、已知某个文件内包含一些电话号码，每个号码为8位数字，统计不同号码的个数。 8位最多99 999 999，大概需要99m个bit，大概10几M字节的内存即可。 2、在2.5亿个整数中找出不重复的整数，内存不足以容纳这2.5亿个整数。 方案1：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32*2bit=1GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。 http://www.frankyang.cn/2017/08/21/da-shu-ju-chu-libitmap/]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Bitmap</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几种垃圾回收GC概述]]></title>
    <url>%2F2017%2F09%2F04%2F%E5%87%A0%E7%A7%8D%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6GC%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[垃圾回收机制引用计数回收器（Reference Counting Collector） 原理是在每个对象内部维护一个整数值，叫做这个对象的引用计数，当对象被引用时引用计数加一，当对象不被引用时引用计数减一。当引用计数为 0 时，自动销毁对象。 目前引用计数法主要用在 c++ 标准库的 std::shared_ptr 、微软的 COM 、Objective-C 和 PHP 中。 计数器表示资源（内存中的对象）的使用次数，当计数器变成零的时候就可以将该资源销毁。在向已有的系统添加垃圾回收器时，开发人员通常会选择计数回收器，因为这种方式最容易与已有的资源管理器和代码集成在一起。 不过引用计数算法存在很多问题。最大的一个问题是，它无法解决循环引用问题。 循环引用是指对象 A 和对象 B 互相持有对方的引用。这样两个对象的引用计数都不是 0 ，因此永远不能被收集，造成内存泄露。除此之外，引用计数需要额外的开销。 虽然现代CPU的运算能力很强，但内存并不快，而计数器的运算需要频繁使用内存。因为计数器需要不断被更新，所以它们不是只读的，而且不能保证线程安全。 引用计数器算法是一种摊销算法（将开销分摊给了程序），不过它是偶然性的摊销，无法保证响应时间。例如，假设程序正在处理一个很大的树结构，最后一个使用这个树的程序会触发销毁操作，根据墨菲定律，其延迟会比期望的要高。 标记清除回收器（Mark Sweep Collector） 标记清除回收算法解决了一些在引用计数算法中存在的问题。它解决了循环引用问题，而且开销要小得多，因为它不需要维护计数器。 这个算法分为两步，标记和清除。 1. 标记：从程序的根节点开始， 递归地 遍历所有对象，将能遍历到的对象打上标记。 2. 清除：讲所有未标记的的对象当作垃圾销毁。 不过，它无法立即检测到垃圾。这个算法还有一个缺陷，就是人们常常说的 STW 问题（Stop The World）。因为算法在标记时必须暂停整个程序，否则其他线程的代码可能会改变对象状态，从而可能把不应该回收的对象当做垃圾收集掉。 标记清除算法有更高的一致性要求，而且难以将其集成到已有的系统中。在标记阶段，回收器要求遍历所有的存活对象，包括封装在对象里的数据。如果某个对象不支持回收器的遍历访问，那么使用这种回收器就会存在风险。 复制回收器（Copying Collector） 节点复制也是基于追踪的算法。其将整个堆等分为两个半区（semi-space），一个包含现有数据，另一个包含已被废弃的数据。节点复制式垃圾收集从切换（flip）两个半区的角色开始，然后收集器在老的半区，也就是 Fromspace 中遍历存活的数据结构，在第一次访问某个单元时把它复制到新半区，也就是 Tospace 中去。在 Fromspace 中所有存活单元都被访问过之后，收集器在 Tospace 中建立一个存活数据结构的副本，用户程序可以重新开始运行了。 优点 所有存活的数据结构都缩并地排列在 Tospace 的底部，这样就不会存在内存碎片的问题。 获取新内存可以简单地通过递增自由空间指针来实现。 缺点 内存得不到充分利用，总有一半的内存空间处于浪费状态。 Copying算法的效率跟存活对象的数目多少有很大的关系，如果存活对象很多，那么Copying算法的效率将会大大降低。 标记整理回收器（Mark Compact Collector） 为了解决Copying算法的缺陷，充分利用内存空间，提出了Mark-Compact算法。该算法标记阶段和Mark-Sweep一样，但是在完成标记之后，它不是直接清理可回收对象，而是将存活对象都向一端移动，然后清理掉端边界以外的内存。 标记整理算法清理内存的方式不是通过清除，而是将对象移动到空余的内存空间。对象总是以不变的次序存留在内存里，先分配到内存的对象总处于较低的内存段，不过因为经过移动，对象间的内存间隙被消除。 新创建的对象总是处于内存的高段。这种内存分配器被称为“bump”，类似于栈的分配，只是没有栈那样的大小限制。有些使用bump分配器的系统甚至不用调用栈来存储数据，它们直接在堆里分配调用帧，并把它们看成对象。 从理论上看，这种回收算法的另一个优势在于，程序具有了更好的内存访问模型，这种模型对现代硬件的内存缓存更加友好。不过，相比其他算法，我们很难直观地感受到这种优势，因为引用计数和标记清除算法里所使用的内存分配器虽然很复杂，但它们很健壮，很高效。 标记整理是一种复杂的算法，需要多次遍历所有分配到内存的对象。这种复杂性所带来的主要好处就是极低的内存开销。Oracle的Hotspot虚拟机使用了多种垃圾回收算法。 三色标记法三色标记算法是对标记阶段的改进，原理如下： 1. 起初所有对象都是白色。 2. 从根出发扫描所有可达对象，标记为灰色，放入待处理队列。 3. 从队列取出灰色对象，将其引用对象标记为灰色放入队列，自身标记为黑色。 4. 重复 3，直到灰色对象队列为空。此时白色对象即为垃圾，进行回收。 这个算法可以实现 “on-the-fly”，也就是在程序执行的同时进行收集，并不需要暂停整个程序。但是也会有一个缺陷，可能程序中的垃圾产生的速度会大于垃圾收集的速度，这样会导致程序中的垃圾越来越多无法被收集掉。 分代收集 分代收集也是传统 Mark-Sweep 的一个改进。这个算法是基于一个经验：绝大多数对象的生命周期都很短。所以按照对象的生命周期长短来进行分代。 一般 GC 都会分三代，在 java 中称之为新生代（Young Generation）、年老代（Tenured Generation）和永久代（Permanent Generation）；在 .NET 中称之为第 0 代、第 1 代和第2代。 原理如下： 1. 新对象放入第 0 代 2. 当内存用量超过一个较小的阈值时，触发 0 代收集 3. 第 0 代幸存的对象（未被收集）放入第 1 代 4. 只有当内存用量超过一个较高的阈值时，才会触发 1 代收集 5. 2 代同理 因为 0 代中的对象十分少，所以每次收集时遍历都会非常快（比 1 代收集快几个数量级）。只有内存消耗过于大的时候才会触发较慢的 1 代和 2 代收集。 因此，分代收集是目前比较好的垃圾回收方式。使用的语言（平台）有 jvm、.NET 。 基于追踪的垃圾回收算法（标记-清扫、节点复制）一个主要问题是在生命周期较长的对象上浪费时间（长生命周期的对象是不需要频繁扫描的）。同时，内存分配存在这么一个事实 “most object die young”。基于这两点，分代垃圾回收算法将对象按生命周期长短存放到堆上的两个（或者更多）区域，这些区域就是分代（generation）。对于新生代的区域的垃圾回收频率要明显高于老年代区域。 分配对象的时候从新生代里面分配，如果后面发现对象的生命周期较长，则将其移到老年代，这个过程叫做 promote。随着不断 promote，最后新生代的大小在整个堆的占用比例不会特别大。收集的时候集中主要精力在新生代就会相对来说效率更高，STW 时间也会更短。 目前大部分垃圾收集器对于新生代都采取Copying算法，因为新生代中每次垃圾回收都要回收大部分对象，也就是说需要复制的操作次数较少，但是实际中并不是按照1：1的比例来划分新生代的空间的，一般来说是将新生代划分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden空间和其中的一块Survivor空间，当进行回收时，将Eden和Survivor中还存活的对象复制到另一块Survivor空间中，然后清理掉Eden和刚才使用过的Survivor空间。 而由于老年代的特点是每次回收都只回收少量对象，一般使用的是Mark-Compact算法。 注意，在堆区之外还有一个代就是永久代（Permanet Generation），它用来存储class类、常量、方法描述等。对永久代的回收主要回收两部分内容：废弃常量和无用的类。 优点性能更优。 缺点实现复杂 http://www.frankyang.cn/2017/08/30/gc/]]></content>
      <categories>
        <category>GC</category>
      </categories>
      <tags>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是隔离见证？简单介绍隔离见证]]></title>
    <url>%2F2017%2F07%2F24%2F%E4%BB%80%E4%B9%88%E6%98%AF%E9%9A%94%E7%A6%BB%E8%A7%81%E8%AF%81%EF%BC%9F%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E9%9A%94%E7%A6%BB%E8%A7%81%E8%AF%81-%E7%B4%AB%E9%AD%94%E6%88%92%2F</url>
    <content type="text"><![CDATA[每一个比特币交易，都可以分为两部分。第一部分是转账记录，第二部分是用来证明这个交易合法性（主要是签名）的。第一部分可称为“交易状态”，第二部分就是所谓的见证（witness）。如果你只关心每个账户的余额，那么转账记录就已经足够了。只有部分人（主要是矿工）才有必要取得交易见证。(交易信息就是谁给谁在什么时间转了多少钱。见证信息就是哪个节点在什么时间验证交易信息的可靠性。) 中本聪在设计比特币的时候直接把这两个信息直接放在了区块内，所以一个区块就承载不了更多的交易信息，如果隔离了“见证信息”，那么区块链只记录交易信息，那么一个区块可承载的交易更多交易。中本聪设计比特币时，并没有把两部分资料分开处理，因此导致交易ID的计算混合了交易和见证。因为见证本身包括签名，而签名不可能对其自身进行签名，因此见证可以由任何人在未得到交易双方同意的情况下进行改变，造成所谓的交易可塑性（malleability）。在交易发出后、确认前，交易ID可以被任意更改，因此基于未确认交易的交易是绝对不安全的。在2014年就曾有人利用这个漏洞大规模攻击比特币网络。 比特币核心开发员Pieter Wuille在2015年12月于香港提出的隔离见证（Segregated Witness，以下简称SW）软分叉解决了这个问题。SW用户在交易时，会把比特币传送到有别于传统的地址。当要使用这些比特币的时候，其签名（即见证）并不会记录为交易ID的一部分，而是进行另外处理。也就是说，交易ID完全是由交易状态来决定的，不会受见证部分的影响。 部分开发者认为比特币的设计有缺陷，在数据结构方面，它把必要的交易信息（输入和输出）和“没那么必要”的交易信息（见证）放在一起打包，这造成了一系列问题，比如“交易可塑性”、交易签名过程复杂、还有存储空间“浪费”。隔离见证是对整个设计缺陷的一个修改方案，原理说起来挺简单，它允许交易把没那么必要的“见证”部分“隔离”在区块外面，这就是隔离见证的意思了。目前人们主要关注的是存储空间浪费，因为现在比特币交易量太大，区块空间不够用，大量交易堆积。隔离见证的设计减少了每个交易脚本在区块内的体积，这相当于区块空间扩容，同时降低了数据传输和验证造成的内存、带宽、CPU成本。 这做法有几个重要的结果: 可以用软分叉增加最大区块容量:因为旧有节点根本看不到这些被隔离的见证，即使真实的区块已超过1MB，它们仍会以为没有超过限制而会接受区块。在整场有关区块容量的辩论中，最大的难点就是硬分叉。SW可以提供约2MB的有效区块空间而没有任何硬分叉风险。 从此以后，只有发出交易的人才可以改变交易ID，没有任何第三方可以做到。如果是多重签名交易，就只有多名签署人同意才能改变交易ID。这可以保证一连串的未确认交易的有效性，是双向支付通道或闪电网络所必须的功能。有了双向支付通道或闪电网络，二人或多人之间就可以实际上进行无限次交易，而无需把大量零碎交易放在区块链，大为减低区块空间压力。 轻量钱包可以变得更轻量，因为它们无需再接收见证数据。 可以大幅改善签署结构。在区块链上，曾经有一个超过5000个输入的交易，因为签署设计缺憾，需要半分钟才能完成检查。在建议中的SW软分叉会把这个问题解决掉。 http://www.frankyang.cn/2017/07/24/segregated-witness/]]></content>
      <categories>
        <category>BlockChain</category>
      </categories>
      <tags>
        <tag>隔离见证</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程死锁及解决办法]]></title>
    <url>%2F2017%2F06%2F25%2F%E8%BF%9B%E7%A8%8B%E6%AD%BB%E9%94%81%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95-%E7%B4%AB%E9%AD%94%E6%88%92%2F</url>
    <content type="text"><![CDATA[【摘要】进程死锁及解决办法一、要点提示（1） 掌握死锁的概念和产生死锁的根本原因。（2） 理解产生死锁的必要条件–以下四个条件同时具备：互斥条件、不可抢占条件、占有且申请条件、循环等待条件。（3） 记住解决死锁的一般方法，掌握死锁的预防和死锁的避免二者的基本思想。（4） 掌握死锁的预防策略中… 阅读全文 http://www.frankyang.cn/2017/06/25/lock/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Process</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何实现守护进程？]]></title>
    <url>%2F2017%2F05%2F25%2F%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[守护进程（Daemon）是运行在后台的一种特殊进程。它独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件。守护进程是一种很有用的进程。 1、守护进程最重要的特性是后台运行。2、守护进程必须与其运行前的环境隔离开来。这些环境包括未关闭的文件描述符，控制终端，会话和进程组，工作目录以及文件创建掩模等。这些环境通常是守护进程从执行它的父进程（特别是shell）中继承下来的。3、守护进程的启动方式有其特殊之处。它可以在Linux系统启动时从启动脚本/etc/rc.d中启动，可以由作业规划进程crond启动，还可以由用户终端（shell）执行。 总之，除开这些特殊性以外，守护进程与普通进程基本上没有什么区别。因此，编写守护进程实际上是把一个普通进程按照上述的守护进程的特性改造成为守护进程。如果对进程有比较深入的认识就更容易理解和编程了。 守护进程之编程规则（1）首先要做的是调用umask将文件模式创建屏蔽字设置为0。 文件权限掩码：是指屏蔽掉文件权限中的对应位。例如，有个文件权限掩码是050，它就屏蔽了文件组拥有者的可读与可执行权限（对应二进制为，rwx, 101）。由于fork函数创建的子进程继承了父进程的文件权限掩码，这就给子进程使用文件带来了诸多的麻烦。因此，把文件权限掩码设置为0（即，不屏蔽任何权限），可以增强该守护进程的灵活性。设置文件权限掩码的函数是umask。通常的使用方法为umask(0)。 （2）调用fork，然后使父进程退出（exit）。if(pid=fork()) exit(0); （3）调用setsid以创建一个新会话，脱离控制终端和进程组。setsid函数作用：用于创建一个新的会话，并担任该会话组的组长。 调用setsid有3个作用：(a) 让进程摆脱原会话的控制；(b) 让进程摆脱原进程组的控制；(c) 让进程摆脱原控制终端的控制； setsid() 使用setsid函数的目的：由于创建守护进程的第一步调用了fork函数来创建子进程再将父进程退出。由于在调用fork函数时，子进程拷贝了父进程的会话期、进程组、控制终端等，虽然父进程退出了，但会话期、进程组、控制终端等并没有改变，因此，这还不是真正意义上的独立开了。使用setsid函数后，能够使进程完全独立出来，从而摆脱其他进程的控制。 （4）将当前工作目录更改为根目录。#define NOFILE 256 for(i=0;i&lt;NOFILE;i++) close(i); （5）关闭不再需要的文件描述符。这使守护进程不再持有从其父进程继承来的某些文件描述符（父进程可能是shell进程，或某个其他进程）。 （6）某些守护进程打开/dev/null使其具有文件描述符0、1和2，这样，任何一个试图读标准输入、写标准输出和标准出错的库例程都不会产生任何效果。因为守护进程并不与终端设备相关联，所以不能在终端设备上显示其输出，也无处从交互式用户那里接受输入。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#include &lt;sys/types.h&gt;#include&lt;sys/stat.h&gt;#include&lt;sys/time.h&gt;#include&lt;sys/resource.h&gt;#include&lt;unistd.h&gt;#include&lt;fcntl.h&gt;#include&lt;stdlib.h&gt;#include&lt;syslog.h&gt; void daemonize(const char *cmd) &#123; int i, fd0, fd1, fd2; pid_t pid; struct rlimit rl; struct sigaction sa; umask(0); // Clear file creation mask. if (getrlimit(RLIMIT_NOFILE, &amp;rl) &lt; 0) &#123; // Get maximum number of file descriptors. err_quit(&quot;%s: can&apos;t get file limit&quot;, cmd); &#125; if ((pid = fork()) &lt; 0) &#123; //这一步fork保证进程不是进程组组长进程 err_quit(&quot;%s: can&apos;t fork&quot;, cmd); &#125; else if (pid != 0) &#123; /* parent */ exit(0); &#125; setsid(); // 创建一个回话，会话只包含子进程，且子进程是会话首进程 /* 会话首进程的退出会出发SIGHUP信号 默认此信号的操作会终止进程 */ sa.sa_handler = SIG_IGN; sigemptyset(&amp;sa.sa_mask); sa.sa_flags = 0; if (sigaction(SIGHUP, &amp;sa, NULL) &lt; 0) &#123; err_quit(&quot;%s: can&apos;t ignore SIGHUP&quot;, cmd); &#125; /* 再次创建子进程，退出父进程，保证守护进程不是会话首进程，这样open的时候就不会被分配终端 */ if ((pid = fork()) &lt; 0) &#123; err_quit(&quot;%s: can&apos;t fork&quot;, cmd); &#125; else if (pid != 0) &#123; /* parent */ exit(0); &#125; if (chdir(&quot;/&quot;) &lt; 0) &#123; // 改变当前工作路径为根目录 err_quit(&quot;%s: can&apos;t change directory to /&quot;, cmd); &#125; if (rl.rlim_max == RLIM_INFINITY) &#123; //关闭所有打开的文件描述符 rl.rlim_max = 1024; &#125; for (i = 0; i &lt; rl.rlim_max; i++) &#123; close(i); &#125; /* 因为前面关闭了所有的文件描述符，此时open返回的必定是最小的0，后面两次dup返回的依次是1、2， 也就完成了对标准输入、标准输出、标准错误重定向至/dev/null的操作 */ fd0 = open(&quot;/dev/null&quot;, O_RDWR); fd1 = dup(0); fd2 = dup(0); /* * Initialize the log file. */ openlog(cmd, LOG_CONS, LOG_DAEMON); if (fd0 != 0 || fd1 != 1 || fd2 != 2) &#123; syslog(LOG_ERR, &quot;unexpected file descriptors %d %d %d&quot;,fd0, fd1, fd2); exit(1); &#125; &#125; http://www.frankyang.cn/2017/05/25/daemon/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Process</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ 4种强制类型转换]]></title>
    <url>%2F2017%2F05%2F10%2FC%2B%2B%204%E7%A7%8D%E5%BC%BA%E5%88%B6%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[C++的四种强制类型转换为：static_cast、const_cast、reinterpret_cast和dynamic_cast 类型转换的一般形式：cast-name(expression); static_cast任何具有明确定义的类型转换，只要不包含底层const，都可以使用static_cast； double slope = static_cast(j) / i; 注： 顶层const：表示指针本身是个常量。如：int const p； 底层const：表示指针所指的对象是一个常量。如：int const p； const_cast该运算符只能改变运算对象的底层const。 12345678910#include&lt;iostream&gt; using namespace std;int main() &#123; const char *pc=" HDU"; char *p=const_cast&lt;char *&gt;(pc); //正确， cout&lt;&lt;"hello"&lt;&lt;p&lt;&lt;endl; return 0; &#125; 注：此处只能用const_cast，而不能用static_cast； reinterpret_cast通常为运算对象的位模式提供较低层次上的重新解释。注： 1、在指针之间转换，将一个类型的指针转换为另一个类型的指针，无关类型； 2、将指针值转换为一个整型数,但不能用于非指针类型的转换。 dynamic_cast只用于对象的指针和引用，不能用于内置的基本数据类型的强制转换。使用dynamic_cast进行转换的，基类中一定要有虚函数，否则编译不通过。运行时类型识别，用于将基类的指针或引用安全地转换成派生类的指针或引用。 对指针进行dynamic_cast，失败返回null，成功返回正常cast后的对象指针；对引用进行dynamic_cast，失败抛出一个异常bad_cast，成功返回正常cast后的对象引用。 对于“向上转换”（即派生类指针或引用类型转换为其基类类型），无论是指针还是引用向上转换都是安全地。对于“向下转型”有两种情况： 1、基类指针所指对象是派生类类型的，这种转换是安全的； 2、基类指针所指对象为基类类型，在这种情况下dynamic_cast在运行时做检查，转换失败，返回结果为0； 在引用上，dynamic_cast依旧是常用于“安全的向下转型”。与指针一样，引用的向下转型也可以分为两种情况，与指针不同的是，并不存在空引用，所以引用的dynamic_cast检测失败时会抛出一个bad_cast异常。 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;iostream&gt;#include &lt;cstring&gt;using namespace std;class A&#123;public: virtual void f() &#123; cout &lt;&lt; "hello" &lt;&lt; endl; &#125;&#125;; class B: public A&#123;public: void f() &#123; cout &lt;&lt; "hello2" &lt;&lt; endl; &#125; &#125;;int main()&#123; A* a1=new B;//a1是A类型的指针指向一个B类型的对象 A* a2=new A;//a2是A类型的指针指向一个A类型的对象 B* b; b=dynamic_cast&lt;B*&gt;(a1); //结果为not null，向下转换成功，a1之前指向的就是B类型的对象，所以可以转换成B类型的指针。 if(b==NULL) cout&lt;&lt;"null"&lt;&lt;endl; else cout&lt;&lt;"not null"&lt;&lt;endl; b=dynamic_cast&lt;B*&gt;(a2);//结果为null，向下转换失败 if(b==NULL) cout&lt;&lt;"null"&lt;&lt;endl; else cout&lt;&lt;"not null"&lt;&lt;endl; return 0;&#125; 总结 基本类型转换用static_cast。 去const属性用const_cast。 不同类型的指针类型转换用reinterpreter_cast。 多态类之间的类型转换用daynamic_cast。 C++ 4种强制类型转换C++的四种强制类型转换为：static_cast、const_cast、reinterpret_cast和dynamic_cast 类型转换的一般形式：cast-name(expression); static_cast任何具有明确定义的类型转换，只要不包含底层const，都可以使用static_cast； double slope = static_cast(j) / i; 注： 顶层const：表示指针本身是个常量。如：int const p； 底层const：表示指针所指的对象是一个常量。如：int const p； const_cast该运算符只能改变运算对象的底层const。 12345678910#include&lt;iostream&gt; using namespace std;int main() &#123; const char *pc=" HDU"; char *p=const_cast&lt;char *&gt;(pc); //正确， cout&lt;&lt;"hello"&lt;&lt;p&lt;&lt;endl; return 0; &#125; 注：此处只能用const_cast，而不能用static_cast； reinterpret_cast通常为运算对象的位模式提供较低层次上的重新解释。注： 1、在指针之间转换，将一个类型的指针转换为另一个类型的指针，无关类型； 2、将指针值转换为一个整型数,但不能用于非指针类型的转换。 dynamic_cast只用于对象的指针和引用，不能用于内置的基本数据类型的强制转换。使用dynamic_cast进行转换的，基类中一定要有虚函数，否则编译不通过。运行时类型识别，用于将基类的指针或引用安全地转换成派生类的指针或引用。 对指针进行dynamic_cast，失败返回null，成功返回正常cast后的对象指针；对引用进行dynamic_cast，失败抛出一个异常bad_cast，成功返回正常cast后的对象引用。 对于“向上转换”（即派生类指针或引用类型转换为其基类类型），无论是指针还是引用向上转换都是安全地。对于“向下转型”有两种情况： 1、基类指针所指对象是派生类类型的，这种转换是安全的； 2、基类指针所指对象为基类类型，在这种情况下dynamic_cast在运行时做检查，转换失败，返回结果为0； 在引用上，dynamic_cast依旧是常用于“安全的向下转型”。与指针一样，引用的向下转型也可以分为两种情况，与指针不同的是，并不存在空引用，所以引用的dynamic_cast检测失败时会抛出一个bad_cast异常。 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;iostream&gt;#include &lt;cstring&gt;using namespace std;class A&#123;public: virtual void f() &#123; cout &lt;&lt; "hello" &lt;&lt; endl; &#125;&#125;; class B: public A&#123;public: void f() &#123; cout &lt;&lt; "hello2" &lt;&lt; endl; &#125; &#125;;int main()&#123; A* a1=new B;//a1是A类型的指针指向一个B类型的对象 A* a2=new A;//a2是A类型的指针指向一个A类型的对象 B* b; b=dynamic_cast&lt;B*&gt;(a1); //结果为not null，向下转换成功，a1之前指向的就是B类型的对象，所以可以转换成B类型的指针。 if(b==NULL) cout&lt;&lt;"null"&lt;&lt;endl; else cout&lt;&lt;"not null"&lt;&lt;endl; b=dynamic_cast&lt;B*&gt;(a2);//结果为null，向下转换失败 if(b==NULL) cout&lt;&lt;"null"&lt;&lt;endl; else cout&lt;&lt;"not null"&lt;&lt;endl; return 0;&#125; 总结 基本类型转换用static_cast。 去const属性用const_cast。 不同类型的指针类型转换用reinterpreter_cast。 多态类之间的类型转换用daynamic_cast。 http://www.frankyang.cn/2017/05/10/c-4%E7%A7%8D%E5%BC%BA%E5%88%B6%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/]]></content>
      <categories>
        <category>Cpp</category>
      </categories>
      <tags>
        <tag>Cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP超时重传、滑动窗口、拥塞控制、快重传和快恢复]]></title>
    <url>%2F2017%2F05%2F08%2FTCP%E8%B6%85%E6%97%B6%E9%87%8D%E4%BC%A0%E3%80%81%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E3%80%81%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E3%80%81%E5%BF%AB%E9%87%8D%E4%BC%A0%E5%92%8C%E5%BF%AB%E6%81%A2%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[TCP超时重传 原理是在发送某一个数据以后就开启一个计时器，在一定时间内如果没有得到发送的数据报的ACK报文，那么就重新发送数据，直到发送成功为止。 影响超时重传机制协议效率的一个关键参数是重传超时时间（RTO，Retransmission TimeOut）。RTO的值被设置过大过小都会对协议造成不利影响。 （1）RTO设长了，重发就慢，没有效率，性能差。 （2）RTO设短了，重发的就快，会增加网络拥塞，导致更多的超时，更多的超时导致更多的重发。 连接往返时间（RTT，Round Trip Time），指发送端从发送TCP包开始到接收它的立即响应所消耗的时间。 TCP滑动窗口作用：（1）提供TCP的可靠性；（2）提供TCP的流控特性 TCP的滑动窗口的可靠性也是建立在“确认重传”基础上的。发送窗口只有收到对端对于本段发送窗口内字节的ACK确认，才会移动发送窗口的左边界。接收端可以根据自己的状况通告窗口大小，从而控制发送端的接收，进行流量控制。 TCP拥塞控制 拥塞控制是一个全局性的过程； 流量控制是点对点通信量的控制 TCP拥塞控制4个核心算法：慢开始（slow start）、拥塞避免（Congestion Avoidance）、快速重传（fast retransmit）、快速回复（fast recovery） 拥塞窗口（cwnd，congestion window），其大小取决于网络的拥塞程度，并且动态地在变化。 慢开始算法的思路就是，不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小。 为了防止cwnd增长过大引起网络拥塞，还需设置一个慢开始门限ssthresh状态变量。ssthresh的用法如下：当cwnd &lt; ssthresh时，使用慢开始算法。当cwnd &gt; ssthresh时，改用拥塞避免算法。当cwnd = ssthresh时，慢开始与拥塞避免算法任意。 拥塞避免算法让拥塞窗口缓慢增长，即每经过一个往返时间RTT就把发送发的拥塞窗口cwnd加1，而不是加倍。 无论是在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞，就把慢开始门限设置为出现拥塞时的发送窗口大小的一半。然后把拥塞窗口设置为1，执行慢开始算法。如下图：拥塞控制的具体过程如下：（1）TCP连接初始化，将拥塞窗口设置为1（2）执行慢开始算法，cwnd按指数规律增长，直到cwnd=ssthresh时，开始执行拥塞避免算法，cwnd按线性规律增长（3）当网络发生拥塞，把ssthresh值更新为拥塞前ssthresh值的一半，cwnd重新设置为1，按照步骤（2）执行 快重传和快恢复 快速重传(Fast retransmit)要求接收方在收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方），而不要等到自己发送数据时捎带确认。 快重传算法规定，发送方只要一连收到3个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计数器时间到期。 快速恢复(Fast Recovery) （1）当发送方连续收到三个重复确认，就执行“乘法减小”算法，把慢开始门限ssthresh减半。这是为了预防网络发生拥塞。请注意：接下去不执行慢开始算法。 （2）由于发送方现在认为网络很可能没有发生拥塞，因此与慢开始不同之处是现在不执行慢开始算法（即拥塞窗口cwnd现在不设置为1），而是把cwnd值设置为慢开始门限ssthresh减半后的数值，然后开始执行拥塞避免算法（“加法增大”），使拥塞窗口缓慢地线性增大。 发送方窗口的上限值 = Min [ rwnd, cwnd ] 当rwnd &lt; cwnd 时，是接收方的接收能力限制发送方窗口的最大值。 当cwnd &lt; rwnd 时，则是网络的拥塞限制发送方窗口的最大值。 http://www.frankyang.cn/2017/05/08/tcp1/]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP与HTTPS异同||HTTP1.0与HTTP1.1差别]]></title>
    <url>%2F2017%2F04%2F19%2FHTTP%E4%B8%8EHTTPS%E5%BC%82%E5%90%8C%3AHTTP1.0%E4%B8%8EHTTP1.1%E5%B7%AE%E5%88%AB%2F</url>
    <content type="text"><![CDATA[HTTP状态码 分类 解释 描述 1XX 信息 服务器收到请求，需要请求者继续执行操作 2XX 成功 操作被成功接收并处理 3XX 重定向 需要进一步的操作以完成请求 4XX 客户端错误 请求包含语法错误或无法完成请求 5XX 服务器错误 服务器在处理请求的过程中发生了错误 常见的状态码 状态代码 状态消息 描述 200 成功 处理请求无误 301 永久移动 内容已移动到位置头中指明的主机上 400 错误请求 服务器不能理解请求 403 禁止 服务器无权访问所请求的文件 404 未发现 服务器不能找到所请求的方法 501 未实现 服务器不支持请求的方法 505 HTTP版本不支持 服务器不支持请求的版本 HTTP与HTTPS一、HTTP和HTTPS的基本概念 HTTP：互联网上应用最为广泛的一种网络协议，是一个客户端和服务器端请求和应答的标准(TCP)，用于从WWW服务器传输超文本到本地浏览器的传输协议。 HTTP协议采用明文传输信息，存在信息窃听、信息篡改和信息劫持的风险，而协议TLS/SSL具有身份验证、信息加密和完整性校验的功能，可以避免此类问题发生。 HTTPS（Hyper Text Transfer over Secure Socket Layer基于安全套接字层的超文本传输协议 ）：是以安全为目标的HTTP通道，简单讲是HTTP的安全版（HTTPS = HTTP + SSL），即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。 HTTPS协议作用 1. 建立一个信息安全通道，来保证数据传输的安全； 2. 另一种就是确认网站的真实性。 二、HTTPS和HTTP的区别主要如下： 1、https协议需要到CA申请证书，一般免费证书较少，因而需要一定费用。 2、http是超文本传输协议，信息是明文传输，https则是具有安全性的SSL加密传输协议。 3、http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 4、http的连接很简单，是无状态的；HTTPS协议是由HTTP+SSL协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。 三、HTTPS加密过程?1.客户端发起HTTPS请求 用户在浏览器里输入一个https网址，然后连接到server的443端口。 2.服务端的配置 采用HTTPS协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请。这套证书其实就是一对公钥和私钥。 3.传送证书 这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。 4.客户端解析证书 这部分工作是有客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。如果证书没有问题，那么就生成一个随即值。然后用证书对该随机值进行加密。 5.传送加密随机值 这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。 6.服务段解密信息 服务端用私钥解密后，得到了客户端传过来的随机值(私钥)，然后把内容通过该值进行对称加密。所谓对称加密就是，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥。 7.传输加密后的信息 这部分信息是服务段用私钥加密后的信息，可以在客户端被还原 8.客户端解密信息 客户端用之前生成的私钥解密服务段传过来的信息，于是获取了解密后的内容。整个过程第三方即使监听到了数据，也束手无策。 四、什么时候该使用 HTTPS? 缺点？银行网站、支付网关、购物网站、登录页、电子邮件以及一些企业部门的网站应该使用 HTTPS ####缺点（1）HTTPS协议握手阶段比较费时，会使页面的加载时间延长近50%，增加10%到20%的耗电； （2）HTTPS连接缓存不如HTTP高效，会增加数据开销和功耗，甚至已有的安全措施也会因此而受到影响； （3）SSL证书需要钱，功能越强大的证书费用越高，个人网站、小网站没有必要一般不会用。 HTTP1.0与HTTP1.11、HTTP 1.1支持长连接（Persistent Connection）和请求的流水线（Pipelining）处理 HTTP 1.0规定浏览器与服务器只保持短暂的连接，浏览器的每次请求都需要与服务器建立一个TCP连接，服务器完成请求处理后立即断开TCP连接，服务器不跟踪每个客户也不记录过去的请求。 HTTP 1.1支持持久连接，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟。 HTTP 1.1还允许客户端不用等待上一次请求结果返回，就可以发出下一次请求，但服务器端必须按照接收到客户端请求的先后顺序依次回送响应结果，以保证客户端能够区分出每次请求的响应内容，这样也显著地减少了整个下载过程所需要的时间。 2、HTTP 1.1增加host字段 在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。 HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。此外，服务器应该接受以绝对路径标记的资源请求。 HTTP 1.1还提供了与身份认证、状态管理和Cache缓存等机制相关的请求头和响应头。 3、100(Continue) Status(节约带宽) HTTP/1.1加入了一个新的状态码100（Continue）。客户端事先发送一个只带头域的请求，如果服务器因为权限拒绝了请求，就回送响应码401（Unauthorized）；如果服务器接收此请求就回送响应码100，客户端就可以继续发送带实体的完整请求了。100 (Continue) 状态代码的使用，允许客户端在发request消息body之前先用request header试探一下server，看server要不要接收request body，再决定要不要发request body。 http://www.frankyang.cn/2017/04/19/http-https/]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Http</tag>
        <tag>Https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP三次握手四次挥手]]></title>
    <url>%2F2017%2F04%2F18%2FTCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%2F</url>
    <content type="text"><![CDATA[TCP三次握手四次挥手 标志位缩写 全称 中文 SYN synchronous 建立联机 ACK acknowledgement 确认 PSH push 传送 FIN finish 结束 RST reset 重置 URG urgent 紧急 Seq Sequence number 顺序号码 ACK Acknowledge number 确认号码 状态名称 意义 LISTEN 侦听来自远方TCP端口的连接请求 SYN-SENT 在发送连接请求后等待匹配的连接请求 SYN-RECEIVED 在收到和发送一个连接请求后等待对连接请求的确认 ESTABLISHED 代表一个打开的连接，数据可以传送给用户 FIN-WAIT-1 等待远程TCP的连接中断请求，或先前的连接中断请求的确认 FIN-WAIT-2 从远程TCP等待连接中断请求 CLOSE-WAIT 等待从本地用户发来的连接中断请求 CLOSING 等待远程TCP对连接中断的确认 LAST-ACK 等待原来发向远程TCP的连接中断请求的确认 TIME-WAIT 等待足够的时间以确保远程TCP接收到连接中断请求的确认 CLOSED 没有任何连接状态 【注意】 在TIME_WAIT状态中，如果TCP client端最后一次发送的ACK丢失了，它将重新发送。TIME_WAIT状态中所需要的时间是依赖于实现方法的。典型的值为30秒、1分钟和2分钟。等待之后连接正式关闭，并且所有的资源(包括端口号)都被释放 根据TCP协议定义的3次握手断开连接规定,发起socket主动关闭的一方 socket将进入TIME_WAIT状态。TIME_WAIT状态将持续2个MSL(Max Segment Lifetime),在Windows下默认为4分钟，即240秒。TIME_WAIT状态下的socket不能被回收使用. 具体现象是对于一个处理大量短连接的服务器,如果是由服务器主动关闭客户端的连接，将导致服务器端存在大量的处于TIME_WAIT状态的socket， 甚至比处于Established状态下的socket多的多,严重影响服务器的处理能力，甚至耗尽可用的socket，停止服务。 【问题】为什么连接的时候是三次握手，不是两次呢？ &emsp;&emsp;现假定出现一种异常情况，即A发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间滞留了，以致延误到连接释放以后的某个时间才到达B。本来这是一个早已失效的报文段。但B收到此失效的连接请求报文段后，就误认为是A又发出一次新的连接请求。于是就向A发出确认报文段，同意建立连接。假定不采用三次握手，那么只要B发出确认，新连接就建立了。&emsp;&emsp;由于现在A并没有发出建立连接的请求，因此不会理睬B的确认，也不会向B发送数据。但B却以为新的运输连接已经建立了，并一直等待A发来数据，B的许多资源就这样白白浪费了。 【问题】关闭的时候却是四次握手？ 答：因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，”你发的FIN报文我收到了”。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四步握手。&emsp;&emsp;关闭连接时，当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，乙方也未必全部数据都发送给对方了，所以乙方可以立即close，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，乙方ACK和FIN一般都会分开发送。 【问题】为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？ 答：虽然按道理，四个报文都发送完毕，我们可以直接进入CLOSE状态了，但是我们必须假象网络是不可靠的，有可以最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。 【问题】主动发起关闭连接的操作的一方将达到TIME_WAIT状态，而且这个状态要保持Maximum Segment Lifetime的两倍时间。为什么要这样做而不是直接进入CLOSED状态？1.保证TCP协议的全双工连接能够可靠关闭 如果Client直接CLOSED了，那么由于IP协议的不可靠性或者是其它网络原因，导致Server没有收到Client最后回复的ACK。那么Server就会在超时之后继续发送FIN，此时由于Client已经CLOSED了，就找不到与重发的FIN对应的连接，最后Server就会收到RST而不是ACK，Server就会以为是连接错误把问题报告给高层。这样的情况虽然不会造成数据丢失，但是却导致TCP协议不符合可靠连接的要求。所以，Client不是直接进入CLOSED，而是要保持TIME_WAIT，当再次收到FIN的时候，能够保证对方收到ACK，最后正确的关闭连接。 2.保证这次连接的重复数据段从网络中消失 如果Client直接CLOSED，然后又再向Server发起一个新连接，我们不能保证这个新连接与刚关闭的连接的端口号是不同的。也就是说有可能新连接和老连接的端口号是相同的。一般来说不会发生什么问题，但是还是有特殊情况出现：假设新连接和已经关闭的老连接端口号是一样的，如果前一次连接的某些数据仍然滞留在网络中，这些延迟数据在建立新连接之后才到达Server，由于新连接和老连接的端口号是一样的，又因为TCP协议判断不同连接的依据是socket pair，于是，TCP协议就认为那个延迟的数据是属于新连接的，这样就和真正的新连接的数据包发生混淆了。所以TCP连接还要在TIME_WAIT状态等待2倍MSL，这样可以保证本次连接的所有数据都从网络中消失。 SYN攻击：&emsp;&emsp;在三次握手过程中，Server发送SYN-ACK之后，收到Client的ACK之前的TCP连接称为半连接（half-open connect），此时Server处于SYN_RCVD状态，当收到ACK后，Server转入ESTABLISHED状态。&emsp;&emsp;SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server回复确认包，并等待Client的确认，由于源地址是不存在的，因此，Server需要不断重发直至超时，这些伪造的SYN包将长时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络堵塞甚至系统瘫痪。&emsp;&emsp;SYN攻击时一种典型的DDOS攻击，检测SYN攻击的方式非常简单，即当Server上有大量半连接状态且源IP地址是随机的，则可以断定遭到SYN攻击了，使用如下命令可以让之现行： #netstat -nap | grep SYN_RECV http://www.frankyang.cn/2017/08/23/tcp-san-ci-wo-shou-si-ci-hui-shou/]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
</search>
